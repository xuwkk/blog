[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog! Here I regularly update learning notes and news about my research."
  },
  {
    "objectID": "posts/news/index.html",
    "href": "posts/news/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post of my blog. I will regularly update my learning note and research outcomes at this website."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Content",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nJun 18, 2024\n\n\nTangent and Adjoint Sensitivity Analysis of Linear Equations\n\n\nAuto Differentiation, Implicit Function, NeuralODE\n\n\n\n\nJun 14, 2024\n\n\nEconomic Dispatch\n\n\nPower System Operation\n\n\n\n\nJun 13, 2024\n\n\nUnit Commitment Problem\n\n\nPower System Operation\n\n\n\n\nJun 11, 2024\n\n\nDeep Implicit Layers: Differentiable Convex Layer\n\n\nAuto Differentiation, Implicit Function, Paper Read, Optimization\n\n\n\n\nJun 11, 2024\n\n\nDeep Implicit Layers: Fixed-Point Iteration\n\n\nAuto Differentiation, Implicit Function\n\n\n\n\nJun 9, 2024\n\n\nPower System Operation: AC and DC Power Flow Model\n\n\nPower System Operation\n\n\n\n\nJun 5, 2024\n\n\nTangent and Adjoint Sensitivity Analysis of Nonlinear Equations\n\n\nAuto Differentiation, Implicit Function\n\n\n\n\nJun 3, 2024\n\n\nTangent and Adjoint Sensitivity Analysis of Linear Equations\n\n\nAuto Differentiation, Implicit Function\n\n\n\n\nMay 31, 2024\n\n\nWelcome To My Blog\n\n\nNews\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_linear_equation.html",
    "href": "posts/learning/autometic_differentiation/adjoint_linear_equation.html",
    "title": "Tangent and Adjoint Sensitivity Analysis of Linear Equations",
    "section": "",
    "text": "This post contains my learning note for YouTube: Adjoint Equation of a Linear System of Equations - by implicit derivative.\nAll credits go to the author of the video."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_linear_equation.html#settings",
    "href": "posts/learning/autometic_differentiation/adjoint_linear_equation.html#settings",
    "title": "Tangent and Adjoint Sensitivity Analysis of Linear Equations",
    "section": "Settings",
    "text": "Settings\nConsider a linear system of equations \\[\nA(\\theta) x = b(\\theta)\n\\tag{1}\\] with a loss function \\(J(x)\\) . Our goal is to find the total derivative \\(\\frac{d J}{d \\theta}\\). This gradient can be useful for:\n\nGradient-based optimization.\nLocal sensitivity analysis of linear equations.\n\nwhere \\(\\theta\\in\\mathbb{R}^P\\) (this can be the weights of neural network); \\(A\\in\\mathbb{R}^{M\\times N}\\); \\(x\\in\\mathbb{R}^N\\); \\(b\\in\\mathbb{R}^M\\); \\(J(x;\\theta): \\mathbb{R}^N \\times \\mathbb{R}^P \\rightarrow \\mathbb{R}\\).\nNote that \\(x\\) is dependent on \\(\\theta\\) through \\(A\\) and \\(b\\). The total derivative \\(\\frac{d J}{d \\theta}\\) can be computed using the chain rule: \\[\n\\frac{d J}{d \\theta} = \\frac{\\partial J}{\\partial x} \\frac{d x}{d \\theta} + \\frac{\\partial J}{\\partial \\theta}\n\\tag{2}\\] where we use the Jacobian convension such that \\(\\frac{d x}{d \\theta}\\) is a matrix of size \\(N\\times P\\) and it is difficult to compute directly."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_linear_equation.html#tangent-sensitivity-analysis",
    "href": "posts/learning/autometic_differentiation/adjoint_linear_equation.html#tangent-sensitivity-analysis",
    "title": "Tangent and Adjoint Sensitivity Analysis of Linear Equations",
    "section": "Tangent Sensitivity Analysis",
    "text": "Tangent Sensitivity Analysis\nDo total derivative of (Equation 1) with respect to \\(\\theta\\): \\[\n\\frac{d}{d\\theta} (A x) =  \\frac{d b}{d\\theta}\n\\]\nwhere the unknown \\(\\frac{d x}{d \\theta}\\) can found by solving the following linear system\n\\[\nA\\cdot\\frac{d x}{d \\theta}  = \\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x\n\\] where \\(\\frac{d x}{d \\theta}\\) can be solved as \\[\n\\frac{d x}{d \\theta} = A^{-1} \\left(\\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x\\right)\n\\tag{3}\\]\nNote that the dimension \\(\\frac{d A}{d \\theta}\\in\\mathbb{R}^{N\\times N\\times P}\\). Therefore the product \\(\\frac{d A}{d\\theta} x\\) is incorrect (but it is ok here).\nThis is a batch of linear system we want to solve. Let \\(\\theta_i\\) be the \\(i\\)-th element of \\(\\theta\\), \\[\nA\\cdot\\frac{d x}{d \\theta_i}  = \\frac{db}{d\\theta_i} - \\underbrace{\\frac{d A}{d\\theta_i}}_{N\\times N} x, \\quad i=1,\\dots,P\n\\]\nWe can view \\(\\frac{d x}{d \\theta_i}\\) as the tangent of \\(x\\), e.g., \\(\\dot{x}_i = \\frac{d x}{d \\theta_i}\\). Then solving the above system follows the idea of tangent sensitivity analysis (or forward-mode AD). The matrix-vector products can be computed efficiently using the JVP. However, this method is less efficient when \\(P\\) is large."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_linear_equation.html#adjoint-sensitivity-analysis",
    "href": "posts/learning/autometic_differentiation/adjoint_linear_equation.html#adjoint-sensitivity-analysis",
    "title": "Tangent and Adjoint Sensitivity Analysis of Linear Equations",
    "section": "Adjoint Sensitivity Analysis",
    "text": "Adjoint Sensitivity Analysis\nPlug (Equation 3) into (Equation 2), we have \\[\n\\frac{d J}{d \\theta} = \\underbrace{\\frac{\\partial J}{\\partial x} A^{-1}}_{\\lambda^T:1\\times N} \\left(\\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x\\right) + \\frac{\\partial J}{\\partial \\theta}\n\\]\nNow instead of solving the linear system as in the tangent method (which requires solving \\(P\\) linear systems), note that the term \\(\\frac{\\partial J}{\\partial x} A^{-1}\\) is a vector of size \\(1\\times N\\) which can be solved by the following linear system once: \\[\nA^T \\lambda = \\left(\\frac{\\partial J}{\\partial x}\\right)^T\n\\tag{4}\\]\nThen the gradient \\(\\frac{d J}{d \\theta}\\) can be computed as \\[\n\\frac{d J}{d \\theta} = \\lambda^T \\left(\\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x\\right) + \\frac{\\partial J}{\\partial \\theta}\n\\]\nThis is the idea of adjoint sensitivity analysis (or reverse-mode AD). It can be considered as assigning the adjoint of \\(x\\) as \\(\\bar{x} = \\frac{\\partial J}{\\partial x}\\).\nThe adjoint sensitivity can be solved by solving two linear systems: (Equation 1) for \\(x\\) and (Equation 4) for \\(\\lambda\\). This is more efficient when \\(P\\) is large (especially when the loss function is a scalar).\nAlso note that the Jacobians \\(\\frac{d A}{d\\theta}\\), \\(\\frac{d b}{d\\theta}\\), \\(\\frac{dA}{d\\theta}\\), and \\(\\frac{\\partial J}{\\partial x}\\) can be computed by automatic differentiation or analytical solution.\n\nAlternative Derivation Using Lagrange Multiplier\nThis part is based on the YouTube: Adjoint Sensitivities of a Linear System of Equations - derived using the Lagrangian.\nAs the sensitivity analysis can be directly used for purturbation analysis in an optimization problem, e.g., to find how a small change on \\(\\theta\\) can affect the objective function \\(J(x)\\), we can consider the following optimization problem \\[\n\\min_{x} J(x, \\theta) \\quad \\text{s.t.} \\quad A(\\theta) x = b(\\theta)\n\\]\nThe equality constraint can be regarded as the KKT condition of another convex optimization problem. I.e., the original problem is actually a bi-level optimization problem. The Lagrangian of the above problem is \\[\n\\mathcal{L}(x, \\theta, \\lambda) = J(x(\\theta), \\lambda) + \\lambda^T (b(\\theta) - A(\\theta) x)\n\\] where \\(\\lambda\\) is the Lagrange multiplier. The total derivative wrt \\(\\theta\\) is \\[\n\\frac{d\\mathcal{L}}{d\\theta}  = \\frac{\\partial J}{\\partial x} \\frac{d x}{d \\theta} + \\frac{\\partial J}{\\partial \\theta} + \\lambda^T \\left(\\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x - A(\\theta)\\frac{dx}{d\\theta}\\right)\n\\]\nAgain, the dimension of \\(\\frac{d A}{d\\theta}\\) is incorrect. The difficult term is \\(\\frac{dx}{d\\theta}\\). After some arrangement, we have \\[\n\\frac{d\\mathcal{L}}{d\\theta} = \\frac{\\partial J}{\\partial \\theta} + \\lambda^T \\left(\\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x\\right) + \\underbrace{\\left(\\frac{\\partial J}{\\partial x} - \\lambda^T A\\right)}_{\\rightarrow 0} \\frac{dx}{d\\theta}\n\\]\nNote that because \\(x\\) is solved as the solution to \\(Ax = b\\), the equality constraint is always satisfied. Therefore, the value of \\(\\lambda\\) can be arbitrary. Consequently, we obtain the adjoint system the same to the previous derivation Equation 4.\nPLugging in \\(\\lambda\\) into the Lagrangian, we have \\[\n\\frac{d \\mathcal{L}}{d \\theta} = \\frac{d J}{d \\theta} = \\lambda^T \\left(\\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x\\right) + \\frac{\\partial J}{\\partial \\theta}\n\\] where the first equality is due to \\(Ax=b\\)."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html",
    "href": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html",
    "title": "Tangent and Adjoint Sensitivity Analysis of Nonlinear Equations",
    "section": "",
    "text": "This post extends from the previous post on linear system to find the sensitivities of the parameter of nonlinear systems. This post is largely learnt from the YouTube: Adjoint Sensitivities of a Non-Linear system of equations | Full Derivation and YouTube: Lagrangian Perspective on the Derivation of Adjoint Sensitivities of Nonlinear Systems. Another reference is Deep Implicit Layers"
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#settings",
    "href": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#settings",
    "title": "Tangent and Adjoint Sensitivity Analysis of Nonlinear Equations",
    "section": "Settings",
    "text": "Settings\nConsider a nonlinear system of equations \\[\nf(x, \\theta) = 0\n\\] where \\(x \\in \\mathbb{R}^N\\) is the state variable and \\(\\theta \\in \\mathbb{R}^P\\) is the parameter. Nonlinear equation solvers such as Newton’s method can be used to find \\(x\\) given \\(\\theta\\). Assume there is a scalar loss function \\(J(x,\\theta)\\) and our goal is to find the sensitivity or total gradient of \\(J\\) with respect to \\(\\theta\\): \\(\\frac{d J}{d \\theta}\\).\nThe total derivative of \\(J\\) is \\[\n\\frac{d J}{d \\theta} = \\frac{\\partial J}{\\partial x} \\frac{d x}{d \\theta} + \\frac{\\partial J}{\\partial \\theta}\n\\tag{1}\\] where \\(\\frac{d x}{d \\theta}\\) is unknown. Do the total derivative of \\(f\\) with respect to \\(\\theta\\): \\[\n\\frac{d f}{d \\theta} = \\frac{\\partial f}{\\partial x} \\frac{d x}{d \\theta} + \\frac{\\partial f}{\\partial \\theta} = 0\n\\tag{2}\\]\nTherefore, \\(\\frac{d x}{d \\theta}\\) can be solved by the above linear equation. In detail, Equation 1 can be rewritten as \\[\n\\frac{d J}{d \\theta} = -\\frac{\\partial J}{\\partial x} \\left( \\frac{\\partial f}{\\partial x} \\right)^{-1} \\frac{\\partial f}{\\partial \\theta} + \\frac{\\partial J}{\\partial \\theta}\n\\tag{3}\\]"
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#tangent-sensitivity-analysis",
    "href": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#tangent-sensitivity-analysis",
    "title": "Tangent and Adjoint Sensitivity Analysis of Nonlinear Equations",
    "section": "Tangent Sensitivity Analysis",
    "text": "Tangent Sensitivity Analysis\nIn the tangent (forward) method, the term \\(\\left( \\frac{\\partial f}{\\partial x} \\right)^{-1} \\frac{\\partial f}{\\partial \\theta}\\) is computed by solving the batch of linear system Equation 2 directly. Denote the \\(i\\)-th column of \\(\\frac{\\partial f}{\\partial \\theta}\\) as \\(g_i\\), then \\(P\\) linear systems need to be solved: \\[\n\\frac{\\partial f}{\\partial x} \\frac{dx}{d\\theta_i} = -g_i\n\\]"
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#adjoint-sensitivity-analysis",
    "href": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#adjoint-sensitivity-analysis",
    "title": "Tangent and Adjoint Sensitivity Analysis of Nonlinear Equations",
    "section": "Adjoint Sensitivity Analysis",
    "text": "Adjoint Sensitivity Analysis\nEquation 3 can be solved from left to right by first computing \\(-\\frac{\\partial J}{\\partial x} \\left( \\frac{\\partial f}{\\partial x} \\right)^{-1}\\) and then multiplying \\(\\frac{\\partial f}{\\partial \\theta}\\). The adjoint linear system is \\[\n\\left( \\frac{\\partial f}{\\partial x} \\right)^T \\lambda = -\\left(\\frac{\\partial J}{\\partial x}\\right)^T\n\\tag{4}\\] which can be solved by conjugate gradient method or LU deomposition. The Jacobian matrix \\(\\frac{\\partial f}{\\partial x}\\) may not need to be solved explicitely but can be found by VJP.\nIn the adjoint method, there is only one linear system to solve (note that the original system is nonlinear), regardless of the number of parameters \\(P\\).\n\nAlternative Derivation using Lagrangian\nSimilar to the linear system, we can derive the adjoint sensitivity analysis for nonlinear system from the Lagrangian perspective.\nConsider the equality constrained optimization: \\[\n\\min_{x} J(x, \\theta) \\quad \\text{s.t.} \\quad f(x, \\theta) = 0\n\\]\nThe Lagrangian is \\[\n\\mathcal{L}(x, \\lambda, \\theta) = J(x, \\theta) + \\lambda^T f(x, \\theta)\n\\]\nTake the total derivative of \\(\\mathcal{L}\\) with respect to \\(\\theta\\): \\[\n\\frac{d \\mathcal{L}}{d \\theta} = \\frac{\\partial J}{\\partial \\theta} + \\frac{\\partial J}{\\partial x} \\frac{dx}{d\\theta} + \\lambda^T \\left(\\frac{\\partial f}{\\partial \\theta} + \\frac{\\partial f}{\\partial x} \\frac{dx}{d\\theta}\\right) = \\frac{\\partial J}{\\partial \\theta} + \\lambda^T\\frac{\\partial f}{\\partial \\theta} + \\left(\\frac{\\partial J}{\\partial x} + \\lambda^T \\frac{\\partial f}{\\partial x}\\right) \\frac{dx}{d\\theta}\n\\]\nBecause the equality constraint is always satisfied (as \\(x\\) is solved from \\(f(x, \\theta) = 0\\)), we can set the dual variable \\(\\lambda\\) arbitarily. Here, we can choose to make the coefficient of \\(\\frac{dx}{d\\theta}\\) to be zero so that this complex term never appears in the final expression. \\[\n\\frac{\\partial J}{\\partial x} + \\lambda^T \\frac{\\partial f}{\\partial x} = 0\n\\] which is the adjoint equation Equation 4.\nAt last, because \\(f(x,\\theta)=0\\), we have \\[\n\\frac{d \\mathcal{L}}{d \\theta} = \\frac{\\partial J}{\\partial \\theta} + \\lambda^T\\frac{\\partial f}{\\partial \\theta} = \\frac{d J}{d\\theta}\n\\]\n\n\nRelation to Linear System\nWe can rewrite the linear system as \\[\nf(x, \\theta) = b(\\theta) - A(\\theta) x = 0\n\\] with \\[\n\\frac{\\partial f}{\\partial \\theta} = \\frac{\\partial f}{\\partial b}\\frac{db}{d\\theta} + \\frac{\\partial f}{\\partial A}\\frac{dA}{d\\theta} = \\frac{db}{d\\theta} - \\frac{dA}{d\\theta}x \\neq 0\n\\]\nTherefore, the total derivative can be written as \\[\n\\frac{df}{d\\theta} = \\frac{\\partial f}{\\partial x} \\frac{dx}{d\\theta} + \\frac{\\partial f}{\\partial \\theta} = -A \\frac{dx}{d\\theta} + \\frac{db}{d\\theta} - \\frac{dA}{d\\theta}x\n\\] which recover the derivation of the linear system.\n\n\nImplementation\nAn example implementation of batched-version of adjoint sensitivity analysis has been added to here."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#relation-to-linear-system",
    "href": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#relation-to-linear-system",
    "title": "Tangent and Adjoint Sensitivity Analysis of Nonlinear Equations",
    "section": "Relation to Linear System",
    "text": "Relation to Linear System\nWe can rewrite the linear system as \\[\nf(x, \\theta) = b(\\theta) - A(\\theta) x = 0\n\\] with \\[\n\\frac{\\partial f}{\\partial \\theta} = \\frac{\\partial f}{\\partial b}\\frac{db}{d\\theta} + \\frac{\\partial f}{\\partial A}\\frac{dA}{d\\theta} = \\frac{db}{d\\theta} - \\frac{dA}{d\\theta}x \\neq 0\n\\]\nTherefore, the total derivative can be written as \\[\n\\frac{df}{d\\theta} = \\frac{\\partial f}{\\partial x} \\frac{dx}{d\\theta} + \\frac{\\partial f}{\\partial \\theta} = -A \\frac{dx}{d\\theta} + \\frac{db}{d\\theta} - \\frac{dA}{d\\theta}x\n\\]"
  },
  {
    "objectID": "posts/learning/autometic_differentiation/implicit_equation.html",
    "href": "posts/learning/autometic_differentiation/implicit_equation.html",
    "title": "Functions Defined Implicitly by Equations (Part A)",
    "section": "",
    "text": "This post contains my understanding on Chapter 1 Functions Defined Implicitly by Equations of the book Implicit Functions and Solution Mappings by Asen L. Dontchev and R. T. Rockafellar."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/implicit_equation.html#introduction",
    "href": "posts/learning/autometic_differentiation/implicit_equation.html#introduction",
    "title": "Functions Defined Implicitly by Equations (Part A)",
    "section": "Introduction",
    "text": "Introduction\nWe study the property of the function defined implicitly by an equation such as\nImplicit function: \\(f(p,x) = 0\\rightarrow x = s(p)\\).\nInversion: \\(f(x) = y \\rightarrow x = f^{-1}(y)\\).\nWe care about 1). when an implicit function or an inverse function exits, 2). has the properties like differentiability.\nAn example of quadratic function is shown below.\n\n\n\nQuadratic function. The inverse of it is not a function but part of it may be (point B) and may not be (point A)\n\n\nThe reflected graph above is not a function but can be regarded as a set-valued mapping. Then we are insterested in to which a graphical localization of a set-valued mapping might be a function and what properties it has.\nNoted that for the inverse of the above example, it assigns two different \\(x\\)s to \\(y\\) when \\(y&gt;0\\), no \\(x\\) to y when \\(y&lt;0\\) and one \\(x\\) to \\(y\\) when \\(y=0\\).\n\nSet-Valued Mappings\nFormally, the set-valued mapping \\(F\\) from \\(\\mathbb{R}^n \\rightrightarrows \\mathbb{R}^m\\) is \\[\nF: \\mathbb{R}^n \\rightrightarrows \\mathbb{R}^m\n\\] which assign to each \\(x\\in \\mathbb{R}^n\\) one (single-valued at \\(x\\)) or more elements (multi-valued at \\(x\\)) of \\(\\mathbb{R}^m\\), or possibly none (empty-valued at \\(x\\)).\nThe graph of \\(F\\) is the set \\[\n\\text{gph} F = \\{(x,y)\\in \\mathbb{R}^n \\times \\mathbb{R}^m: y\\in F(x)\\}.\n\\]\n\\(F\\) is a function such that it is single-valued at each \\(x\\) if \\(\\text{gph} F\\). In this case, \\(F: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) or \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\).\nEvery set-valued mapping has an inverse, namelt the set-valued mapping \\(F^{-1}\\) defined by \\[\nF^{-1}(y) = \\{x\\in \\mathbb{R}^n: y\\in F(x)\\}.\n\\]\nNote that 1. \\(F^{-1}\\) is another set-valued mapping. 2. All function \\(f\\) (as a set-valued mapping) has an inverse \\(f^{-1}\\) which is set-valued mapping. The question than becomes passing to some piece of the graph of \\(f^{-1}\\), which is related to the notion of graphical localization.\n!INFORMAL. Given a pair \\((\\bar{x},\\bar{y})\\in \\text{gph} F\\) and neighborhood \\(U\\) and \\(V\\) around \\(\\bar{x}\\) and \\(\\bar{y}\\), the graphical localization of \\(F\\) at \\(\\bar{x}\\) for \\(\\bar{y}\\) is a set-valued mapping \\(\\tilde{F}: U\\rightarrow V\\). The inverse of \\(\\tilde{F}\\) is the graphical localization of the set-valued mapping \\(F^{-1}\\) at \\(\\bar{y}\\) for \\(\\bar{x}\\), e.g. \\(\\tilde{F}^{-1}: V\\rightarrow U\\).\n!INFORMAL. By a single-valued localization of \\(F\\) at \\(\\bar{x}\\) for \\(\\bar{y}\\) will be meant a graphical localization that is a function.\nThe set-valued inverse \\(f^{-1}\\) of the quadratic function is single-valued at \\(y=0\\). However, it is not a single-valued localization at \\(y=0\\) for \\(x=0\\) (as it is not a function at all).\n\n\n\n\n\n\nSummary\n\n\n\nInformally, for set-valued mapping, it may be possible to define a single-valued localization at (or around) a point \\(\\bar{x}\\) to a point \\(\\bar{y}\\), which is a function.\n\n\n\n\nImplicit Functions\nFormally, the implicit function is defined as \\(f(p,x)=0\\) with the function \\(f: \\mathbb{R}^d \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) in which \\(p\\) acts as a parameter.\nThe solution mapping is \\[\nS: \\mathbb{R}^d \\rightrightarrows \\mathbb{R}^n \\text { with } S(p)=\\{x \\mid f(p, x)=0\\}\n\\]\nWe can then look at pairs \\((\\bar{p},\\bar{x}\\) in \\(\\text{gph} S\\) and ask whether \\(S\\) has a single-valued localization around \\(\\bar{p}\\) for \\(\\bar{x}\\) (this is the same as the inversion). Such as localization is an implicit function.\n\n\nFuture Discussion\nThe Dini’s classic implicit function theorem assumes that \\(f\\) is continuousely differentiable around \\((\\bar{p},\\bar{x})\\) and the partial Jacobian \\(\\nabla_xf(\\bar{p},\\bar{x})\\) is nonsingular (\\(m=n\\)). Then a single-valued localization \\(s\\) of \\(S\\) exists around \\(\\bar{p}\\) for \\(\\bar{x}\\) which is continuously differentiable around \\(\\bar{p}\\) with \\[\n\\nabla s(p) = -\\nabla_xf(p,s(p))^{-1}\\nabla_pf(p,s(p)).\n\\]\nLater we will depart from the assumption of continuousely differentiability of \\(f\\) and reformulate the assumption on the Jacobian \\(\\nabla_xf(\\bar{p},\\bar{x})\\) as an assumption on \\[\nh(x)=f(\\bar{p}, \\bar{x})+\\nabla_x f(\\bar{p}, \\bar{x})(x-\\bar{x})\n\\] with \\(h(\\bar{x}) = 0\\). The assumption on the invertibility of \\(\\nabla_xf(\\bar{p},\\bar{x})\\) will be replaced by the assumption that the inverse mapping \\(h^{-1}\\), with \\(\\bar{x}\\in h^{-1}(0)\\) has a single-valued localization around 0 for \\(\\bar{x}\\)."
  },
  {
    "objectID": "posts/learning/power_system/power_system_operation.html",
    "href": "posts/learning/power_system/power_system_operation.html",
    "title": "Power System Operation: AC and DC Power Flow Model",
    "section": "",
    "text": "This post is the first one in the series of power system operation. In this post, we will introduce the basic modelling method of power system operations. I feel this can be a great summary of the knowlege. Meanwhile, this series of post can be a reference to the open-source package power system operation I am developing.\nI mainly follow the modelling method from MATPOWER. A reference can be found by the MATPOWER Manual.\nMatrix form of the power flow model will be followed."
  },
  {
    "objectID": "posts/learning/power_system/power_system_operation.html#introduction",
    "href": "posts/learning/power_system/power_system_operation.html#introduction",
    "title": "Power System Operation: AC and DC Power Flow Model",
    "section": "",
    "text": "This post is the first one in the series of power system operation. In this post, we will introduce the basic modelling method of power system operations. I feel this can be a great summary of the knowlege. Meanwhile, this series of post can be a reference to the open-source package power system operation I am developing.\nI mainly follow the modelling method from MATPOWER. A reference can be found by the MATPOWER Manual.\nMatrix form of the power flow model will be followed."
  },
  {
    "objectID": "posts/learning/power_system/power_system_operation.html#ac-power-flow-model",
    "href": "posts/learning/power_system/power_system_operation.html#ac-power-flow-model",
    "title": "Power System Operation: AC and DC Power Flow Model",
    "section": "AC Power Flow Model",
    "text": "AC Power Flow Model\n\nBranch Model\nThe branch model is shown in the following figure.\n\n\n\nBranch Model\n\n\n\n\\(z_s = r_s + jx_s\\): series impedance.\n\\(\\tau, \\theta_{\\text{shift}}\\): transformer tap ratio magnitude and phase angle (in radians). The transformer is located at the from bus on a branch. If there is no transformer, \\(\\tau = 1\\) and \\(\\theta_{\\text{shift}} = 0\\).\n\nFor a single branch, \\[\n\\left[\\begin{array}{l}\ni_f \\\\\ni_t\n\\end{array}\\right]=Y_{b r}\\left[\\begin{array}{l}\nv_f \\\\\nv_t\n\\end{array}\\right]\n\\] where the branch admittance matrix \\(Y_{b r}\\) can be found by KCL law: \\[\nY_{b r}=\\left[\\begin{array}{cc}\n\\left(y_s+j \\frac{b_c}{2}\\right) \\frac{1}{\\tau^2} & -y_s \\frac{1}{\\tau e^{-j \\theta_{\\text {shif }}}} \\\\\n-y_s \\frac{1}{\\tau e^{j \\theta_{\\text {shift }}}} & y_s+j \\frac{b_c}{2}\n\\end{array}\\right]\n\\]\nNote that \\(Y_{b r}\\) is in general not symmetric unless \\(\\theta_{\\text{shift}} = 0\\).\nLet’s denote the four elements of branch \\(i\\) as \\[\nY_{b r}^i=\\left[\\begin{array}{cc}\ny_{f f}^i & y_{f t}^i \\\\\ny_{t f}^i & y_{t t}^i\n\\end{array}\\right]\n\\]\nThe branch number of the four elements can be summarized into vectors \\(Y_{ff}\\), \\(Y_{ft}\\), \\(Y_{tf}\\), \\(Y_{tt}\\).\nMeanwhile, the from-side and to-side incidence matrix \\(C_f\\) and \\(C_t\\) are deifned such that the \\((i,j)\\) entry of \\(C_f\\) and the \\((i,k)\\) entry of \\(C_t\\) are 1 if branch \\(i\\) is connected from bus \\(j\\) to \\(k\\), respectively, and 0 otherwise. The branch-to-bus incidence matrix \\(A = C_f - C_t\\).\n\n\nGenerator Model\nThe generator complex power injection can be written as \\[\nS_g = P_g + jQ_g\n\\]\nThe generator incidence matrix \\(C_g\\) is defined such that the \\((i,j)\\) entry of \\(C_g\\) is 1 if generator \\(j\\) is connected to bus \\(i\\), and 0 otherwise. Therefore, its contribution to bus (nodal) power injection is \\[\nS_{g,\\text{bus}} = C_g S_g\n\\]"
  },
  {
    "objectID": "posts/learning/power_system/power_system_operation.html#ac-model",
    "href": "posts/learning/power_system/power_system_operation.html#ac-model",
    "title": "Power System Operation: AC and DC Power Flow Model",
    "section": "AC Model",
    "text": "AC Model\n\nBranch Model\nThe branch model is shown in the following figure.\n\n\n\nBranch Model\n\n\n\n\\(z_s = r_s + jx_s\\): series impedance.\n\\(\\tau, \\theta_{\\text{shift}}\\): transformer tap ratio magnitude and phase angle (in radians). The transformer is located at the from bus on a branch. If there is no transformer, \\(\\tau = 1\\) and \\(\\theta_{\\text{shift}} = 0\\).\n\nFor a single branch, \\[\n\\left[\\begin{array}{l}\ni_f \\\\\ni_t\n\\end{array}\\right]=Y_{b r}\\left[\\begin{array}{l}\nv_f \\\\\nv_t\n\\end{array}\\right]\n\\] where the branch admittance matrix \\(Y_{b r}\\) can be found by KCL law: \\[\nY_{b r}=\\left[\\begin{array}{cc}\n\\left(y_s+j \\frac{b_c}{2}\\right) \\frac{1}{\\tau^2} & -y_s \\frac{1}{\\tau e^{-j \\theta_{\\text {shif }}}} \\\\\n-y_s \\frac{1}{\\tau e^{j \\theta_{\\text {shift }}}} & y_s+j \\frac{b_c}{2}\n\\end{array}\\right]\n\\tag{1}\\]\nNote that \\(Y_{b r}\\) is in general not symmetric unless \\(\\theta_{\\text{shift}} = 0\\).\nLet’s denote the four elements of branch \\(i\\) as \\[\nY_{b r}^i=\\left[\\begin{array}{cc}\ny_{f f}^i & y_{f t}^i \\\\\ny_{t f}^i & y_{t t}^i\n\\end{array}\\right]\n\\]\nThe branch number of the four elements can be summarized into vectors \\(Y_{ff}\\), \\(Y_{ft}\\), \\(Y_{tf}\\), \\(Y_{tt}\\).\nMeanwhile, the from-side and to-side incidence matrix \\(C_f\\) and \\(C_t\\) are deifned such that the \\((i,j)\\) entry of \\(C_f\\) and the \\((i,k)\\) entry of \\(C_t\\) are 1 if branch \\(i\\) is connected from bus \\(j\\) to \\(k\\), respectively, and 0 otherwise. The branch-to-bus incidence matrix \\(A = C_f - C_t\\).\n\n\nGenerator Model\nThe generator complex power injection can be written as \\[\nS_g = P_g + jQ_g\n\\]\nThe generator incidence matrix \\(C_g\\) is defined such that the \\((i,j)\\) entry of \\(C_g\\) is 1 if generator \\(j\\) is connected to bus \\(i\\), and 0 otherwise. Therefore, its contribution to bus (nodal) power injection is \\[\nS_{g,\\text{bus}} = C_g S_g\n\\]\nOther type of generators, such as solar or wind renewables can be defined in the same way. The constant renewable power injection can also be viewed as negative load.\n\n\nLoad Model\nA constant power load is modeled as active and reactive power consumption at each bus. The load complex power injection can be written as \\[\nS_d = P_d + jQ_d\n\\]\n\n\nShunt Elements\nA shunt=connected element, such as a capacitor or an inductor, is modeled as a fixed impedance to ground at a bus, whose admittance is \\[\nY_{sh} = G_{sh} + jB_{sh}\n\\]\n\n\nNetwork Equations\nLet \\(V\\) be the bus voltage and \\(I_{\\text{bus}}\\) be the bus current injection. \\[\n\\begin{aligned}\nI_{\\text{bus}} = Y_{\\text{bus}} V \\\\\nI_f = Y_{f} V \\\\\nI_t = Y_{t} V\n\\end{aligned}\n\\] with the system admittance matrices defined as \\[\n\\begin{aligned}\nY_f & =\\left[Y_{f f}\\right] C_f+\\left[Y_{f t}\\right] C_t, \\\\\nY_t & =\\left[Y_{t t}\\right] C_f+\\left[Y_{t t}\\right] C_t, \\\\\nY_{\\text {bus }} & =C_f^{\\top} Y_f+C_t^{\\top} Y_t+\\left[Y_{s h}\\right] .\n\\end{aligned}\n\\] where \\([\\cdot]\\) denotes the diagonal matrix of the vector.\nIn detail, we have the bus current injection as \\[\nI_{\\text{bus}} = C_f^T[Y_{ff}]C_f V + C_f^T[Y_{ft}]C_t V + C_t^T[Y_{tf}]C_f V + C_t^T[Y_{tt}]C_t V + [Y_{sh}]V\n\\]\nTo understand this, using the first term as an example, \\(C_fV\\) is the voltage at the from bus of the branch, \\([Y_{ff}]C_fV\\) is the current flowing at the from bus of the branch, e.g. the \\(I_f\\). \\(C_f^T\\) is the transpose of \\(C_f\\), which is the incidence matrix of the branches connected to the bus.\nThen the complex power injection and flows can be written as (the power flow equation) \\[\n\\begin{aligned}\nS_{\\text {bus }}(V) & =[V] I_{\\text {bus }}^*=[V] Y_{\\text {bus }}^* V^*, \\\\\nS_f(V) & =\\left[C_f V\\right] I_f^*=\\left[C_f V\\right] Y_f^* V^*, \\\\\nS_t(V) & =\\left[C_t V\\right] I_t^*=\\left[C_t V\\right] Y_t^* V^* .\n\\end{aligned}\n\\] where \\((\\cdot)^\\star\\) is the element-wise conjugate operator on complex number. Note that \\((AB)^\\star = A^\\star B^\\star\\).\nThe bus injection can be written as (power injection balance) \\[\ng_S\\left(V, S_g\\right)=S_{\\text {bus }}(V)+S_d-C_g S_g=0 .\n\\]"
  },
  {
    "objectID": "posts/learning/power_system/power_system_operation.html#dc-model",
    "href": "posts/learning/power_system/power_system_operation.html#dc-model",
    "title": "Power System Operation: AC and DC Power Flow Model",
    "section": "DC Model",
    "text": "DC Model\nThe DC model takes three assumption on the AC model:\n\nThe voltage magnitude is fixed at 1 p.u., e.g., \\(v_i = e^{j\\theta_i}\\).\nThe branches are lossless, e.g., \\(y_s = 0\\). The line charging susceptance \\(b_c\\) is also ignored. Therefore, the branch admittance is \\(y_s = \\frac{1}{jx_s}\\).\nThe voltage angle difference on each branch is small, e.g., \\(\\sin(\\theta_f - \\theta_t - \\theta_{\\text{shift}}) \\approx \\theta_f - \\theta_t - \\theta_{\\text{shift}}\\).\n\nBased on the three assumptions, the branch admittance matrix \\(Y_{b r}\\) Equation 1 can be simplified as \\[\nY_{b r} \\approx \\frac{1}{j x_s}\\left[\\begin{array}{cc}\n\\frac{1}{\\tau^2} & -\\frac{1}{\\tau e^{-j \\theta_{\\text {shift }}}} \\\\\n-\\frac{1}{\\tau e^{j \\theta_{\\text {shift }}}} & 1\n\\end{array}\\right]\n\\]\nLet \\(b_i = \\frac{1}{x_s^i\\tau^i}\\) and \\(B_{ff}\\) be the vector of \\(b_i\\) for all branches. Let \\(P_{f,\\text{shift}}\\) be the vector of \\(-\\theta_{\\text{shift}}^ib_i\\). Then the bus power injection can be written as \\[\nP_{\\text{bus}} = B_{\\text{bus}}(\\Theta) + P_{\\text{bus,shift}}\n\\] where \\[\nP_{\\text {bus,shift }}=\\left(C_f-C_t\\right)^{\\top} P_{f, \\text { shift }}\n\\]\nThe power flow equation can be written as \\[\nP_f(\\Theta)=B_f \\Theta+P_{f, \\text { shift }} = -P_t(\\Theta)\n\\]\nThe DC-model system matrices can be written as \\[\n\\begin{aligned}\nB_f & =\\left[B_{f f}\\right]\\left(C_f-C_t\\right), \\\\\nB_{\\text {bus }} & =\\left(C_f-C_t\\right)^{\\top} B_f .\n\\end{aligned}\n\\]\nThe bus power injection balance can be written as \\[\ng_P\\left(\\Theta, P_g\\right)=B_{\\mathrm{bus}} \\Theta+P_{\\mathrm{bus}, \\text { shift }}+P_d+G_{s h}-C_g P_g=0\n\\]"
  },
  {
    "objectID": "posts/learning/autometic_differentiation/deep_implicit_layers.html",
    "href": "posts/learning/autometic_differentiation/deep_implicit_layers.html",
    "title": "Deep Implicit Layers: Fixed-Point Iteration",
    "section": "",
    "text": "In the previous posts, I summarize the mathematical background of the adjoint method for linear system and nonlinear system. This post will summarize them in the view of deep implicit layers. The main reference of this post is Deep Implicit Layers.\nThe nonlinear equation \\(g(x,z)=0\\) can be viewed beyond the a simple algebraic equation. Similiar to the sensitivity analysis of the equation, we can design the neural network as an implicit function of the parameter and the solution. The gradinet used for backpropagation can be found by the same idea of sensitivity analysis in the previous posts. This means that we can 1. Encode the implicit layer with physical meaning as part of the neural network. 2. Regard the entire neural network or part of it as a implicit model. For example, ResNet can be viewed as NeuralODE and feedforward neural network can be viewed as deep equilibrium model.\n\nImplicit layers for different types of equations.\n\n\n\n\n\n\nEquation Type\nNeural Network\n\n\n\n\nAlgebraic equation (fixed point iteration)\nDeep equilibrium model\n\n\nOrdinary differential equation\nNeuralODE\n\n\nConvex optimization\nDifferentiable convex layer\n\n\n\nThe benefits of using implicit layers are 1. The solution can be found by off-the-shelf solver, regardless of the the layer itself. E.g., the fixed point iteration can be solved by Newton’s method; the ODE can be solved by the ODE solver such as Euler’s method; the convex optimization can be solved by the convex optimization solver, such as ADMM. And more. 2. Because the solution procedure is separated from the layer, it does not need to be recorded on the computational graph (although the solution procedure can be unrolled on the computatonal graph). This improves the memory efficiency and numetical stability. 3. Because the forward pass of implicit layer requires a solution procedure which is usually an iterative process (thus repeated nonlinearity), the representation power of the implicit layer is stronger than the explicit layer."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/deep_implicit_layers.html#introduction",
    "href": "posts/learning/autometic_differentiation/deep_implicit_layers.html#introduction",
    "title": "Deep Implicit Layers: Fixed-Point Iteration",
    "section": "",
    "text": "In the previous posts, I summarize the mathematical background of the adjoint method for linear system and nonlinear system. This post will summarize them in the view of deep implicit layers. The main reference of this post is Deep Implicit Layers.\nThe nonlinear equation \\(g(x,z)=0\\) can be viewed beyond the a simple algebraic equation. Similiar to the sensitivity analysis of the equation, we can design the neural network as an implicit function of the parameter and the solution. The gradinet used for backpropagation can be found by the same idea of sensitivity analysis in the previous posts. This means that we can 1. Encode the implicit layer with physical meaning as part of the neural network. 2. Regard the entire neural network or part of it as a implicit model. For example, ResNet can be viewed as NeuralODE and feedforward neural network can be viewed as deep equilibrium model.\n\nImplicit layers for different types of equations.\n\n\n\n\n\n\nEquation Type\nNeural Network\n\n\n\n\nAlgebraic equation (fixed point iteration)\nDeep equilibrium model\n\n\nOrdinary differential equation\nNeuralODE\n\n\nConvex optimization\nDifferentiable convex layer\n\n\n\nThe benefits of using implicit layers are 1. The solution can be found by off-the-shelf solver, regardless of the the layer itself. E.g., the fixed point iteration can be solved by Newton’s method; the ODE can be solved by the ODE solver such as Euler’s method; the convex optimization can be solved by the convex optimization solver, such as ADMM. And more. 2. Because the solution procedure is separated from the layer, it does not need to be recorded on the computational graph (although the solution procedure can be unrolled on the computatonal graph). This improves the memory efficiency and numetical stability. 3. Because the forward pass of implicit layer requires a solution procedure which is usually an iterative process (thus repeated nonlinearity), the representation power of the implicit layer is stronger than the explicit layer."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/deep_implicit_layers.html#fixed-point-iteration",
    "href": "posts/learning/autometic_differentiation/deep_implicit_layers.html#fixed-point-iteration",
    "title": "Deep Implicit Layers: Fixed-Point Iteration",
    "section": "Fixed Point Iteration",
    "text": "Fixed Point Iteration\nA fixed point iteration \\[\nz^{\\star}=\\tanh \\left(W z^{\\star}+x\\right)\n\\] can be written as a nonlinear equation \\[\ng(x, z)=z-\\tanh \\left(W z+x\\right)=0\n\\] where \\(W\\in\\mathbb{R}^{n\\times n}\\).\n\nForward Pass\nFor a given \\(x\\), Newton’s method can be used to iteratively solve the equation, \\[\nz:= z - \\left(\\frac{\\partial g}{\\partial z}\\right)^{-1} g(x,z)\n\\]\nThe partial Jacobian of \\(g\\) with repect to \\(z\\) can be found by automatic differentation tool such as torch.func.jacfwd() in general. For the simple case, it can be found analytically as \\[\n\\frac{\\partial g}{\\partial z} = I - \\text{diag}(\\tanh'(Wz+x))W\n\\]\nNote that the forward pass is not recorded on the computational graph.\n\n\nBackward Pass\nWe can use the same technique in nonlinear adjoint method to do the reverse mode differentiation or bachpropagation. To have a quick review, do the total derivative of \\(g\\) with respect to \\(x\\): \\[\n\\frac{d g}{d x} = \\frac{\\partial g}{\\partial x} + \\frac{\\partial g}{\\partial z} \\frac{d z}{d x} = 0\n\\]\nThe Jacobian \\(\\frac{d z}{d x}\\) can be found as \\[\n\\frac{d z}{d x} = -\\left(\\frac{\\partial g}{\\partial z}\\right)^{-1} \\frac{\\partial g}{\\partial x}\n\\]\nNote that the term \\(\\frac{\\partial g}{\\partial x}\\) has already been computed in the forward pass, e.g. we can directly use the Jacobian and its inverse calculated in the last iteration of Newton’s method.\n\n\n\n\n\n\nTip\n\n\n\nThis is a very common observation in various implicit layer application where you can do the reverse-mode differentiation for ‘free’.\n\n\nDirectly solving the Jacobian \\(\\frac{d z}{d x}\\) is not efficient. As in the Tangent method, the number of linear systems that need to solve is the same as the number of the parameters \\(x\\).\nModern deep learning model is trained by reverse-mode differentiation, which is more efficient than the forward-mode differentiation. Let \\(\\ell(\\cdot)\\) be the scalar loss function, the Jacobian with respect to \\(x\\) can be found by \\[\n\\frac{d \\ell}{d x} = \\frac{d \\ell}{d z} \\frac{d z}{d x} = \\frac{d \\ell}{d z} \\left(-\\left(\\frac{\\partial g}{\\partial z}\\right)^{-1} \\frac{\\partial g}{\\partial x}\\right)\n\\]\nAgain we can find the vector Jacobian product of the first two terms as the new adjoint of \\(z\\) denoted as \\(\\dot{z}\\): \\[\n\\left(\\frac{\\partial g}{\\partial z}\\right)^T \\dot{z} = \\left(\\frac{d \\ell}{d z}\\right)^T\n\\tag{1}\\]\nThen we need to re-engage \\(x\\) on the computational graph. This can be done by calculating \\[\nz:= z - g(x,z)\n\\] whose gradient with respect to \\(x\\) is \\(-\\frac{\\partial g}{\\partial x}\\), which is the last term in \\(\\frac{d\\ell}{dx}\\).\n(The minus sign depends how the adjoint system is defined.)\nTo sum up, the process of forward and backward pass of fixed-point iteration is\n\nForward pass: Solve the nonlinear equation \\(g(x,z)=0\\) by off-the-shelf solver. This is outside the automatic differentiation tape.\nIn the automatic differentiation tape, re-engage \\(x\\) on the computational graph by \\(z:= z - g(x,z)\\).\nModify the gradient of the above \\(z\\) as the solution to Equation 1, using the register_hook() method in PyTorch."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/deep_implicit_layers.html#pytorch-implementation",
    "href": "posts/learning/autometic_differentiation/deep_implicit_layers.html#pytorch-implementation",
    "title": "Deep Implicit Layers: Fixed-Point Iteration",
    "section": "PyTorch Implementation",
    "text": "PyTorch Implementation\nHere, I implement a simple fixed-point iteration layer in PyTorch and compare it to the previous method in the nonlinear adjoint method.\nimport torch\nfrom torch import nn\n\ndef loss_fn(z):\n    return torch.sum(z**2, axis = -1).mean()\n\nclass FixedPointLayer(torch.nn.Module):\n    def __init__(self, W, tol = 1e-4, max_iter = 1):\n        super(FixedPointLayer, self).__init__()\n        self.W = torch.nn.Parameter(W, requires_grad = True)\n        self.tol = tol\n        self.max_iter = max_iter\n        # implement by vmap\n        self.implicit_model = torch.vmap(self.implicit_model_)\n        self.jac_batched = torch.vmap(torch.func.jacfwd(self.implicit_model_, argnums = 0))\n\n    def implicit_model_(self, z, x):\n        return z - torch.tanh(self.W @ z + x)\n    \n    def newton_step(self, z, x, g):\n        J = self.jac_batched(z, x)\n        z = z - torch.linalg.solve(J, g)\n        return z, J\n\n    def forward(self, x):\n        self.iteration = 0\n        with torch.no_grad():\n            z = torch.tanh(x)\n            while self.iteration &lt; self.max_iter:\n                g = self.implicit_model(z, x)\n                self.err = torch.norm(g)\n\n                if self.err &lt; self.tol:\n                    break\n\n                # newton's method\n                z, J = self.newton_step(z, x, g)\n                self.iteration += 1\n        \n        # re-engage the autograd tape\n        z = z - self.implicit_model(z, x)\n        z.register_hook(lambda grad : torch.linalg.solve(J.transpose(1,2), grad))\n\n        return z\n\ndef implicit_model(W, x, z):\n    # the g function\n    return z - torch.tanh(W @ z + x)\n\n\ndef implicit_model_test(W, x, z):\n\n    if x.dim() == 1:\n        # single sample case\n        print('using the implicit model on one sample')\n        z_ = z.clone().detach()\n        x_ = x.clone().detach()\n        \n        dl_dz = torch.func.grad(loss_fn)(z_)\n        df_dW, df_dz = torch.func.jacfwd(implicit_model, argnums = (0,2))(W, x_, z_)\n        \n        adjoint_variable = torch.linalg.solve(df_dz.T, -dl_dz)\n        \n        dl_dW = torch.einsum('i,ikl-&gt;kl', adjoint_variable, df_dW)\n    \n    else:\n        print('using the implicit model on all samples')\n        z = z.clone().detach()\n        x = x.clone().detach()\n        \n        dl_dz = torch.func.grad(loss_fn)(z)\n\n        jacfwd_batched = torch.vmap(torch.func.jacfwd(implicit_model, argnums = (0,2)), in_dims = (None, 0, 0))\n        df_dW, df_dz = jacfwd_batched(W, x, z)\n\n        adjoint_variable = torch.linalg.solve(df_dz.transpose(1,2), -dl_dz)\n\n        dl_dW = torch.einsum('bi,bikl-&gt;kl', adjoint_variable, df_dW)\n    \n    print('dl_dz', dl_dz.shape)\n    print('df_dW', df_dW.shape)\n    print('df_dz', df_dz.shape)\n    print('adjoint_variable', adjoint_variable.shape)\n    print('dl_dW', dl_dW)\n\n# maint function\ntorch.random.manual_seed(0)\n\nbatch_size = 10\nn = 5\nW = torch.randn(n,n).double() * 0.5\nx = torch.randn(batch_size,n, requires_grad=True).double()\n\nprint('using the model')\nmodel = FixedPointLayer(W, tol=1e-10, max_iter = 50).double()\n\n# check with the numerical gradient\ntorch.autograd.gradcheck(model, x, check_undefined_grad=False, raise_exception=True)\n\nz = model(x)\nloss = loss_fn(z)\nloss.backward()\nprint(model.W.grad)\n\n# implicit model method\nimplicit_model_test(W, x[0], z[0])\nimplicit_model_test(W, x, z)\nIn the above example, torch.vmap() is used for multi-batch implementation of the implicit function and the Jacobian. The torch.func.jacfwd() is used for the Jacobian calculation. Note that the Jacobian \\(\\frac{\\partial g}{\\partial z}\\) is found by automatic differentiation, instead of analytical computation. The torch.linalg.solve() is used for the linear system solution. The torch.einsum() is used for the matrix multiplication.\nIn detail, in\njacfwd_batched = torch.vmap(torch.func.jacfwd(implicit_model, argnums = (0,2)), in_dims = (None, 0, 0)\nThe torch.func.jacfwd(implicit_model, argnums = (0,2)) is to find the Jacobian of the implicit model with respect to \\(W\\) and \\(z\\). The torch.vmap() is to apply the torch.func.jacfwd() to each batch of the input. The in_dims = (None, 0, 0) is to specify that only z and x are batched while W is not."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/differetiable_convex_layer.html",
    "href": "posts/learning/autometic_differentiation/differetiable_convex_layer.html",
    "title": "Deep Implicit Layers: Differentiable Convex Layer",
    "section": "",
    "text": "In this post, I summarize my learning note on another type of implicit model - the differentiable convex layer. This layer has similar formulation, forward, and backward passes as the fixed point layer. This is because its implicit function can be formulated by the KKT condition, no matter which off-the-shelf optimization solver is used. This topic is related to the tangent and adjoint sensitivity analysis of nonlinear system as the KKT condition forms a nonlinear system in general. The main reference is the paper Differentiable Convex Optimization Layers by Amos et al. (2019) and the blog: differentiable optimization."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/differetiable_convex_layer.html#introduction",
    "href": "posts/learning/autometic_differentiation/differetiable_convex_layer.html#introduction",
    "title": "Deep Implicit Layers: Differentiable Convex Layer",
    "section": "",
    "text": "In this post, I summarize my learning note on another type of implicit model - the differentiable convex layer. This layer has similar formulation, forward, and backward passes as the fixed point layer. This is because its implicit function can be formulated by the KKT condition, no matter which off-the-shelf optimization solver is used. This topic is related to the tangent and adjoint sensitivity analysis of nonlinear system as the KKT condition forms a nonlinear system in general. The main reference is the paper Differentiable Convex Optimization Layers by Amos et al. (2019) and the blog: differentiable optimization."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/differetiable_convex_layer.html#formulation-on-convex-optimization-layer",
    "href": "posts/learning/autometic_differentiation/differetiable_convex_layer.html#formulation-on-convex-optimization-layer",
    "title": "Deep Implicit Layers: Differentiable Convex Layer",
    "section": "Formulation on Convex Optimization Layer",
    "text": "Formulation on Convex Optimization Layer\nConsider the following optimization proble: \\[\n\\begin{aligned}\n\\underset{z}{\\operatorname{minimize}} & f(z) \\\\\n\\text { subject to } & g(z) \\leq 0 \\\\\n& h(z)=0,\n\\end{aligned}\n\\]\nTo make the optimization problem convex, we assume that \\(f(z)\\) is convex, \\(g(z)\\) is convex, and \\(h(z)\\) is affine. The KKT condition of this optimization problem is \\[\n\\begin{aligned}\ng\\left(z^{\\star}\\right) & \\leq 0 \\\\\nh\\left(z^{\\star}\\right) & =0 \\\\\n\\lambda^{\\star} & \\geq 0 \\\\\n\\lambda^{\\star} \\circ g\\left(z^{\\star}\\right) & =0 \\\\\n\\nabla f\\left(z^{\\star}\\right)+\\sum_{i=1}^m \\lambda_i^{\\star} \\nabla g_i\\left(z^{\\star}\\right)+\\sum_{i=1}^p \\nu_i^{\\star} \\nabla h_i\\left(z^{\\star}\\right) & =0\n\\end{aligned}\n\\]\nSimilarly to the previous parametric setting, we can define the convex layer as \\[\n\\begin{aligned}\nz^{\\star}(x)=\\underset{z}{\\operatorname{argmin}} & f(z, x) \\\\\n\\text { subject to } & g(z, x) \\leq 0 \\\\\n& h(z, x)=0\n\\end{aligned}\n\\] where \\(z\\) is the decision variable and \\(x\\) is the input of the layer or the parameter of the optimization problem. Apart from \\(z^\\star(x)\\) (as an implicit function of \\(x\\)), we can also include the dual variables, \\[\n(z^\\star, \\lambda^\\star, \\nu^\\star)(x)\n\\]\nThe differentiable convex layer can be viewed as 1) a nonlinear implicit equation \\(G(x,\\lambda,\\nu,x) = 0\\) and 2) a fixed point iteration as iterative optimization algorithm (e.g., interior point, SQP, ADMM, etc) can be used to solve the optimization problem.\nThe equality condition KKT can be written as: \\[\nG(z, \\lambda, \\nu,x)=\\left[\\begin{array}{c}\n\\frac{\\partial}{\\partial z} f(z, x)+\\frac{\\partial}{\\partial z} g(z, x)^T \\lambda+\\frac{\\partial}{\\partial z} h(z, x)^T \\nu \\\\\n\\lambda \\circ g(z,x) \\\\\nh(z,x)\n\\end{array}\\right]\n\\] where \\(\\circ\\) denotes the element-wise product.\nAssume that the optimal primal and dual pair has been found by the forward pass. At the optimal point, we have \\[\nG(z^\\star(x), \\lambda^\\star(x), \\nu^\\star(x), x) = 0\n\\]\n\nGeneral Formulation\nDenote \\(y(x) = (z^\\star(x), \\lambda^\\star(x), \\nu^\\star(x))\\), then we can directly use the method implemented in nonlinear equation sensitivity analysis to compute the gradient of \\(y(x)\\) with respect to \\(x\\). Do the total derivative with respect to \\(x\\): \\[\n\\frac{d G}{d x} = \\frac{\\partial G}{\\partial y}\\frac{d y}{d x} + \\frac{\\partial G}{\\partial x} = 0\n\\]\nTherefore, the Jacobian is found as \\[\n\\frac{d y}{d x} = -\\left(\\frac{\\partial G}{\\partial y}\\right)^{-1} \\frac{\\partial G}{\\partial x}\n\\]\nIn the backward pass, the adjoint of the optimal point \\(\\dot{y}\\) is computed backpropagated by the scalar loss \\(\\ell(\\cdot)\\) as \\(\\dot{y} = \\frac{\\partial \\ell}{\\partial y}\\). Note that when \\(\\ell(\\cdot)\\) does not depend on the primal and dual variables, the associated adjoint is zero.\nConsequently, \\[\n\\frac{d y}{d x} = - \\frac{d \\ell}{d y} \\left(\\frac{\\partial G}{\\partial y}\\right)^{-1} \\frac{\\partial G}{\\partial x}\n\\]\nThe same treatment as in fixed point iteration can be used to compute the above gradient. In detail, the first two items form the adjoint equation of \\(y\\) whose adjoint (or gradient) needs to be modified. \\[\n\\left(\\frac{\\partial G}{\\partial y}\\right)^T \\dot{y} = \\frac{\\partial G}{\\partial x}\n\\tag{1}\\]\n\n\nDetailed Formulation\nThe partial Jacobian \\(\\frac{\\partial G}{\\partial y}\\) can be analytically computed as \\[\n\\frac{\\partial}{\\partial z, \\lambda, \\nu} G\\left(z^{\\star}, \\lambda^{\\star}, \\nu^{\\star}, x\\right)=\n\\left[\\begin{array}{ccc}\n\\frac{\\partial^2}{\\partial z^2} f\\left(z^{\\star}, x\\right)+\\sum_{i=1}^m \\lambda_i^{\\star} \\frac{\\partial^2}{\\partial z^2} g_i\\left(z^{\\star}, x\\right) & \\frac{\\partial}{\\partial z} g\\left(z^{\\star}, x\\right) & \\frac{\\partial}{\\partial z} h\\left(z^{\\star}, x\\right) \\\\\n\\frac{\\partial}{\\partial z} g\\left(z^{\\star}, x\\right)^T \\operatorname{diag}\\left(\\lambda^{\\star}\\right) & \\operatorname{diag}\\left(g\\left(z^{\\star}, x\\right)\\right) & 0 \\\\\n\\frac{\\partial}{\\partial z} h\\left(z^{\\star}, x\\right)^T & 0 & 0\n\\end{array}\\right]\n\\]\n\n\nQuadratic Programming Formulation\nIn the OptNet paper, the author implemented the differentiable QP layer. In this section, the explicit formulations are derived. We will follow the same notation in the\nConsider the following QP problem: \\[\n\\begin{aligned}\n\\min_z & \\quad \\frac{1}{2} z^T Q z + q^T z \\\\\n\\text{s.t.} & \\quad A z = b \\\\\n& \\quad G z \\leq h\n\\end{aligned}\n\\] where the parameters \\(Q, q, A, b, C, h\\) are functions of the input \\(z_i\\) (the term \\(z_i\\) is used as in the paper this is referred to the output from the previous layer). Our goal is to compute the Jacobian of \\(\\frac{d \\ell}{d z_i}\\).\nThe KKT condition (only the equality part) of the above QP can be written as \\[\n\\begin{aligned}\nQz^\\star + q + A^T \\nu^\\star + C^T \\lambda^\\star &= 0 \\\\\nAz^\\star - b &= 0 \\\\\nD(\\lambda^\\star) (Gz^\\star - h) &= 0\n\\end{aligned}\n\\] where \\(D(\\lambda^\\star)\\) is a diagonal matrix with the diagonal elements being \\(\\lambda^\\star\\).\nDifferentiating the KKT equations with respect to the input \\(z_i\\) gives \\[\n\\begin{aligned}\n\\frac{dQ}{dz_i}z^\\star + Q\\frac{dz^\\star}{dz_i} + \\frac{dq}{dz_i} + \\frac{dA^T}{dz_i}\\nu^\\star + A^T \\frac{d\\nu^\\star}{dz_i} + \\frac{dG^T}{dz_i}\\lambda^\\star + G^T \\frac{d\\lambda^\\star}{dz_i} &= 0 \\\\\n\\frac{dA}{dz_i}z^\\star + A\\frac{dz^\\star}{dz_i} - \\frac{db}{dz_i} &= 0 \\\\\nD(Gz^\\star - h)\\frac{dz^\\star}{dz_i} + D(\\lambda^\\star)\\left(\\frac{dG}{dz_i}z^\\star + G\\frac{dz^\\star}{dz_i} - \\frac{dh}{dz_i} \\right) &=0\n\\end{aligned}\n\\] which can be compactly written as matrix form \\[\n\\begin{bmatrix}\nQ & G^T & A^T \\\\\nD(\\lambda^\\star)G & D(Gz^\\star - h) & 0 \\\\\nA & 0 & 0\n\\end{bmatrix} \\begin{bmatrix} \\frac{dz}{dz_i} \\\\ \\frac{d\\lambda}{dz_i} \\\\ \\frac{d\\nu}{dz_i} \\end{bmatrix} = -\\begin{bmatrix} \\frac{dQ}{dz_i}z^\\star + \\frac{dq}{dz_i} + \\frac{dA^T}{dz_i}\\nu^\\star + \\frac{dG^T}{dz_i}\\lambda \\\\ D(\\lambda^\\star)\\frac{dG}{dz_i}z^\\star - D(\\lambda^\\star)\\frac{dh}{dz_i} \\\\ \\frac{dA}{dz_i}z - \\frac{db}{dz_i} \\end{bmatrix}\n\\]\nThe paper also suggest using the adjoint method to compute the gradient of the loss with respect to the input \\(z_i\\).\nThe paper also finds out that the above matrix (that needs to be inverted when solving the adjoint equation) has been already computed/factorized during the forward pass using the interior point method. Therefore, the computational cost of backpropagation is ‘almost free’. This finding is similar to the fixed point iteration where the Jacobian of the fixed point iteration is already computed during the forward pass using the Newton’s method.\n\n\nSteps\nSimilar to the fixed point iteration. The steps of implementing the differentiable convex layer are:\n\nOutside the gradient tape, solve the optimization problem to find the optimal primal and dual variables \\((z^\\star, \\lambda^\\star, \\nu^\\star)\\) using any off-the-shelf optimization solver, such as interior point, SQP, ADMM, etc.\nInside the gradient tape, engage the input \\(x\\) to the computation graph by \\((z,\\lambda,\\nu):= (z^\\star,\\lambda^\\star,\\nu^\\star) - G(z^\\star,\\lambda^\\star,\\nu^\\star,x)\\). This will provide the the gradient \\(-\\frac{d G}{d x}\\) in the computation graph.\nRegister the gradient of \\((z,\\lambda,\\nu)\\) with the solution of the linear adjoint system Equation 1.\n\n\n\nPytorch Implementation\nAn example code snippet, including the an implementation of OptNet and comparison to CvxpyLayers can be found at my-github."
  },
  {
    "objectID": "posts/learning/power_system/ncuc.html",
    "href": "posts/learning/power_system/ncuc.html",
    "title": "Unit Commitment Problem",
    "section": "",
    "text": "In the previous post Power System Operation: AC and DC Power Flow Model, we discuss how to model the power system in the steady-state for both AC and DC formats. In this post, unit commitment (UC) problem and econimic dispatch (ED) will be formulated. The UC problem is usually solved in day-ahead manner to decide the on/off conditions and the set-points of the generator. It includes 1). the simplest UC problem without binary variables, 2). the UC problem with binary variables. Both problem considers the network constraints and reserve requirements. The ED problem focuses more on ‘real-time’ balance of the load and generation and meet the physical constraints and safety requirements.\nNote that the UC and ED can be formulated in different ways, including stochastic, robust, and chance-constrained formats. In this post, we will focus on the deterministic formulation.\nThe main reference is Conejo’s lecture notes."
  },
  {
    "objectID": "posts/learning/power_system/ncuc.html#unit-commitment-without-binary-variables",
    "href": "posts/learning/power_system/ncuc.html#unit-commitment-without-binary-variables",
    "title": "Unit Commitment Problem",
    "section": "Unit Commitment Without Binary Variables",
    "text": "Unit Commitment Without Binary Variables\nThe UC without binary varibales can be formulated as linear programming (LP) problem as follows:\n\\[\n\\begin{aligned}\n\\min_{P_g, \\theta, P_{ls}, P_{sc}, P_{wc}} & \\sum_{t=1}^T c_{gv}^TP_g(t) + c_{ls}^TP_{ls}(t) + c_{sc}^TP_{sc}(t) + c_{wc}^TP_{wc}(t) \\\\\n\\text{s.t.} & P_g^{\\text{min}} \\leq P_g(t) \\leq P_g^{\\text{max}} \\\\\n& -R_{\\text{down}} \\leq P_g(t) - P_g(t-1) \\leq R_{\\text{up}} \\\\\n& -P_f^{\\text{max}} \\leq B_f\\theta(t) + P_{f,\\text{shift}} \\leq P_f^{\\text{max}} \\\\\n& C_gP_g(t) + C_s(P_s(t) - P_{sc}(t)) + C_w(P_w(t) - P_{wc}(t)) - C_l(P_l(t) - P_{ls}(t)) \\\\\n&  \\qquad = B_{\\text{bus}}\\theta(t) + P_{\\text{bus,shift}} \\\\\n& \\theta_{\\text{ref}}(t) = \\theta_0, \\quad \\forall t=1,2,\\ldots,T \\\\\n& \\sum P_g^{\\text{max}} \\geq \\sum_{i=1}^TP_g(t) + r(t) \\\\\n& 0 \\leq P_{ls}(t) \\leq P_l(t), \\quad \\forall t=1,2,\\ldots,T \\\\\n& 0 \\leq P_{sc}(t) \\leq P_s(t), \\quad \\forall t=1,2,\\ldots,T \\\\\n& 0 \\leq P_{wc}(t) \\leq P_w(t), \\quad \\forall t=1,2,\\ldots,T \\\\\n\\end{aligned}\n\\]\nThe decision variables include\n\nGenerator output \\(P_g(t)\\)\nLoad shedding \\(P_{ls}(t)\\)\nSolar energy curtailment \\(P_{sc}(t)\\)\nWind energy curtailment \\(P_{wc}(t)\\)\nBus voltage angle \\(\\theta(t)\\)\n\nAll the decision variables are vectors and from \\(t=1\\) to \\(T\\). The objective function is to minimize the total cost, which includes the generator cost, load shedding cost, solar curtailment cost, and wind curtailment cost. The constraints include the generator output limits, ramping limits, line flow limits, power balance, bus voltage angle reference, and reserve requirements. Meanwhile, the parameters include\n\nLoad \\(P_l(t)\\)\nSolar generation \\(P_s(t)\\)\nWind generation \\(P_w(t)\\)\nReserve requirement \\(r(t)\\) which are assumed to be known.\n\nThe initial condition, e.g., \\(P_g(0)\\) should also be given.\nThe definition of the bus and branch admittance matrix \\(B_{\\text{bus}}\\) and \\(B_f\\) are the same as the previous post Power System Operation: AC and DC Power Flow Model."
  },
  {
    "objectID": "posts/learning/power_system/ncuc.html#unit-commitment-with-binary-variables",
    "href": "posts/learning/power_system/ncuc.html#unit-commitment-with-binary-variables",
    "title": "Unit Commitment Problem",
    "section": "Unit Commitment With Binary Variables",
    "text": "Unit Commitment With Binary Variables\nThe UC with binary variables can be formulated as mixed-integer linear programming (MILP) problem as follows. Apart from the decision variables in the previous section, the binary variables \\(u(t)\\) are introduced to decide the on/off status of the generator.\nThe optimization problem can be formulated as \\[\n\\begin{aligned}\n\\min_{u_g, y_g, z_g, P_g, \\theta, P_{ls}, P_{sc}, P_{wc}} & \\quad \\sum_{t=1}^T c_{gf}^Tu_g(t) + c_{gv}^TP_g(t) + c_{gsu}^Ty_g(t) + c_{gsd}^Tz_g(t) + c_{ls}^TP_{ls}(t) + c_{sc}^TP_{sc}(t) + c_{wc}^TP_{wc}(t) \\\\\n\\text{s.t.} & \\quad y_g(t) - z_g(t) = u_g(t) - u_g(t-1) \\\\\n& \\quad y_g(t) + z_g(t) \\leq 1 \\\\\n& \\quad u_g(t)\\circ P_g^{\\text{min}} \\leq P_g(t) \\leq u_g(t)\\circ P_g^{\\text{max}} \\\\\n& \\quad P_g(t) - P_g(t-1) \\leq R_{gu}u_g(t-1) + R_{gsu} y_g(t)\\\\\n& \\quad P_g(t-1) - P_g(t) \\leq R_{du}u_g(t) + R_{gsd} z_g(t)\\\\\n& \\quad -P_f^{\\text{max}} \\leq B_f\\theta(t) + P_{f,\\text{shift}} \\leq P_f^{\\text{max}} \\\\\n& \\quad C_gP_g(t) + C_s(P_s(t) - P_{sc}(t)) + C_w(P_w(t) - P_{wc}(t)) - C_l(P_l(t) - P_{ls}(t)) \\\\\n& \\quad  \\qquad = B_{\\text{bus}}\\theta(t) + P_{\\text{bus,shift}} \\\\\n& \\quad \\theta_{\\text{ref}}(t) = \\theta_0, \\quad \\forall t=1,2,\\ldots,T \\\\\n& \\quad \\sum u_g(t) \\circ P_g^{\\text{max}} \\geq \\sum_{i=1}^TP_g(t) + r(t) \\\\\n& \\quad 0 \\leq P_{ls}(t) \\leq P_l(t), \\quad \\forall t=1,2,\\ldots,T \\\\\n& \\quad 0 \\leq P_{sc}(t) \\leq P_s(t), \\quad \\forall t=1,2,\\ldots,T \\\\\n& \\quad 0 \\leq P_{wc}(t) \\leq P_w(t), \\quad \\forall t=1,2,\\ldots,T \\\\\n& \\quad u_g(t) \\in \\{0,1\\}, \\quad y_g(t) \\in \\{0,1\\}, \\quad z_g(t) \\in \\{0,1\\}\n\\end{aligned}\n\\] where \\(\\circ\\) denotes the element-wise product.\nApart from the decision variables in the previous section, the following binary variables are introduced:\n\nGenerator on/off status \\(u(t)\\) with cost \\(c_{gf}\\)\nGenerator start-up status \\(y(t)\\) with cost \\(c_{gsu}\\)\nGenerator shut-down status \\(z(t)\\) with cost \\(c_{gsd}\\)\n\nThe objective function is to minimize the total cost, which includes the generator fixed (on) cost, generator variable cost, generator start-up cost, generator shut-down cost, load shedding cost, solar curtailment cost, and wind curtailment cost.\nThe first two constraints denote the relationship between the on-off status and the start-up and shut-down status, which can be formulated as the following table:\n\nThe relationship between the on-off status and the start-up and shut-down status\n\n\n\\(u_g(t-1)\\)\n\\(u_g(t)\\)\n\\(y_g(t)\\)\n\\(z_g(t)\\)\nExplain\n\n\n\n\n0\n0\n0\n0\nKeep off\n\n\n0\n1\n1\n0\nStart-up\n\n\n1\n0\n0\n1\nShut-down\n\n\n1\n1\n0\n0\nKeep on\n\n\n\nThe ramp-up constraint is also modified where \\(R_{gu}\\) is the ramp-up limit as the previous section and \\(R_{gsu}\\) is the start-up ramp-up limit. The ramp-up status only has three states\n\nThe relationship between the on-off status and the start-up status\n\n\n\\(u_g(t-1)\\)\n\\(y_g(t)\\)\nExplain\n\n\n\n\n0\n0\nKeep off\n\n\n0\n1\nStart-up\n\n\n1\n0\nShut-down or Keep on\n\n\n\nThe ramp-down constraint is also modified where \\(R_{du}\\) is the ramp-down limit as the previous section and \\(R_{gsd}\\) is the shut-down ramp-down limit.\nSimilarly, the ramp-down constraint is modified where \\(R_{du}\\) is the ramp-down limit as the previous section and \\(R_{gsd}\\) is the shut-down ramp-down limit. The ramp-down status only has three states as well\n\nThe relationship between the on-off status and the shut-down status\n\n\n\\(u_g(t)\\)\n\\(z_g(t)\\)\nExplain\n\n\n\n\n0\n0\nKeep off\n\n\n0\n1\nShut down\n\n\n1\n0\nKeep on or Start up\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe initial conditions \\(u_g(0), y_g(0), z_g(0), P_g(0)\\) need to be given."
  },
  {
    "objectID": "posts/learning/power_system/ed.html",
    "href": "posts/learning/power_system/ed.html",
    "title": "Economic Dispatch",
    "section": "",
    "text": "The economic dispatch problem is a fundamental optimization problem in power system operation. It aims to minimize the total generation cost while meeting the power demand and satisfying the operational constraints. This post provides an overview of the economic dispatch problem, its formulation, and solution methods. The main reference of this post is Conejo’s lecture notes. Apart from the basic setting, the on-off and set-point conditions of the unit-commitment problem is also passes through to the ED formulation.\nThis post is also a supplement to the github repository PowerSystemOperation"
  },
  {
    "objectID": "posts/learning/autometic_differentiation/forward_mode_ode.html",
    "href": "posts/learning/autometic_differentiation/forward_mode_ode.html",
    "title": "Tangent and Adjoint Sensitivity Analysis of Linear Equations",
    "section": "",
    "text": "This post summarizes my learning note on the forward-mode (or tangent method) for sensitivity analysis of ordinary differential equations (ODEs). Similar to the previous posts on tangent method for linear and nonlinear equations, it can be extended for finding the sensitivity of the solution of ODEs or the gradient through the NeuralODE with repsect to the initial condition or the parameters. This post is based on the YouTube video: Neural ODEs - Pushforward/Jvp rule.\nConsider the nonlinear ODE: \\[\n\\frac{du}{dt} = f(u, \\theta)\n\\] which is evaluated at \\(t=T\\) with initial condition \\(u(t=0)=u_0\\) as \\[\nq(\\theta,u_0,T) = u(T)\n\\] where \\(\\theta\\in\\mathbb{R}^P\\) is the parameter; \\(u_0\\in\\mathbb{R}^N\\) is the initial condition; \\(T\\) is the final time; and \\(u(T)\\in\\mathbb{R}^N\\) is the final state. The ODE can be solved by any ODE solver, such as the Euler method or Runge-Kutta method.\nOur task is to forward-propagate the tangent information (a vector) on the inpus \\(\\dot{\\theta}\\in\\mathbb{R}^P, \\dot{u}_0\\in\\mathbb{R}^{N}, T\\in\\mathbb{R}\\) to the output \\(\\dot{u}(T)\\in\\mathbb{R}^N\\) without unrolling the solver and applying forward-mode AD to its operation.\nIn general, the forward-mode AD can be found the Jacobian-vector product on the total derivative of the output: \\[\n\\dot{u}(T) = \\underbrace{\\frac{\\partial q}{\\partial \\theta}\\dot{\\theta}}_{\\dot{u}(T)_\\theta} + \\underbrace{\\frac{\\partial q}{\\partial u_0}\\dot{u}_0}_{\\dot{u}(T)_{u_0}} + \\underbrace{\\frac{\\partial q}{\\partial T}\\dot{T}}_{\\dot{u}(T)_T}\n\\]\nWe can find the tangent information item by item."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/forward_mode_ode.html#tangent-condition-from-parameter-theta",
    "href": "posts/learning/autometic_differentiation/forward_mode_ode.html#tangent-condition-from-parameter-theta",
    "title": "Tangent and Adjoint Sensitivity Analysis of Linear Equations",
    "section": "Tangent Condition from parameter \\(\\theta\\)",
    "text": "Tangent Condition from parameter \\(\\theta\\)\nFrom \\[\nu(T) = u_0 + \\int_0^T f(u(t), \\theta) dt\n\\] take the total derivative with respect to \\(\\theta\\): \\[\n\\frac{d}{d\\theta}u(T) = \\frac{d}{d\\theta}u_0 + \\int_0^T \\frac{\\partial f}{\\partial u}\\frac{d}{d\\theta}u(t) + \\frac{\\partial f}{\\partial \\theta} dt\n\\]\nMultiply both sides by \\(\\dot{\\theta}\\): \\[\n\\underbrace{\\frac{d}{d\\theta}u(T) \\cdot \\dot{\\theta}}_{\\dot{u}(T)_\\theta} = \\underbrace{\\frac{d}{d\\theta}u_0 \\cdot \\dot{\\theta}}_{=0} + \\int_0^T \\frac{\\partial f}{\\partial u}\\underbrace{\\frac{d}{d\\theta}u(t) \\cdot \\dot{\\theta}}_{\\dot{u}(t)_\\theta} + \\frac{\\partial f}{\\partial \\theta} \\cdot \\dot{\\theta} dt\n\\]\n\n\n\n\n\n\nNote\n\n\n\nIt is assumed that the initial condition \\(u_0\\) is not dependent on \\(\\theta\\). However, this may not be true for example when the event is considered.\n\n\nThe above relationship is the solution of the following ODE on state \\(\\dot{u}(t)_\\theta\\): \\[\n\\begin{aligned}\n\\frac{d}{dt}\\dot{u}(t)_\\theta &= \\frac{\\partial f}{\\partial u}\\dot{u}(t)_\\theta + \\frac{\\partial f}{\\partial \\theta}\\dot{\\theta} \\\\\n\\dot{u}(t=0)_\\theta &= 0\n\\end{aligned}\n\\tag{1}\\]\nBy solving the tangent ODE using any ODE solver, we can find the tangent information \\(\\dot{u}(t)_\\theta\\). Note that this ODE is linear and inhomogeneous, although the original ODE can be nonlinear."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/forward_mode_ode.html#tangent-condition-from-initial-condition-u_0",
    "href": "posts/learning/autometic_differentiation/forward_mode_ode.html#tangent-condition-from-initial-condition-u_0",
    "title": "Tangent and Adjoint Sensitivity Analysis of Linear Equations",
    "section": "Tangent Condition from initial condition \\(u_0\\)",
    "text": "Tangent Condition from initial condition \\(u_0\\)\nFind the total derivative of \\(u(T)\\) with respect to \\(u_0\\) on both sides of Equation 1: \\[\n\\frac{d}{du_0}u(T) = \\frac{\\partial u_0}{\\partial u_0} + \\int_0^T \\frac{\\partial f}{\\partial u}\\frac{d}{du_0}u(t) dt\n\\]\nMultiply both sides by \\(\\dot{u}_0\\): \\[\n\\underbrace{\\frac{d}{du_0}u(T) \\cdot \\dot{u}_0}_{\\dot{u}(T)_{u_0}} = \\underbrace{\\frac{\\partial u_0}{\\partial u_0} \\cdot \\dot{u}_0}_{=\\dot{u}_0} + \\int_0^T \\frac{\\partial f}{\\partial u}\\underbrace{\\frac{d}{du_0}u(t) \\cdot \\dot{u}_0}_{\\dot{u}(t)_{u_0}} dt\n\\] which is again as the solution of the following ODE on state \\(\\dot{u}(t)_{u_0}\\): \\[\n\\begin{aligned}\n\\frac{d}{dt}\\dot{u}(t)_{u_0} &= \\frac{\\partial f}{\\partial u}\\dot{u}(t)_{u_0} \\\\\n\\dot{u}(t=0)_{u_0} &= \\dot{u}_0\n\\end{aligned}\n\\] which is a linear and homogeneous ODE."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/forward_mode_ode.html#computation",
    "href": "posts/learning/autometic_differentiation/forward_mode_ode.html#computation",
    "title": "Tangent and Adjoint Sensitivity Analysis of Linear Equations",
    "section": "Computation",
    "text": "Computation\nNote that \\(\\dot{\\theta}, \\dot{u}_0, \\dot{T}\\) are known vectors. Meanwhile, the Jacobian \\(\\frac{\\partial f}{\\partial u}\\) and \\(\\frac{\\partial f}{\\partial \\theta}\\) can be found by analytical derivation or automatic differentiation. In most of the cases, the Jacobians do not need to be computed or stored explicitly. Instead, the Jacobian-vector product (JVP) can be computed by the forward-mode AD, e.g. by torch.autograd.forward_ad in PyTorch.\nBecause \\(\\frac{\\partial f}{\\partial u}\\) and \\(\\frac{\\partial f}{\\partial \\theta}\\) are functions of \\(u(t)\\), its value is dependent on the solution of the primal ODE. One method is to store the solution of the primal ODE. However, a more compact method is to simutaneously solve the primal ODE and the tangent ODEs. For example, \\[\n\\begin{aligned}\n\\frac{du}{dt} &= f(u, \\theta) \\\\\n\\frac{d}{dt}\\dot{u}(t)_{u_0} &= \\frac{\\partial f}{\\partial u}\\dot{u}(t)_{u_0} \\\\\n\\frac{d}{dt}\\dot{u}(t)_\\theta &= \\frac{\\partial f}{\\partial u}\\dot{u}(t)_\\theta + \\frac{\\partial f}{\\partial \\theta}\\dot{\\theta}\n\\end{aligned}\n\\] where the initial conditions are \\(u(t=0)=u_0\\), \\(\\dot{u}(t=0)_{u_0}=\\dot{u}_0\\) and \\(\\dot{u}(t=0)_\\theta=0\\). The ODEs can be solved by any ODE solver, such as the Euler method or Runge-Kutta method.\nThe tangent method can also be used to find the Jacobian of the output with respect to the parameter \\(\\theta\\) by setting the tangent vector \\(\\dot{\\theta}\\) to be the unit vectors. Not suprisingly, the \\(P\\) number of ODEs need to be solved to find the Jacobian \\(\\frac{d u(t)}{d \\theta}\\)."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/forward_mode_ode.html#tangent-condition-from-final-time-t",
    "href": "posts/learning/autometic_differentiation/forward_mode_ode.html#tangent-condition-from-final-time-t",
    "title": "Tangent and Adjoint Sensitivity Analysis of Linear Equations",
    "section": "Tangent Condition from final time \\(T\\)",
    "text": "Tangent Condition from final time \\(T\\)\nFind the total derivative of \\(u(T)\\) with respect to \\(T\\) on both sides of Equation 1: \\[\n\\frac{d}{dT}u(T) = \\frac{d}{dT}u_0 + f(u(T), \\theta)\n\\] Note that we have \\(\\frac{d}{dT}\\int_0^T f(u(t), \\theta) dt = f(u(T), \\theta)\\).\nMultiply both sides by \\(\\dot{T}\\): \\[\n\\underbrace{\\frac{d}{dT}u(T) \\cdot \\dot{T}}_{\\dot{u}(T)_T} = \\underbrace{\\frac{d}{dT}u_0 \\cdot \\dot{T}}_{=0} + f(u(T), \\theta) \\cdot \\dot{T}\n\\] which is an algebraic equation (only for the final time)."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/forward_mode_ode.html#pytorch-implementation",
    "href": "posts/learning/autometic_differentiation/forward_mode_ode.html#pytorch-implementation",
    "title": "Tangent and Adjoint Sensitivity Analysis of Linear Equations",
    "section": "Pytorch Implementation",
    "text": "Pytorch Implementation\nAn example code snippet can be found at my-github including a parallel batched implementation."
  }
]