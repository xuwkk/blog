[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog! Here I regularly update learning notes and news about my research."
  },
  {
    "objectID": "posts/news/index.html",
    "href": "posts/news/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post of my blog. I will regularly update my learning note and research outcomes at this website."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Content",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nJun 11, 2024\n\n\nDeep Implicit Layers: Fixed-Point Iteration\n\n\nLearning Note, Auto Differentiation, Implicit Function\n\n\n\n\nJun 9, 2024\n\n\nPower System Operation: AC and DC Power Flow Model\n\n\nLearning Note, Power System Operation\n\n\n\n\nJun 5, 2024\n\n\nTangent and Adjoint Sensitivity Analysis of Nonlinear Equations\n\n\nLearning Note, Auto Differentiation, Sensitivity Analysis, Implicit Function\n\n\n\n\nJun 3, 2024\n\n\nTangent and Adjoint Sensitivity Analysis of Linear Equations\n\n\nLearning Note, Auto Differentiation, Sensitivity Analysis, Implicit Function\n\n\n\n\nMay 31, 2024\n\n\nWelcome To My Blog\n\n\nNews\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_linear_equation.html",
    "href": "posts/learning/autometic_differentiation/adjoint_linear_equation.html",
    "title": "Tangent and Adjoint Sensitivity Analysis of Linear Equations",
    "section": "",
    "text": "This post contains my learning note for YouTube: Adjoint Equation of a Linear System of Equations - by implicit derivative.\nAll credits go to the author of the video."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_linear_equation.html#settings",
    "href": "posts/learning/autometic_differentiation/adjoint_linear_equation.html#settings",
    "title": "Tangent and Adjoint Sensitivity Analysis of Linear Equations",
    "section": "Settings",
    "text": "Settings\nConsider a linear system of equations \\[\nA(\\theta) x = b(\\theta)\n\\tag{1}\\] with a loss function \\(J(x)\\) . Our goal is to find the total derivative \\(\\frac{d J}{d \\theta}\\). This gradient can be useful for:\n\nGradient-based optimization.\nLocal sensitivity analysis of linear equations.\n\nwhere \\(\\theta\\in\\mathbb{R}^P\\) (this can be the weights of neural network); \\(A\\in\\mathbb{R}^{M\\times N}\\); \\(x\\in\\mathbb{R}^N\\); \\(b\\in\\mathbb{R}^M\\); \\(J(x;\\theta): \\mathbb{R}^N \\times \\mathbb{R}^P \\rightarrow \\mathbb{R}\\).\nNote that \\(x\\) is dependent on \\(\\theta\\) through \\(A\\) and \\(b\\). The total derivative \\(\\frac{d J}{d \\theta}\\) can be computed using the chain rule: \\[\n\\frac{d J}{d \\theta} = \\frac{\\partial J}{\\partial x} \\frac{d x}{d \\theta} + \\frac{\\partial J}{\\partial \\theta}\n\\tag{2}\\] where we use the Jacobian convension such that \\(\\frac{d x}{d \\theta}\\) is a matrix of size \\(N\\times P\\) and it is difficult to compute directly."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_linear_equation.html#tangent-sensitivity-analysis",
    "href": "posts/learning/autometic_differentiation/adjoint_linear_equation.html#tangent-sensitivity-analysis",
    "title": "Tangent and Adjoint Sensitivity Analysis of Linear Equations",
    "section": "Tangent Sensitivity Analysis",
    "text": "Tangent Sensitivity Analysis\nDo total derivative of (Equation 1) with respect to \\(\\theta\\): \\[\n\\frac{d}{d\\theta} (A x) =  \\frac{d b}{d\\theta}\n\\]\nwhere the unknown \\(\\frac{d x}{d \\theta}\\) can found by solving the following linear system\n\\[\nA\\cdot\\frac{d x}{d \\theta}  = \\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x\n\\] where \\(\\frac{d x}{d \\theta}\\) can be solved as \\[\n\\frac{d x}{d \\theta} = A^{-1} \\left(\\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x\\right)\n\\tag{3}\\]\nNote that the dimension \\(\\frac{d A}{d \\theta}\\in\\mathbb{R}^{N\\times N\\times P}\\). Therefore the product \\(\\frac{d A}{d\\theta} x\\) is incorrect (but it is ok here).\nThis is a batch of linear system we want to solve. Let \\(\\theta_i\\) be the \\(i\\)-th element of \\(\\theta\\), \\[\nA\\cdot\\frac{d x}{d \\theta_i}  = \\frac{db}{d\\theta_i} - \\underbrace{\\frac{d A}{d\\theta_i}}_{N\\times N} x, \\quad i=1,\\dots,P\n\\]\nWe can view \\(\\frac{d x}{d \\theta_i}\\) as the tangent of \\(x\\), e.g., \\(\\dot{x}_i = \\frac{d x}{d \\theta_i}\\). Then solving the above system follows the idea of tangent sensitivity analysis (or forward-mode AD). The matrix-vector products can be computed efficiently using the JVP. However, this method is less efficient when \\(P\\) is large."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_linear_equation.html#adjoint-sensitivity-analysis",
    "href": "posts/learning/autometic_differentiation/adjoint_linear_equation.html#adjoint-sensitivity-analysis",
    "title": "Tangent and Adjoint Sensitivity Analysis of Linear Equations",
    "section": "Adjoint Sensitivity Analysis",
    "text": "Adjoint Sensitivity Analysis\nPlug (Equation 3) into (Equation 2), we have \\[\n\\frac{d J}{d \\theta} = \\underbrace{\\frac{\\partial J}{\\partial x} A^{-1}}_{\\lambda^T:1\\times N} \\left(\\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x\\right) + \\frac{\\partial J}{\\partial \\theta}\n\\]\nNow instead of solving the linear system as in the tangent method (which requires solving \\(P\\) linear systems), note that the term \\(\\frac{\\partial J}{\\partial x} A^{-1}\\) is a vector of size \\(1\\times N\\) which can be solved by the following linear system once: \\[\nA^T \\lambda = \\left(\\frac{\\partial J}{\\partial x}\\right)^T\n\\tag{4}\\]\nThen the gradient \\(\\frac{d J}{d \\theta}\\) can be computed as \\[\n\\frac{d J}{d \\theta} = \\lambda^T \\left(\\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x\\right) + \\frac{\\partial J}{\\partial \\theta}\n\\]\nThis is the idea of adjoint sensitivity analysis (or reverse-mode AD). It can be considered as assigning the adjoint of \\(x\\) as \\(\\bar{x} = \\frac{\\partial J}{\\partial x}\\).\nThe adjoint sensitivity can be solved by solving two linear systems: (Equation 1) for \\(x\\) and (Equation 4) for \\(\\lambda\\). This is more efficient when \\(P\\) is large (especially when the loss function is a scalar).\nAlso note that the Jacobians \\(\\frac{d A}{d\\theta}\\), \\(\\frac{d b}{d\\theta}\\), \\(\\frac{dA}{d\\theta}\\), and \\(\\frac{\\partial J}{\\partial x}\\) can be computed by automatic differentiation or analytical solution.\n\nAlternative Derivation Using Lagrange Multiplier\nThis part is based on the YouTube: Adjoint Sensitivities of a Linear System of Equations - derived using the Lagrangian.\nAs the sensitivity analysis can be directly used for purturbation analysis in an optimization problem, e.g., to find how a small change on \\(\\theta\\) can affect the objective function \\(J(x)\\), we can consider the following optimization problem \\[\n\\min_{x} J(x, \\theta) \\quad \\text{s.t.} \\quad A(\\theta) x = b(\\theta)\n\\]\nThe equality constraint can be regarded as the KKT condition of another convex optimization problem. I.e., the original problem is actually a bi-level optimization problem. The Lagrangian of the above problem is \\[\n\\mathcal{L}(x, \\theta, \\lambda) = J(x(\\theta), \\lambda) + \\lambda^T (b(\\theta) - A(\\theta) x)\n\\] where \\(\\lambda\\) is the Lagrange multiplier. The total derivative wrt \\(\\theta\\) is \\[\n\\frac{d\\mathcal{L}}{d\\theta}  = \\frac{\\partial J}{\\partial x} \\frac{d x}{d \\theta} + \\frac{\\partial J}{\\partial \\theta} + \\lambda^T \\left(\\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x - A(\\theta)\\frac{dx}{d\\theta}\\right)\n\\]\nAgain, the dimension of \\(\\frac{d A}{d\\theta}\\) is incorrect. The difficult term is \\(\\frac{dx}{d\\theta}\\). After some arrangement, we have \\[\n\\frac{d\\mathcal{L}}{d\\theta} = \\frac{\\partial J}{\\partial \\theta} + \\lambda^T \\left(\\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x\\right) + \\underbrace{\\left(\\frac{\\partial J}{\\partial x} - \\lambda^T A\\right)}_{\\rightarrow 0} \\frac{dx}{d\\theta}\n\\]\nNote that because \\(x\\) is solved as the solution to \\(Ax = b\\), the equality constraint is always satisfied. Therefore, the value of \\(\\lambda\\) can be arbitrary. Consequently, we obtain the adjoint system the same to the previous derivation Equation 4.\nPLugging in \\(\\lambda\\) into the Lagrangian, we have \\[\n\\frac{d \\mathcal{L}}{d \\theta} = \\frac{d J}{d \\theta} = \\lambda^T \\left(\\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x\\right) + \\frac{\\partial J}{\\partial \\theta}\n\\] where the first equality is due to \\(Ax=b\\)."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html",
    "href": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html",
    "title": "Tangent and Adjoint Sensitivity Analysis of Nonlinear Equations",
    "section": "",
    "text": "This post extends from the previous post on linear system to find the sensitivities of the parameter of nonlinear systems. This post is largely learnt from the YouTube: Adjoint Sensitivities of a Non-Linear system of equations | Full Derivation and YouTube: Lagrangian Perspective on the Derivation of Adjoint Sensitivities of Nonlinear Systems. Another reference is Deep Implicit Layers"
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#settings",
    "href": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#settings",
    "title": "Tangent and Adjoint Sensitivity Analysis of Nonlinear Equations",
    "section": "Settings",
    "text": "Settings\nConsider a nonlinear system of equations \\[\nf(x, \\theta) = 0\n\\] where \\(x \\in \\mathbb{R}^N\\) is the state variable and \\(\\theta \\in \\mathbb{R}^P\\) is the parameter. Nonlinear equation solvers such as Newton’s method can be used to find \\(x\\) given \\(\\theta\\). Assume there is a scalar loss function \\(J(x,\\theta)\\) and our goal is to find the sensitivity or total gradient of \\(J\\) with respect to \\(\\theta\\): \\(\\frac{d J}{d \\theta}\\).\nThe total derivative of \\(J\\) is \\[\n\\frac{d J}{d \\theta} = \\frac{\\partial J}{\\partial x} \\frac{d x}{d \\theta} + \\frac{\\partial J}{\\partial \\theta}\n\\tag{1}\\] where \\(\\frac{d x}{d \\theta}\\) is unknown. Do the total derivative of \\(f\\) with respect to \\(\\theta\\): \\[\n\\frac{d f}{d \\theta} = \\frac{\\partial f}{\\partial x} \\frac{d x}{d \\theta} + \\frac{\\partial f}{\\partial \\theta} = 0\n\\tag{2}\\]\nTherefore, \\(\\frac{d x}{d \\theta}\\) can be solved by the above linear equation. In detail, Equation 1 can be rewritten as \\[\n\\frac{d J}{d \\theta} = -\\frac{\\partial J}{\\partial x} \\left( \\frac{\\partial f}{\\partial x} \\right)^{-1} \\frac{\\partial f}{\\partial \\theta} + \\frac{\\partial J}{\\partial \\theta}\n\\tag{3}\\]"
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#tangent-sensitivity-analysis",
    "href": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#tangent-sensitivity-analysis",
    "title": "Tangent and Adjoint Sensitivity Analysis of Nonlinear Equations",
    "section": "Tangent Sensitivity Analysis",
    "text": "Tangent Sensitivity Analysis\nIn the tangent (forward) method, the term \\(\\left( \\frac{\\partial f}{\\partial x} \\right)^{-1} \\frac{\\partial f}{\\partial \\theta}\\) is computed by solving the batch of linear system Equation 2 directly. Denote the \\(i\\)-th column of \\(\\frac{\\partial f}{\\partial \\theta}\\) as \\(g_i\\), then \\(P\\) linear systems need to be solved: \\[\n\\frac{\\partial f}{\\partial x} \\frac{dx}{d\\theta_i} = -g_i\n\\]"
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#adjoint-sensitivity-analysis",
    "href": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#adjoint-sensitivity-analysis",
    "title": "Tangent and Adjoint Sensitivity Analysis of Nonlinear Equations",
    "section": "Adjoint Sensitivity Analysis",
    "text": "Adjoint Sensitivity Analysis\nEquation 3 can be solved from left to right by first computing \\(-\\frac{\\partial J}{\\partial x} \\left( \\frac{\\partial f}{\\partial x} \\right)^{-1}\\) and then multiplying \\(\\frac{\\partial f}{\\partial \\theta}\\). The adjoint linear system is \\[\n\\left( \\frac{\\partial f}{\\partial x} \\right)^T \\lambda = -\\left(\\frac{\\partial J}{\\partial x}\\right)^T\n\\tag{4}\\] which can be solved by conjugate gradient method or LU deomposition. The Jacobian matrix \\(\\frac{\\partial f}{\\partial x}\\) may not need to be solved explicitely but can be found by VJP.\nIn the adjoint method, there is only one linear system to solve (note that the original system is nonlinear), regardless of the number of parameters \\(P\\).\n\nAlternative Derivation using Lagrangian\nSimilar to the linear system, we can derive the adjoint sensitivity analysis for nonlinear system from the Lagrangian perspective.\nConsider the equality constrained optimization: \\[\n\\min_{x} J(x, \\theta) \\quad \\text{s.t.} \\quad f(x, \\theta) = 0\n\\]\nThe Lagrangian is \\[\n\\mathcal{L}(x, \\lambda, \\theta) = J(x, \\theta) + \\lambda^T f(x, \\theta)\n\\]\nTake the total derivative of \\(\\mathcal{L}\\) with respect to \\(\\theta\\): \\[\n\\frac{d \\mathcal{L}}{d \\theta} = \\frac{\\partial J}{\\partial \\theta} + \\frac{\\partial J}{\\partial x} \\frac{dx}{d\\theta} + \\lambda^T \\left(\\frac{\\partial f}{\\partial \\theta} + \\frac{\\partial f}{\\partial x} \\frac{dx}{d\\theta}\\right) = \\frac{\\partial J}{\\partial \\theta} + \\lambda^T\\frac{\\partial f}{\\partial \\theta} + \\left(\\frac{\\partial J}{\\partial x} + \\lambda^T \\frac{\\partial f}{\\partial x}\\right) \\frac{dx}{d\\theta}\n\\]\nBecause the equality constraint is always satisfied (as \\(x\\) is solved from \\(f(x, \\theta) = 0\\)), we can set the dual variable \\(\\lambda\\) arbitarily. Here, we can choose to make the coefficient of \\(\\frac{dx}{d\\theta}\\) to be zero so that this complex term never appears in the final expression. \\[\n\\frac{\\partial J}{\\partial x} + \\lambda^T \\frac{\\partial f}{\\partial x} = 0\n\\] which is the adjoint equation Equation 4.\nAt last, because \\(f(x,\\theta)=0\\), we have \\[\n\\frac{d \\mathcal{L}}{d \\theta} = \\frac{\\partial J}{\\partial \\theta} + \\lambda^T\\frac{\\partial f}{\\partial \\theta} = \\frac{d J}{d\\theta}\n\\]\n\n\nRelation to Linear System\nWe can rewrite the linear system as \\[\nf(x, \\theta) = b(\\theta) - A(\\theta) x = 0\n\\] with \\[\n\\frac{\\partial f}{\\partial \\theta} = \\frac{\\partial f}{\\partial b}\\frac{db}{d\\theta} + \\frac{\\partial f}{\\partial A}\\frac{dA}{d\\theta} = \\frac{db}{d\\theta} - \\frac{dA}{d\\theta}x \\neq 0\n\\]\nTherefore, the total derivative can be written as \\[\n\\frac{df}{d\\theta} = \\frac{\\partial f}{\\partial x} \\frac{dx}{d\\theta} + \\frac{\\partial f}{\\partial \\theta} = -A \\frac{dx}{d\\theta} + \\frac{db}{d\\theta} - \\frac{dA}{d\\theta}x\n\\] which recover the derivation of the linear system.\n\n\nImplementation\nAn example implementation of batched-version of adjoint sensitivity analysis has been added to here."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#relation-to-linear-system",
    "href": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#relation-to-linear-system",
    "title": "Tangent and Adjoint Sensitivity Analysis of Nonlinear Equations",
    "section": "Relation to Linear System",
    "text": "Relation to Linear System\nWe can rewrite the linear system as \\[\nf(x, \\theta) = b(\\theta) - A(\\theta) x = 0\n\\] with \\[\n\\frac{\\partial f}{\\partial \\theta} = \\frac{\\partial f}{\\partial b}\\frac{db}{d\\theta} + \\frac{\\partial f}{\\partial A}\\frac{dA}{d\\theta} = \\frac{db}{d\\theta} - \\frac{dA}{d\\theta}x \\neq 0\n\\]\nTherefore, the total derivative can be written as \\[\n\\frac{df}{d\\theta} = \\frac{\\partial f}{\\partial x} \\frac{dx}{d\\theta} + \\frac{\\partial f}{\\partial \\theta} = -A \\frac{dx}{d\\theta} + \\frac{db}{d\\theta} - \\frac{dA}{d\\theta}x\n\\]"
  },
  {
    "objectID": "posts/learning/autometic_differentiation/implicit_equation.html",
    "href": "posts/learning/autometic_differentiation/implicit_equation.html",
    "title": "Functions Defined Implicitly by Equations (Part A)",
    "section": "",
    "text": "This post contains my understanding on Chapter 1 Functions Defined Implicitly by Equations of the book Implicit Functions and Solution Mappings by Asen L. Dontchev and R. T. Rockafellar."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/implicit_equation.html#introduction",
    "href": "posts/learning/autometic_differentiation/implicit_equation.html#introduction",
    "title": "Functions Defined Implicitly by Equations (Part A)",
    "section": "Introduction",
    "text": "Introduction\nWe study the property of the function defined implicitly by an equation such as\nImplicit function: \\(f(p,x) = 0\\rightarrow x = s(p)\\).\nInversion: \\(f(x) = y \\rightarrow x = f^{-1}(y)\\).\nWe care about 1). when an implicit function or an inverse function exits, 2). has the properties like differentiability.\nAn example of quadratic function is shown below.\n\n\n\nQuadratic function. The inverse of it is not a function but part of it may be (point B) and may not be (point A)\n\n\nThe reflected graph above is not a function but can be regarded as a set-valued mapping. Then we are insterested in to which a graphical localization of a set-valued mapping might be a function and what properties it has.\nNoted that for the inverse of the above example, it assigns two different \\(x\\)s to \\(y\\) when \\(y&gt;0\\), no \\(x\\) to y when \\(y&lt;0\\) and one \\(x\\) to \\(y\\) when \\(y=0\\).\n\nSet-Valued Mappings\nFormally, the set-valued mapping \\(F\\) from \\(\\mathbb{R}^n \\rightrightarrows \\mathbb{R}^m\\) is \\[\nF: \\mathbb{R}^n \\rightrightarrows \\mathbb{R}^m\n\\] which assign to each \\(x\\in \\mathbb{R}^n\\) one (single-valued at \\(x\\)) or more elements (multi-valued at \\(x\\)) of \\(\\mathbb{R}^m\\), or possibly none (empty-valued at \\(x\\)).\nThe graph of \\(F\\) is the set \\[\n\\text{gph} F = \\{(x,y)\\in \\mathbb{R}^n \\times \\mathbb{R}^m: y\\in F(x)\\}.\n\\]\n\\(F\\) is a function such that it is single-valued at each \\(x\\) if \\(\\text{gph} F\\). In this case, \\(F: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) or \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\).\nEvery set-valued mapping has an inverse, namelt the set-valued mapping \\(F^{-1}\\) defined by \\[\nF^{-1}(y) = \\{x\\in \\mathbb{R}^n: y\\in F(x)\\}.\n\\]\nNote that 1. \\(F^{-1}\\) is another set-valued mapping. 2. All function \\(f\\) (as a set-valued mapping) has an inverse \\(f^{-1}\\) which is set-valued mapping. The question than becomes passing to some piece of the graph of \\(f^{-1}\\), which is related to the notion of graphical localization.\n!INFORMAL. Given a pair \\((\\bar{x},\\bar{y})\\in \\text{gph} F\\) and neighborhood \\(U\\) and \\(V\\) around \\(\\bar{x}\\) and \\(\\bar{y}\\), the graphical localization of \\(F\\) at \\(\\bar{x}\\) for \\(\\bar{y}\\) is a set-valued mapping \\(\\tilde{F}: U\\rightarrow V\\). The inverse of \\(\\tilde{F}\\) is the graphical localization of the set-valued mapping \\(F^{-1}\\) at \\(\\bar{y}\\) for \\(\\bar{x}\\), e.g. \\(\\tilde{F}^{-1}: V\\rightarrow U\\).\n!INFORMAL. By a single-valued localization of \\(F\\) at \\(\\bar{x}\\) for \\(\\bar{y}\\) will be meant a graphical localization that is a function.\nThe set-valued inverse \\(f^{-1}\\) of the quadratic function is single-valued at \\(y=0\\). However, it is not a single-valued localization at \\(y=0\\) for \\(x=0\\) (as it is not a function at all).\n\n\n\n\n\n\nSummary\n\n\n\nInformally, for set-valued mapping, it may be possible to define a single-valued localization at (or around) a point \\(\\bar{x}\\) to a point \\(\\bar{y}\\), which is a function.\n\n\n\n\nImplicit Functions\nFormally, the implicit function is defined as \\(f(p,x)=0\\) with the function \\(f: \\mathbb{R}^d \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) in which \\(p\\) acts as a parameter.\nThe solution mapping is \\[\nS: \\mathbb{R}^d \\rightrightarrows \\mathbb{R}^n \\text { with } S(p)=\\{x \\mid f(p, x)=0\\}\n\\]\nWe can then look at pairs \\((\\bar{p},\\bar{x}\\) in \\(\\text{gph} S\\) and ask whether \\(S\\) has a single-valued localization around \\(\\bar{p}\\) for \\(\\bar{x}\\) (this is the same as the inversion). Such as localization is an implicit function.\n\n\nFuture Discussion\nThe Dini’s classic implicit function theorem assumes that \\(f\\) is continuousely differentiable around \\((\\bar{p},\\bar{x})\\) and the partial Jacobian \\(\\nabla_xf(\\bar{p},\\bar{x})\\) is nonsingular (\\(m=n\\)). Then a single-valued localization \\(s\\) of \\(S\\) exists around \\(\\bar{p}\\) for \\(\\bar{x}\\) which is continuously differentiable around \\(\\bar{p}\\) with \\[\n\\nabla s(p) = -\\nabla_xf(p,s(p))^{-1}\\nabla_pf(p,s(p)).\n\\]\nLater we will depart from the assumption of continuousely differentiability of \\(f\\) and reformulate the assumption on the Jacobian \\(\\nabla_xf(\\bar{p},\\bar{x})\\) as an assumption on \\[\nh(x)=f(\\bar{p}, \\bar{x})+\\nabla_x f(\\bar{p}, \\bar{x})(x-\\bar{x})\n\\] with \\(h(\\bar{x}) = 0\\). The assumption on the invertibility of \\(\\nabla_xf(\\bar{p},\\bar{x})\\) will be replaced by the assumption that the inverse mapping \\(h^{-1}\\), with \\(\\bar{x}\\in h^{-1}(0)\\) has a single-valued localization around 0 for \\(\\bar{x}\\)."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/implicit_equation_a.html",
    "href": "posts/learning/autometic_differentiation/implicit_equation_a.html",
    "title": "Functions Defined Implicitly by Equations (Part A)",
    "section": "",
    "text": "This post contains my understanding on Chapter 1 Functions Defined Implicitly by Equations of the book Implicit Functions and Solution Mappings by Asen L. Dontchev and R. T. Rockafellar."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/implicit_equation_a.html#introduction",
    "href": "posts/learning/autometic_differentiation/implicit_equation_a.html#introduction",
    "title": "Functions Defined Implicitly by Equations (Part A)",
    "section": "Introduction",
    "text": "Introduction\nWe study the property of the function defined implicitly by an equation such as\nImplicit function: \\(f(p,x) = 0\\rightarrow x = s(p)\\).\nInversion: \\(f(x) = y \\rightarrow x = f^{-1}(y)\\).\nWe care about 1). when an implicit function or an inverse function exits, 2). has the properties like differentiability.\nAn example of quadratic function is shown below.\n\n\n\nQuadratic function. The inverse of it is not a function but part of it may be (point B) and may not be (point A)\n\n\nThe reflected graph above is not a function but can be regarded as a set-valued mapping. Then we are insterested in to which a graphical localization of a set-valued mapping might be a function and what properties it has.\nNoted that for the inverse of the above example, it assigns two different \\(x\\)s to \\(y\\) when \\(y&gt;0\\), no \\(x\\) to y when \\(y&lt;0\\) and one \\(x\\) to \\(y\\) when \\(y=0\\).\n\nSet-Valued Mappings\nFormally, the set-valued mapping \\(F\\) from \\(\\mathbb{R}^n \\rightrightarrows \\mathbb{R}^m\\) is \\[\nF: \\mathbb{R}^n \\rightrightarrows \\mathbb{R}^m\n\\] which assign to each \\(x\\in \\mathbb{R}^n\\) one (single-valued at \\(x\\)) or more elements (multi-valued at \\(x\\)) of \\(\\mathbb{R}^m\\), or possibly none (empty-valued at \\(x\\)).\nThe graph of \\(F\\) is the set \\[\n\\text{gph} F = \\{(x,y)\\in \\mathbb{R}^n \\times \\mathbb{R}^m: y\\in F(x)\\}.\n\\]\n\\(F\\) is a function such that it is single-valued at each \\(x\\) if \\(\\text{gph} F\\). In this case, \\(F: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) or \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\).\nEvery set-valued mapping has an inverse, namelt the set-valued mapping \\(F^{-1}\\) defined by \\[\nF^{-1}(y) = \\{x\\in \\mathbb{R}^n: y\\in F(x)\\}.\n\\]\nNote that 1. \\(F^{-1}\\) is another set-valued mapping. 2. All function \\(f\\) (as a set-valued mapping) has an inverse \\(f^{-1}\\) which is set-valued mapping. The question than becomes passing to some piece of the graph of \\(f^{-1}\\), which is related to the notion of graphical localization.\n!INFORMAL. Given a pair \\((\\bar{x},\\bar{y})\\in \\text{gph} F\\) and neighborhood \\(U\\) and \\(V\\) around \\(\\bar{x}\\) and \\(\\bar{y}\\), the graphical localization of \\(F\\) at \\(\\bar{x}\\) for \\(\\bar{y}\\) is a set-valued mapping \\(\\tilde{F}: U\\rightarrow V\\). The inverse of \\(\\tilde{F}\\) is the graphical localization of the set-valued mapping \\(F^{-1}\\) at \\(\\bar{y}\\) for \\(\\bar{x}\\), e.g. \\(\\tilde{F}^{-1}: V\\rightarrow U\\).\n!INFORMAL. By a single-valued localization of \\(F\\) at \\(\\bar{x}\\) for \\(\\bar{y}\\) will be meant a graphical localization that is a function.\nThe set-valued inverse \\(f^{-1}\\) of the quadratic function is single-valued at \\(y=0\\). However, it is not a single-valued localization at \\(y=0\\) for \\(x=0\\) (as it is not a function at all).\n\n\n\n\n\n\nSummary\n\n\n\nInformally, for set-valued mapping, it may be possible to define a single-valued localization at (or around) a point \\(\\bar{x}\\) to a point \\(\\bar{y}\\), which is a function.\n\n\n\n\nImplicit Functions\nFormally, the implicit function is defined as \\(f(p,x)=0\\) with the function \\(f: \\mathbb{R}^d \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) in which \\(p\\) acts as a parameter.\nThe solution mapping is \\[\nS: \\mathbb{R}^d \\rightrightarrows \\mathbb{R}^n \\text { with } S(p)=\\{x \\mid f(p, x)=0\\}\n\\]\nWe can then look at pairs \\((\\bar{p},\\bar{x}\\) in \\(\\text{gph} S\\) and ask whether \\(S\\) has a single-valued localization around \\(\\bar{p}\\) for \\(\\bar{x}\\) (this is the same as the inversion). Such as localization is an implicit function.\n\n\nFuture Discussion\nThe Dini’s classic implicit function theorem assumes that \\(f\\) is continuousely differentiable around \\((\\bar{p},\\bar{x})\\) and the partial Jacobian \\(\\nabla_xf(\\bar{p},\\bar{x})\\) is nonsingular (\\(m=n\\)). Then a single-valued localization \\(s\\) of \\(S\\) exists around \\(\\bar{p}\\) for \\(\\bar{x}\\) which is continuously differentiable around \\(\\bar{p}\\) with \\[\n\\nabla s(p) = -\\nabla_xf(p,s(p))^{-1}\\nabla_pf(p,s(p)).\n\\]\nLater we will depart from the assumption of continuousely differentiability of \\(f\\) and reformulate the assumption on the Jacobian \\(\\nabla_xf(\\bar{p},\\bar{x})\\) as an assumption on \\[\nh(x)=f(\\bar{p}, \\bar{x})+\\nabla_x f(\\bar{p}, \\bar{x})(x-\\bar{x})\n\\] with \\(h(\\bar{x}) = 0\\). The assumption on the invertibility of \\(\\nabla_xf(\\bar{p},\\bar{x})\\) will be replaced by the assumption that the inverse mapping \\(h^{-1}\\), with \\(\\bar{x}\\in h^{-1}(0)\\) has a single-valued localization around 0 for \\(\\bar{x}\\)."
  },
  {
    "objectID": "posts/learning/power_system/power_system_operation.html",
    "href": "posts/learning/power_system/power_system_operation.html",
    "title": "Power System Operation: AC and DC Power Flow Model",
    "section": "",
    "text": "This post is the first one in the series of power system operation. In this post, we will introduce the basic modelling method of power system operations. I feel this can be a great summary of the knowlege. Meanwhile, this series of post can be a reference to the open-source package power system operation I am developing.\nI mainly follow the modelling method from MATPOWER. A reference can be found by the MATPOWER Manual.\nMatrix form of the power flow model will be followed."
  },
  {
    "objectID": "posts/learning/power_system/power_system_operation.html#introduction",
    "href": "posts/learning/power_system/power_system_operation.html#introduction",
    "title": "Power System Operation: AC and DC Power Flow Model",
    "section": "",
    "text": "This post is the first one in the series of power system operation. In this post, we will introduce the basic modelling method of power system operations. I feel this can be a great summary of the knowlege. Meanwhile, this series of post can be a reference to the open-source package power system operation I am developing.\nI mainly follow the modelling method from MATPOWER. A reference can be found by the MATPOWER Manual.\nMatrix form of the power flow model will be followed."
  },
  {
    "objectID": "posts/learning/power_system/power_system_operation.html#ac-power-flow-model",
    "href": "posts/learning/power_system/power_system_operation.html#ac-power-flow-model",
    "title": "Power System Operation: AC and DC Power Flow Model",
    "section": "AC Power Flow Model",
    "text": "AC Power Flow Model\n\nBranch Model\nThe branch model is shown in the following figure.\n\n\n\nBranch Model\n\n\n\n\\(z_s = r_s + jx_s\\): series impedance.\n\\(\\tau, \\theta_{\\text{shift}}\\): transformer tap ratio magnitude and phase angle (in radians). The transformer is located at the from bus on a branch. If there is no transformer, \\(\\tau = 1\\) and \\(\\theta_{\\text{shift}} = 0\\).\n\nFor a single branch, \\[\n\\left[\\begin{array}{l}\ni_f \\\\\ni_t\n\\end{array}\\right]=Y_{b r}\\left[\\begin{array}{l}\nv_f \\\\\nv_t\n\\end{array}\\right]\n\\] where the branch admittance matrix \\(Y_{b r}\\) can be found by KCL law: \\[\nY_{b r}=\\left[\\begin{array}{cc}\n\\left(y_s+j \\frac{b_c}{2}\\right) \\frac{1}{\\tau^2} & -y_s \\frac{1}{\\tau e^{-j \\theta_{\\text {shif }}}} \\\\\n-y_s \\frac{1}{\\tau e^{j \\theta_{\\text {shift }}}} & y_s+j \\frac{b_c}{2}\n\\end{array}\\right]\n\\]\nNote that \\(Y_{b r}\\) is in general not symmetric unless \\(\\theta_{\\text{shift}} = 0\\).\nLet’s denote the four elements of branch \\(i\\) as \\[\nY_{b r}^i=\\left[\\begin{array}{cc}\ny_{f f}^i & y_{f t}^i \\\\\ny_{t f}^i & y_{t t}^i\n\\end{array}\\right]\n\\]\nThe branch number of the four elements can be summarized into vectors \\(Y_{ff}\\), \\(Y_{ft}\\), \\(Y_{tf}\\), \\(Y_{tt}\\).\nMeanwhile, the from-side and to-side incidence matrix \\(C_f\\) and \\(C_t\\) are deifned such that the \\((i,j)\\) entry of \\(C_f\\) and the \\((i,k)\\) entry of \\(C_t\\) are 1 if branch \\(i\\) is connected from bus \\(j\\) to \\(k\\), respectively, and 0 otherwise. The branch-to-bus incidence matrix \\(A = C_f - C_t\\).\n\n\nGenerator Model\nThe generator complex power injection can be written as \\[\nS_g = P_g + jQ_g\n\\]\nThe generator incidence matrix \\(C_g\\) is defined such that the \\((i,j)\\) entry of \\(C_g\\) is 1 if generator \\(j\\) is connected to bus \\(i\\), and 0 otherwise. Therefore, its contribution to bus (nodal) power injection is \\[\nS_{g,\\text{bus}} = C_g S_g\n\\]"
  },
  {
    "objectID": "posts/learning/power_system/power_system_operation.html#ac-model",
    "href": "posts/learning/power_system/power_system_operation.html#ac-model",
    "title": "Power System Operation: AC and DC Power Flow Model",
    "section": "AC Model",
    "text": "AC Model\n\nBranch Model\nThe branch model is shown in the following figure.\n\n\n\nBranch Model\n\n\n\n\\(z_s = r_s + jx_s\\): series impedance.\n\\(\\tau, \\theta_{\\text{shift}}\\): transformer tap ratio magnitude and phase angle (in radians). The transformer is located at the from bus on a branch. If there is no transformer, \\(\\tau = 1\\) and \\(\\theta_{\\text{shift}} = 0\\).\n\nFor a single branch, \\[\n\\left[\\begin{array}{l}\ni_f \\\\\ni_t\n\\end{array}\\right]=Y_{b r}\\left[\\begin{array}{l}\nv_f \\\\\nv_t\n\\end{array}\\right]\n\\] where the branch admittance matrix \\(Y_{b r}\\) can be found by KCL law: \\[\nY_{b r}=\\left[\\begin{array}{cc}\n\\left(y_s+j \\frac{b_c}{2}\\right) \\frac{1}{\\tau^2} & -y_s \\frac{1}{\\tau e^{-j \\theta_{\\text {shif }}}} \\\\\n-y_s \\frac{1}{\\tau e^{j \\theta_{\\text {shift }}}} & y_s+j \\frac{b_c}{2}\n\\end{array}\\right]\n\\tag{1}\\]\nNote that \\(Y_{b r}\\) is in general not symmetric unless \\(\\theta_{\\text{shift}} = 0\\).\nLet’s denote the four elements of branch \\(i\\) as \\[\nY_{b r}^i=\\left[\\begin{array}{cc}\ny_{f f}^i & y_{f t}^i \\\\\ny_{t f}^i & y_{t t}^i\n\\end{array}\\right]\n\\]\nThe branch number of the four elements can be summarized into vectors \\(Y_{ff}\\), \\(Y_{ft}\\), \\(Y_{tf}\\), \\(Y_{tt}\\).\nMeanwhile, the from-side and to-side incidence matrix \\(C_f\\) and \\(C_t\\) are deifned such that the \\((i,j)\\) entry of \\(C_f\\) and the \\((i,k)\\) entry of \\(C_t\\) are 1 if branch \\(i\\) is connected from bus \\(j\\) to \\(k\\), respectively, and 0 otherwise. The branch-to-bus incidence matrix \\(A = C_f - C_t\\).\n\n\nGenerator Model\nThe generator complex power injection can be written as \\[\nS_g = P_g + jQ_g\n\\]\nThe generator incidence matrix \\(C_g\\) is defined such that the \\((i,j)\\) entry of \\(C_g\\) is 1 if generator \\(j\\) is connected to bus \\(i\\), and 0 otherwise. Therefore, its contribution to bus (nodal) power injection is \\[\nS_{g,\\text{bus}} = C_g S_g\n\\]\nOther type of generators, such as solar or wind renewables can be defined in the same way. The constant renewable power injection can also be viewed as negative load.\n\n\nLoad Model\nA constant power load is modeled as active and reactive power consumption at each bus. The load complex power injection can be written as \\[\nS_d = P_d + jQ_d\n\\]\n\n\nShunt Elements\nA shunt=connected element, such as a capacitor or an inductor, is modeled as a fixed impedance to ground at a bus, whose admittance is \\[\nY_{sh} = G_{sh} + jB_{sh}\n\\]\n\n\nNetwork Equations\nLet \\(V\\) be the bus voltage and \\(I_{\\text{bus}}\\) be the bus current injection. \\[\n\\begin{aligned}\nI_{\\text{bus}} = Y_{\\text{bus}} V \\\\\nI_f = Y_{f} V \\\\\nI_t = Y_{t} V\n\\end{aligned}\n\\] with the system admittance matrices defined as \\[\n\\begin{aligned}\nY_f & =\\left[Y_{f f}\\right] C_f+\\left[Y_{f t}\\right] C_t, \\\\\nY_t & =\\left[Y_{t t}\\right] C_f+\\left[Y_{t t}\\right] C_t, \\\\\nY_{\\text {bus }} & =C_f^{\\top} Y_f+C_t^{\\top} Y_t+\\left[Y_{s h}\\right] .\n\\end{aligned}\n\\] where \\([\\cdot]\\) denotes the diagonal matrix of the vector.\nIn detail, we have the bus current injection as \\[\nI_{\\text{bus}} = C_f^T[Y_{ff}]C_f V + C_f^T[Y_{ft}]C_t V + C_t^T[Y_{tf}]C_f V + C_t^T[Y_{tt}]C_t V + [Y_{sh}]V\n\\]\nTo understand this, using the first term as an example, \\(C_fV\\) is the voltage at the from bus of the branch, \\([Y_{ff}]C_fV\\) is the current flowing at the from bus of the branch, e.g. the \\(I_f\\). \\(C_f^T\\) is the transpose of \\(C_f\\), which is the incidence matrix of the branches connected to the bus.\nThen the complex power injection and flows can be written as (the power flow equation) \\[\n\\begin{aligned}\nS_{\\text {bus }}(V) & =[V] I_{\\text {bus }}^*=[V] Y_{\\text {bus }}^* V^*, \\\\\nS_f(V) & =\\left[C_f V\\right] I_f^*=\\left[C_f V\\right] Y_f^* V^*, \\\\\nS_t(V) & =\\left[C_t V\\right] I_t^*=\\left[C_t V\\right] Y_t^* V^* .\n\\end{aligned}\n\\] where \\((\\cdot)^\\star\\) is the element-wise conjugate operator on complex number. Note that \\((AB)^\\star = A^\\star B^\\star\\).\nThe bus injection can be written as (power injection balance) \\[\ng_S\\left(V, S_g\\right)=S_{\\text {bus }}(V)+S_d-C_g S_g=0 .\n\\]"
  },
  {
    "objectID": "posts/learning/power_system/power_system_operation.html#dc-model",
    "href": "posts/learning/power_system/power_system_operation.html#dc-model",
    "title": "Power System Operation: AC and DC Power Flow Model",
    "section": "DC Model",
    "text": "DC Model\nThe DC model takes three assumption on the AC model:\n\nThe voltage magnitude is fixed at 1 p.u., e.g., \\(v_i = e^{j\\theta_i}\\).\nThe branches are lossless, e.g., \\(y_s = 0\\). The line charging susceptance \\(b_c\\) is also ignored. Therefore, the branch admittance is \\(y_s = \\frac{1}{jx_s}\\).\nThe voltage angle difference on each branch is small, e.g., \\(\\sin(\\theta_f - \\theta_t - \\theta_{\\text{shift}}) \\approx \\theta_f - \\theta_t - \\theta_{\\text{shift}}\\).\n\nBased on the three assumptions, the branch admittance matrix \\(Y_{b r}\\) Equation 1 can be simplified as \\[\nY_{b r} \\approx \\frac{1}{j x_s}\\left[\\begin{array}{cc}\n\\frac{1}{\\tau^2} & -\\frac{1}{\\tau e^{-j \\theta_{\\text {shift }}}} \\\\\n-\\frac{1}{\\tau e^{j \\theta_{\\text {shift }}}} & 1\n\\end{array}\\right]\n\\]\nLet \\(b_i = \\frac{1}{x_s^i\\tau^i}\\) and \\(B_{ff}\\) be the vector of \\(b_i\\) for all branches. Let \\(P_{f,\\text{shift}}\\) be the vector of \\(-\\theta_{\\text{shift}}^ib_i\\). Then the bus power injection can be written as \\[\nP_{\\text{bus}} = B_{\\text{bus}}(\\Theta) + P_{\\text{bus,shift}}\n\\] where \\[\nP_{\\text {bus,shift }}=\\left(C_f-C_t\\right)^{\\top} P_{f, \\text { shift }}\n\\]\nThe power flow equation can be written as \\[\nP_f(\\Theta)=B_f \\Theta+P_{f, \\text { shift }} = -P_t(\\Theta)\n\\]\nThe DC-model system matrices can be written as \\[\n\\begin{aligned}\nB_f & =\\left[B_{f f}\\right]\\left(C_f-C_t\\right), \\\\\nB_{\\text {bus }} & =\\left(C_f-C_t\\right)^{\\top} B_f .\n\\end{aligned}\n\\]\nThe bus power injection balance can be written as \\[\ng_P\\left(\\Theta, P_g\\right)=B_{\\mathrm{bus}} \\Theta+P_{\\mathrm{bus}, \\text { shift }}+P_d+G_{s h}-C_g P_g=0\n\\]"
  },
  {
    "objectID": "posts/learning/autometic_differentiation/deep_implicit_layers.html",
    "href": "posts/learning/autometic_differentiation/deep_implicit_layers.html",
    "title": "Deep Implicit Layers: Fixed-Point Iteration",
    "section": "",
    "text": "In the previous posts, I summarize the mathematical background of the adjoint method for linear system and nonlinear system. This post will summarize them in the view of deep implicit layers. The main reference of this post is Deep Implicit Layers.\nThe nonlinear equation \\(g(x,z)=0\\) can be viewed beyond the a simple algebraic equation. Similiar to the sensitivity analysis of the equation, we can design the neural network as an implicit function of the parameter and the solution. The gradinet used for backpropagation can be found by the same idea of sensitivity analysis in the previous posts. This means that we can 1. Encode the implicit layer with physical meaning as part of the neural network. 2. Regard the entire neural network or part of it as a implicit model. For example, ResNet can be viewed as NeuralODE and feedforward neural network can be viewed as deep equilibrium model.\n\nImplicit layers for different types of equations.\n\n\n\n\n\n\nEquation Type\nNeural Network\n\n\n\n\nAlgebraic equation (fixed point iteration)\nDeep equilibrium model\n\n\nOrdinary differential equation\nNeuralODE\n\n\nConvex optimization\nDifferentiable convex layer\n\n\n\nThe benefits of using implicit layers are 1. The solution can be found by off-the-shelf solver, regardless of the the layer itself. E.g., the fixed point iteration can be solved by Newton’s method; the ODE can be solved by the ODE solver such as Euler’s method; the convex optimization can be solved by the convex optimization solver, such as ADMM. And more. 2. Because the solution procedure is separated from the layer, it does not need to be recorded on the computational graph (although the solution procedure can be unrolled on the computatonal graph). This improves the memory efficiency and numetical stability. 3. Because the forward pass of implicit layer requires a solution procedure which is usually an iterative process (thus repeated nonlinearity), the representation power of the implicit layer is stronger than the explicit layer."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/deep_implicit_layers.html#introduction",
    "href": "posts/learning/autometic_differentiation/deep_implicit_layers.html#introduction",
    "title": "Deep Implicit Layers: Fixed-Point Iteration",
    "section": "",
    "text": "In the previous posts, I summarize the mathematical background of the adjoint method for linear system and nonlinear system. This post will summarize them in the view of deep implicit layers. The main reference of this post is Deep Implicit Layers.\nThe nonlinear equation \\(g(x,z)=0\\) can be viewed beyond the a simple algebraic equation. Similiar to the sensitivity analysis of the equation, we can design the neural network as an implicit function of the parameter and the solution. The gradinet used for backpropagation can be found by the same idea of sensitivity analysis in the previous posts. This means that we can 1. Encode the implicit layer with physical meaning as part of the neural network. 2. Regard the entire neural network or part of it as a implicit model. For example, ResNet can be viewed as NeuralODE and feedforward neural network can be viewed as deep equilibrium model.\n\nImplicit layers for different types of equations.\n\n\n\n\n\n\nEquation Type\nNeural Network\n\n\n\n\nAlgebraic equation (fixed point iteration)\nDeep equilibrium model\n\n\nOrdinary differential equation\nNeuralODE\n\n\nConvex optimization\nDifferentiable convex layer\n\n\n\nThe benefits of using implicit layers are 1. The solution can be found by off-the-shelf solver, regardless of the the layer itself. E.g., the fixed point iteration can be solved by Newton’s method; the ODE can be solved by the ODE solver such as Euler’s method; the convex optimization can be solved by the convex optimization solver, such as ADMM. And more. 2. Because the solution procedure is separated from the layer, it does not need to be recorded on the computational graph (although the solution procedure can be unrolled on the computatonal graph). This improves the memory efficiency and numetical stability. 3. Because the forward pass of implicit layer requires a solution procedure which is usually an iterative process (thus repeated nonlinearity), the representation power of the implicit layer is stronger than the explicit layer."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/deep_implicit_layers.html#fixed-point-iteration",
    "href": "posts/learning/autometic_differentiation/deep_implicit_layers.html#fixed-point-iteration",
    "title": "Deep Implicit Layers: Fixed-Point Iteration",
    "section": "Fixed Point Iteration",
    "text": "Fixed Point Iteration\nA fixed point iteration \\[\nz^{\\star}=\\tanh \\left(W z^{\\star}+x\\right)\n\\] can be written as a nonlinear equation \\[\ng(x, z)=z-\\tanh \\left(W z+x\\right)=0\n\\] where \\(W\\in\\mathbb{R}^{n\\times n}\\).\n\nForward Pass\nFor a given \\(x\\), Newton’s method can be used to iteratively solve the equation, \\[\nz:= z - \\left(\\frac{\\partial g}{\\partial z}\\right)^{-1} g(x,z)\n\\]\nThe partial Jacobian of \\(g\\) with repect to \\(z\\) can be found by automatic differentation tool such as torch.func.jacfwd() in general. For the simple case, it can be found analytically as \\[\n\\frac{\\partial g}{\\partial z} = I - \\text{diag}(\\tanh'(Wz+x))W\n\\]\nNote that the forward pass is not recorded on the computational graph.\n\n\nBackward Pass\nWe can use the same technique in nonlinear adjoint method to do the reverse mode differentiation or bachpropagation. To have a quick review, do the total derivative of \\(g\\) with respect to \\(x\\): \\[\n\\frac{d g}{d x} = \\frac{\\partial g}{\\partial x} + \\frac{\\partial g}{\\partial z} \\frac{d z}{d x} = 0\n\\]\nThe Jacobian \\(\\frac{d z}{d x}\\) can be found as \\[\n\\frac{d z}{d x} = -\\left(\\frac{\\partial g}{\\partial z}\\right)^{-1} \\frac{\\partial g}{\\partial x}\n\\]\nNote that the term \\(\\frac{\\partial g}{\\partial x}\\) has already been computed in the forward pass, e.g. we can directly use the Jacobian and its inverse calculated in the last iteration of Newton’s method.\n\n\n\n\n\n\nTip\n\n\n\nThis is a very common observation in various implicit layer application where you can do the reverse-mode differentiation for ‘free’.\n\n\nDirectly solving the Jacobian \\(\\frac{d z}{d x}\\) is not efficient. As in the Tangent method, the number of linear systems that need to solve is the same as the number of the parameters \\(x\\).\nModern deep learning model is trained by reverse-mode differentiation, which is more efficient than the forward-mode differentiation. Let \\(\\ell(\\cdot)\\) be the scalar loss function, the Jacobian with respect to \\(x\\) can be found by \\[\n\\frac{d \\ell}{d x} = \\frac{d \\ell}{d z} \\frac{d z}{d x} = \\frac{d \\ell}{d z} \\left(-\\left(\\frac{\\partial g}{\\partial z}\\right)^{-1} \\frac{\\partial g}{\\partial x}\\right)\n\\]\nAgain we can find the vector Jacobian product of the first two terms as the new adjoint of \\(z\\) denoted as \\(\\dot{z}\\): \\[\n\\left(\\frac{\\partial g}{\\partial z}\\right)^T \\dot{z} = \\left(\\frac{d \\ell}{d z}\\right)^T\n\\tag{1}\\]\nThen we need to re-engage \\(x\\) on the computational graph. This can be done by calculating \\[\nz:= z - g(x,z)\n\\] whose gradient with respect to \\(x\\) is \\(-\\frac{\\partial g}{\\partial x}\\), which is the last term in \\(\\frac{d\\ell}{dx}\\).\n(The minus sign depends how the adjoint system is defined.)\nTo sum up, the process of forward and backward pass of fixed-point iteration is\n\nForward pass: Solve the nonlinear equation \\(g(x,z)=0\\) by off-the-shelf solver. This is outside the automatic differentiation tape.\nIn the automatic differentiation tape, re-engage \\(x\\) on the computational graph by \\(z:= z - g(x,z)\\).\nModify the gradient of the above \\(z\\) as the solution to Equation 1, using the register_hook() method in PyTorch."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/deep_implicit_layers.html#pytorch-implementation",
    "href": "posts/learning/autometic_differentiation/deep_implicit_layers.html#pytorch-implementation",
    "title": "Deep Implicit Layers: Fixed-Point Iteration",
    "section": "PyTorch Implementation",
    "text": "PyTorch Implementation\nHere, I implement a simple fixed-point iteration layer in PyTorch and compare it to the previous method in the nonlinear adjoint method.\nimport torch\nfrom torch import nn\n\ndef loss_fn(z):\n    return torch.sum(z**2, axis = -1).mean()\n\nclass FixedPointLayer(torch.nn.Module):\n    def __init__(self, W, tol = 1e-4, max_iter = 1):\n        super(FixedPointLayer, self).__init__()\n        self.W = torch.nn.Parameter(W, requires_grad = True)\n        self.tol = tol\n        self.max_iter = max_iter\n        # implement by vmap\n        self.implicit_model = torch.vmap(self.implicit_model_)\n        self.jac_batched = torch.vmap(torch.func.jacfwd(self.implicit_model_, argnums = 0))\n\n    def implicit_model_(self, z, x):\n        return z - torch.tanh(self.W @ z + x)\n    \n    def newton_step(self, z, x, g):\n        J = self.jac_batched(z, x)\n        z = z - torch.linalg.solve(J, g)\n        return z, J\n\n    def forward(self, x):\n        self.iteration = 0\n        with torch.no_grad():\n            z = torch.tanh(x)\n            while self.iteration &lt; self.max_iter:\n                g = self.implicit_model(z, x)\n                self.err = torch.norm(g)\n\n                if self.err &lt; self.tol:\n                    break\n\n                # newton's method\n                z, J = self.newton_step(z, x, g)\n                self.iteration += 1\n        \n        # re-engage the autograd tape\n        z = z - self.implicit_model(z, x)\n        z.register_hook(lambda grad : torch.linalg.solve(J.transpose(1,2), grad))\n\n        return z\n\ndef implicit_model(W, x, z):\n    # the g function\n    return z - torch.tanh(W @ z + x)\n\n\ndef implicit_model_test(W, x, z):\n\n    if x.dim() == 1:\n        # single sample case\n        print('using the implicit model on one sample')\n        z_ = z.clone().detach()\n        x_ = x.clone().detach()\n        \n        dl_dz = torch.func.grad(loss_fn)(z_)\n        df_dW, df_dz = torch.func.jacfwd(implicit_model, argnums = (0,2))(W, x_, z_)\n        \n        adjoint_variable = torch.linalg.solve(df_dz.T, -dl_dz)\n        \n        dl_dW = torch.einsum('i,ikl-&gt;kl', adjoint_variable, df_dW)\n    \n    else:\n        print('using the implicit model on all samples')\n        z = z.clone().detach()\n        x = x.clone().detach()\n        \n        dl_dz = torch.func.grad(loss_fn)(z)\n\n        jacfwd_batched = torch.vmap(torch.func.jacfwd(implicit_model, argnums = (0,2)), in_dims = (None, 0, 0))\n        df_dW, df_dz = jacfwd_batched(W, x, z)\n\n        adjoint_variable = torch.linalg.solve(df_dz.transpose(1,2), -dl_dz)\n\n        dl_dW = torch.einsum('bi,bikl-&gt;kl', adjoint_variable, df_dW)\n    \n    print('dl_dz', dl_dz.shape)\n    print('df_dW', df_dW.shape)\n    print('df_dz', df_dz.shape)\n    print('adjoint_variable', adjoint_variable.shape)\n    print('dl_dW', dl_dW)\n\n# maint function\ntorch.random.manual_seed(0)\n\nbatch_size = 10\nn = 5\nW = torch.randn(n,n).double() * 0.5\nx = torch.randn(batch_size,n, requires_grad=True).double()\n\nprint('using the model')\nmodel = FixedPointLayer(W, tol=1e-10, max_iter = 50).double()\n\n# check with the numerical gradient\ntorch.autograd.gradcheck(model, x, check_undefined_grad=False, raise_exception=True)\n\nz = model(x)\nloss = loss_fn(z)\nloss.backward()\nprint(model.W.grad)\n\n# implicit model method\nimplicit_model_test(W, x[0], z[0])\nimplicit_model_test(W, x, z)\nIn the above example, torch.vmap() is used for multi-batch implementation of the implicit function and the Jacobian. The torch.func.jacfwd() is used for the Jacobian calculation. Note that the Jacobian \\(\\frac{\\partial g}{\\partial z}\\) is found by automatic differentiation, instead of analytical computation. The torch.linalg.solve() is used for the linear system solution. The torch.einsum() is used for the matrix multiplication.\nIn detail, in\njacfwd_batched = torch.vmap(torch.func.jacfwd(implicit_model, argnums = (0,2)), in_dims = (None, 0, 0)\nThe torch.func.jacfwd(implicit_model, argnums = (0,2)) is to find the Jacobian of the implicit model with respect to \\(W\\) and \\(z\\). The torch.vmap() is to apply the torch.func.jacfwd() to each batch of the input. The in_dims = (None, 0, 0) is to specify that only z and x are batched while W is not."
  }
]