<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Wangkun&#39;s Blog</title>
<link>https://xuwkk.github.io/blog/</link>
<atom:link href="https://xuwkk.github.io/blog/index.xml" rel="self" type="application/rss+xml"/>
<description>This is a blog for sharing my thoughts and notes. I will post my learning experience on machine learning, optimization, and power systems.</description>
<generator>quarto-1.5.40</generator>
<lastBuildDate>Tue, 03 Sep 2024 23:00:00 GMT</lastBuildDate>
<item>
  <title>ADMM: the OSQP Solver Explained!</title>
  <dc:creator>Wangkun Xu</dc:creator>
  <link>https://xuwkk.github.io/blog/posts/learning/optimization/osqp.html</link>
  <description><![CDATA[ 





<p>This post introduces the OSQP solver, which is an operatir splitting solver for quadratic programs. The original paper is <em>Stellato, Bartolomeo, et al.&nbsp;“OSQP: An operator splitting solver for quadratic programs.” Mathematical Programming Computation 12.4 (2020): 637-672.</em>. We also borrow the basic idea from the ADMM review <em>Boyd, Stephen, et al.&nbsp;“Distributed optimization and statistical learning via the alternating direction method of multipliers.” Foundations and Trends® in Machine learning 3.1 (2011): 1-122.</em>.</p>
<p>There are many features of the OSQP, compared to other ADMM-based algorithms. For example,</p>
<ol type="1">
<li>The algorithm does <strong>not</strong> require on the problem data such as positive definiteness of the objective function or LICQ condition of the constraints, meaning that the algorithm is applicable to the entire class of convex quadratic programs.</li>
<li>The algorithm requires the solution of a quasi-definite linear system with the same coefficient matrix at almost every iteration, which is very efficient. (usually ten times faster than interior-point methods.)</li>
<li>The method also supports factorization caching and warm starting, making it suitable for <em>parametric programming</em>.</li>
</ol>
<p>In contrast, the interior-point methods are not easily warm started and do not scale well for very large problems. ADMM, instead, has been shown to reliably provide <strong>modest accurate</strong> solutions to QPs in a relatively small number of computationally inexpensive iterations.</p>
<p>The drawback of ADMM (or any other first-order method) is that its convergence is highly dependent on the choice of step-size parameter and the problem data.</p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>The OSQP considers the following QP problem, <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Barray%7D%7Bll%7D%0A%5Coperatorname%7Bminimize%7D%20&amp;%20(1%20/%202)%20x%5ET%20P%20x+q%5ET%20x%20%5C%5C%0A%5Ctext%20%7B%20subject%20to%20%7D%20&amp;%20l%20%5Cleq%20A%20x%20%5Cleq%20u,%0A%5Cend%7Barray%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?P%20%5Cin%20%5Cmathbb%7BS%7D%5E%7Bn%7D_%7B+%7D">, <img src="https://latex.codecogs.com/png.latex?q%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%7D">, <img src="https://latex.codecogs.com/png.latex?A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20n%7D">, <img src="https://latex.codecogs.com/png.latex?l%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%7D%20%5Ccup%20-%5Cinfty%5Em">, and <img src="https://latex.codecogs.com/png.latex?u%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%7D%20%5Ccup%20%5Cinfty%5Em">. Note that the linear programming can be considered as a special case with <img src="https://latex.codecogs.com/png.latex?P=0">.</p>
<p>Equivalently, the problem can be rewritten by introducing the slack variables <img src="https://latex.codecogs.com/png.latex?z"> <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Barray%7D%7Bll%7D%0A%5Coperatorname%7Bminimize%7D%20&amp;%20(1%20/%202)%20x%5ET%20P%20x+q%5ET%20x%20%5C%5C%0A%5Ctext%20%7B%20subject%20to%20%7D%20&amp;%20A%20x=z%20%5C%5C%0A&amp;%20l%20%5Cleq%20z%20%5Cleq%20u%0A%5Cend%7Barray%7D%0A"></p>
<section id="optimal-condition" class="level3">
<h3 class="anchored" data-anchor-id="optimal-condition">Optimal Condition</h3>
The KKT condition of the original problem is $$
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0APx%20+%20q%20+%20A%5ET%20y%5Eu%20-%20A%5ET%20y%5El%20&amp;%20=%200%20%5C%5C%0AAx%20-%20u%20&amp;%20%5Cleq%200%20%5C%5C%0A-Ax%20+%20l%20&amp;%20%5Cleq%200%20%5C%5C%0Ay%5Eu%20%5Ccirc%20(Ax%20-%20u)%20&amp;%20=%200%20%5C%5C%0Ay%5El%20%5Ccirc%20(-Ax%20+%20l)%20&amp;%20=%200%20%5C%5C%0Ay%5Eu%20&amp;%20%5Cgeq%200%20%5C%5C%0Ay%5El%20&amp;%20%5Cgeq%200%0A%5Cend%7Baligned%7D">
<p>where <img src="https://latex.codecogs.com/png.latex?y%5Eu"> and <img src="https://latex.codecogs.com/png.latex?y%5El"> are the dual variables associated with the upper and lower constraints.</p>
<p>Now let <img src="https://latex.codecogs.com/png.latex?y=y%5Eu%20-%20y%5El"> and let <img src="https://latex.codecogs.com/png.latex?Ax%20=%20z">. The first KKT condition becomes <img src="https://latex.codecogs.com/png.latex?Px%20+%20q%20+A%5ETy%20=%200">. The second and third conditions becomes <img src="https://latex.codecogs.com/png.latex?l%20%5Cleq%20z%20%5Cleq%20u">. Moreover, when <img src="https://latex.codecogs.com/png.latex?y%5Eu_i%20%3E%200">, we have <img src="https://latex.codecogs.com/png.latex?A_ix=u_i">. Then if at the same time <img src="https://latex.codecogs.com/png.latex?y%5El_i%20%3E%200">, we have <img src="https://latex.codecogs.com/png.latex?A_ix=l_i">, which means that <img src="https://latex.codecogs.com/png.latex?y%5El_i=y_i%5Eu">, e.g., an equality constraint. Otherwise, we must have <img src="https://latex.codecogs.com/png.latex?y%5El=0">. Concequentlty, the complementary slackness conditions become <img src="https://latex.codecogs.com/png.latex?y_+%5ET%5Ccirc(z-u)=0"> and <img src="https://latex.codecogs.com/png.latex?y_-%5ET%5Ccirc(z-l)=0">. To sum up, the KKT condition can be rewritten as <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0APx%20+%20q%20+%20A%5ETy%20=%200%20%5C%5C%0AAx%20=%20z%20%5C%5C%0Al%20%5Cleq%20z%20%5Cleq%20u%20%5C%5C%0Ay_+%20%5Ccirc%20(z-u)%20=%200%20%5C%5C%0Ay_-%20%5Ccirc%20(z-l)%20=%200%20%5C%5C%0A%5Cend%7Baligned%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?y"> be the slack variable for the equality constraint.</p>
<p>Moreover, we can define the primal and dual residuals as <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0Ar_%7B%5Ctext%20%7Bprim%20%7D%7D%20&amp;%20=A%20x-z%20%5C%5C%0Ar_%7B%5Ctext%20%7Bdual%20%7D%7D%20&amp;%20=P%20x+q+A%5ET%20y%0A%5Cend%7Baligned%7D%0A"></p>
</section>
</section>
<section id="solution-with-admm" class="level2">
<h2 class="anchored" data-anchor-id="solution-with-admm">Solution with ADMM</h2>
<p>By introducing auxiliary variables <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bx%7D%5Cin%5Cmathbb%7BR%7D%5En"> and <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bz%7D%5Cin%5Cmathbb%7BR%7D%5Em">, we can have an equivalent problem, <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Barray%7D%7Bll%7D%0A%5Coperatorname%7Bminimize%7D%20&amp;%20(1%20/%202)%20f(%5Ctilde%7Bx%7D,%5Ctilde%7Bz%7D)%20+%20%5Cmathcal%7BI%7D_%5Cmathcal%7BC%7D(z)%20%5C%5C%0A%5Ctext%7B%20subject%20to%20%7D%20&amp;%20(%5Ctilde%7Bx%7D,%5Ctilde%7Bz%7D)%20=%20(x,z)%0A%5Cend%7Barray%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?f(%5Ctilde%7Bx%7D,%5Ctilde%7Bz%7D)%20=%20%5Ctilde%7Bx%7D%5ET%20P%20%5Ctilde%7Bx%7D%20+%20q%5ET%20%5Ctilde%7Bx%7D"> with domain <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7Bdom%7D%20f%20=%20%5C%7B(%5Ctilde%7Bx%7D,%5Ctilde%7Bz%7D):A%5Ctilde%7Bx%7D%20=%20%5Ctilde%7Bz%7D%5C%7D"> and the indicator function <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BI%7D_%7B%5Cmathcal%7BC%7D%7D(z)=%20%5Cbegin%7Bcases%7D0%20&amp;%20z%20%5Cin%20%5Cmathcal%7BC%7D%20%5C%5C%20+%5Cinfty%20&amp;%20%5Ctext%20%7B%20otherwise%20%7D%5Cend%7Bcases%7D%0A"></p>
<p>Similar to the standard ADMM, we can view <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bx%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bz%7D"> as the primal variables, and <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?z"> as the slack variables. Therefore, the first step is to update <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bx%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bz%7D">. The second step is to update <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?z">. Both by minimising the Augmented Lagrangian function. The third step is to update the dual variable <img src="https://latex.codecogs.com/png.latex?y">.</p>
<p>In detail, the unscaled ADMM algorithm is as follows, <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%5Cleft(%5Ctilde%7Bx%7D%5E%7Bk+1%7D,%20%5Ctilde%7Bz%7D%5E%7Bk+1%7D%5Cright)%20%5Cleftarrow%20%5Cunderset%7B(%5Ctilde%7Bx%7D,%20%5Ctilde%7Bz%7D):%20A%20%5Ctilde%7Bx%7D=%5Ctilde%7Bz%7D%7D%7B%5Coperatorname%7Bargmin%7D%7D(1%20/%202)%20%5Ctilde%7Bx%7D%5ET%20P%20%5Ctilde%7Bx%7D+q%5ET%20%5Ctilde%7Bx%7D%20%5C%5C%0A&amp;+(%5Csigma%20/%202)%5Cleft%5C%7C%5Ctilde%7Bx%7D-x%5Ek+%5Csigma%5E%7B-1%7D%20w%5Ek%5Cright%5C%7C_2%5E2+(%5Crho%20/%202)%5Cleft%5C%7C%5Ctilde%7Bz%7D-z%5Ek+%5Crho%5E%7B-1%7D%20y%5Ek%5Cright%5C%7C_2%5E2%0A%5Cend%7Baligned%7D%0A"> <img src="https://latex.codecogs.com/png.latex?%0Ax%5E%7Bk+1%7D%20%5Cleftarrow%20%5Calpha%20%5Ctilde%7Bx%7D%5E%7Bk+1%7D+(1-%5Calpha)%20x%5Ek+%5Csigma%5E%7B-1%7D%20w%5Ek%0A"> <img src="https://latex.codecogs.com/png.latex?%0Az%5E%7Bk+1%7D%20%5Cleftarrow%20%5CPi%5Cleft(%5Calpha%20%5Ctilde%7Bz%7D%5E%7Bk+1%7D+(1-%5Calpha)%20z%5Ek+%5Crho%5E%7B-1%7D%20y%5Ek%5Cright)%0A"> <img src="https://latex.codecogs.com/png.latex?%0Aw%5E%7Bk+1%7D%20%5Cleftarrow%20w%5Ek+%5Csigma%5Cleft(%5Calpha%20%5Ctilde%7Bx%7D%5E%7Bk+1%7D+(1-%5Calpha)%20x%5Ek-x%5E%7Bk+1%7D%5Cright)%0A"> <img src="https://latex.codecogs.com/png.latex?%0Ay%5E%7Bk+1%7D%20%5Cleftarrow%20y%5Ek+%5Crho%5Cleft(%5Calpha%20%5Ctilde%7Bz%7D%5E%7Bk+1%7D+(1-%5Calpha)%20z%5Ek-z%5E%7Bk+1%7D%5Cright)%0A"> where</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Csigma%3E0"> and <img src="https://latex.codecogs.com/png.latex?%5Crho%3E0"> are the penalty/step parameters.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Calpha%5Cin(0,2)"> is the relaxation parameter.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5CPi"> is the projection operator onto the feasible set <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D">.</li>
<li><img src="https://latex.codecogs.com/png.latex?(w%5Ek,y%5Ek)"> are the dual variables associated with the <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bx%7D%20=%20x"> and <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bz%7D%20=%20z"> constraints.</li>
<li>It can be derived <img src="https://latex.codecogs.com/png.latex?w%5E%7Bk+1%7D=0"> for all <img src="https://latex.codecogs.com/png.latex?k%5Cgeq0">. Therefore, the <img src="https://latex.codecogs.com/png.latex?w"> update can be omitted.</li>
</ul>
<section id="solving-the-linear-system" class="level3">
<h3 class="anchored" data-anchor-id="solving-the-linear-system">Solving the linear system</h3>
<p>The introduction of <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bx%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bz%7D"> ensures that the first update is always solvable even the when <img src="https://latex.codecogs.com/png.latex?P=0"> and <img src="https://latex.codecogs.com/png.latex?A"> is not full row rank (the LICQ consition does not hold).</p>
<p>The <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bx%7D,%5Ctilde%7Bz%7D"> update is to solve a equality constrained QP, whose KKT condition is given as (note that <img src="https://latex.codecogs.com/png.latex?w%5Ek=0">): <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AP%20%5Ctilde%7Bx%7D%5E%7Bk+1%7D+q+%5Csigma%5Cleft(%5Ctilde%7Bx%7D%5E%7Bk+1%7D-x%5Ek%5Cright)+A%5ET%20v%5E%7Bk+1%7D%20&amp;%20=0%20%5C%5C%0A%5Crho%5Cleft(%5Ctilde%7Bz%7D%5E%7Bk+1%7D-z%5Ek%5Cright)+y%5Ek-v%5E%7Bk+1%7D%20&amp;%20=0%20%5C%5C%0AA%20%5Ctilde%7Bx%7D%5E%7Bk+1%7D-%5Ctilde%7Bz%7D%5E%7Bk+1%7D%20&amp;%20=0%0A%5Cend%7Baligned%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?v%5E%7Bk+1%7D"> is the dual variable associated with the equality constraint. Eliminating <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bz%7D%5E%7Bk+1%7D"> results in <img src="https://latex.codecogs.com/png.latex?%0A%5Cunderbrace%7B%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%0AP+%5Csigma%20I%20&amp;%20A%5ET%20%5C%5C%0AA%20&amp;%20-%5Crho%5E%7B-1%7D%20I%0A%5Cend%7Barray%7D%5Cright%5D%7D_%7BK%7D%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%0A%5Ctilde%7Bx%7D%5E%7Bk+1%7D%20%5C%5C%0Av%5E%7Bk+1%7D%0A%5Cend%7Barray%7D%5Cright%5D=%5Cunderbrace%7B%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%0A%5Csigma%20x%5Ek-q%20%5C%5C%0Az%5Ek-%5Crho%5E%7B-1%7D%20y%5Ek%0A%5Cend%7Barray%7D%5Cright%5D%7D_%7By%7D%0A"> and <img src="https://latex.codecogs.com/png.latex?%0A%5Ctilde%7Bz%7D%5E%7Bk+1%7D=z%5Ek+%5Crho%5E%7B-1%7D%5Cleft(%5Cnu%5E%7Bk+1%7D-y%5Ek%5Cright)%0A"></p>
<p>Note that <img src="https://latex.codecogs.com/png.latex?K"> is always invertible. For a given problem, if <img src="https://latex.codecogs.com/png.latex?%5Crho"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma"> are unchanged during the iteration, we can factorize <img src="https://latex.codecogs.com/png.latex?K"> once and reuse the factorization at each iteration. For example, in pytorch,</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb1-2">LU, pivots <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.linalg.lu_factor(K)</span>
<span id="cb1-3">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.linalg.lu_solve(LU, pivots, b)</span></code></pre></div>
<p>which significantly reduces the computational cost. For instance, the complexity of the factorization is <img src="https://latex.codecogs.com/png.latex?O(n%5E3)">. In interior-point methods, this complexity is repeated at each iteration but in OSQP, we only need to factorize once (or when <img src="https://latex.codecogs.com/png.latex?%5Crho"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma"> change).</p>
</section>
<section id="stopping-criterion" class="level3">
<h3 class="anchored" data-anchor-id="stopping-criterion">Stopping criterion</h3>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5C%7Cr_%7B%5Ctext%20%7Bprim%20%7D%7D%5Ek%5Cright%5C%7C_%7B%5Cinfty%7D%20%5Cleq%20%5Cvarepsilon_%7B%5Ctext%20%7Bprim%20%7D%7D,%20%5Cquad%5Cleft%5C%7Cr_%7B%5Ctext%20%7Bdual%20%7D%7D%5Ek%5Cright%5C%7C_%7B%5Cinfty%7D%20%5Cleq%20%5Cvarepsilon_%7B%5Ctext%20%7Bdual%20%7D%7D%0A"> with <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cvarepsilon_%7B%5Ctext%20%7Bprim%20%7D%7D%20&amp;%20=%5Cvarepsilon_%7B%5Ctext%20%7Babs%20%7D%7D+%5Cvarepsilon_%7B%5Ctext%20%7Brel%20%7D%7D%20%5Cmax%20%5Cleft%5C%7B%5Cleft%5C%7CA%20x%5Ek%5Cright%5C%7C_%7B%5Cinfty%7D,%5Cleft%5C%7Cz%5Ek%5Cright%5C%7C_%7B%5Cinfty%7D%5Cright%5C%7D%20%5C%5C%0A%5Cvarepsilon_%7B%5Ctext%20%7Bdual%20%7D%7D%20&amp;%20=%5Cvarepsilon_%7B%5Ctext%20%7Babs%20%7D%7D+%5Cvarepsilon_%7B%5Ctext%20%7Brel%20%7D%7D%20%5Cmax%20%5Cleft%5C%7B%5Cleft%5C%7CP%20x%5Ek%5Cright%5C%7C_%7B%5Cinfty%7D,%5Cleft%5C%7CA%5ET%20y%5Ek%5Cright%5C%7C_%7B%5Cinfty%7D,%5C%7Cq%5C%7C_%7B%5Cinfty%7D%5Cright%5C%7D%0A%5Cend%7Baligned%7D%0A"></p>
</section>
</section>
<section id="solution-polishing" class="level2">
<h2 class="anchored" data-anchor-id="solution-polishing">Solution Polishing</h2>
<p>Assume that the ‘less accurate’ solution <img src="https://latex.codecogs.com/png.latex?(x,y,z)"> can correctly identify the active constraints, we can refine the solution by solving a new QP with known active constraints as equality constraints.</p>
<p>Given the dual solution, we can define the set of lower- and upper-active constraints as <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathcal%7BL%7D%20&amp;%20=%5Cleft%5C%7Bi%20%5Cin%5C%7B1,%20%5Cldots,%20m%5C%7D%20%5Cmid%20y_i%3C0%5Cright%5C%7D%20%5C%5C%0A%5Cmathcal%7BU%7D%20&amp;%20=%5Cleft%5C%7Bi%20%5Cin%5C%7B1,%20%5Cldots,%20m%5C%7D%20%5Cmid%20y_i%3E0%5Cright%5C%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>Once the set of active constraints are known, then the solution can be polished by solving <img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5B%5Cbegin%7Barray%7D%7Bccc%7D%0AP%20&amp;%20A_%7B%5Cmathcal%7BL%7D%7D%5ET%20&amp;%20A_%7B%5Cmathcal%7BU%7D%7D%5ET%20%5C%5C%0AA_%7B%5Cmathcal%7BL%7D%7D%20&amp;%20%5C%5C%0AA_%7B%5Cmathcal%7BU%7D%7D%20&amp;%20&amp;%0A%5Cend%7Barray%7D%5Cright%5D%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%0Ax%20%5C%5C%0Ay_%7B%5Cmathcal%7BL%7D%7D%20%5C%5C%0Ay_%7B%5Cmathcal%7BU%7D%7D%0A%5Cend%7Barray%7D%5Cright%5D=%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%0A-q%20%5C%5C%0Al_%7B%5Cmathcal%7BL%7D%7D%20%5C%5C%0Au_%7B%5Cmathcal%7BU%7D%7D%0A%5Cend%7Barray%7D%5Cright%5D%0A"> <img src="https://latex.codecogs.com/png.latex?%0Ay_i=0,%20%5Cquad%20i%20%5Cnotin(%5Cmathcal%7BL%7D%20%5Ccup%20%5Cmathcal%7BU%7D)%0A"> <img src="https://latex.codecogs.com/png.latex?%0Az=A%20x%0A"></p>
<p>The above system is much smaller than the original system (e.g., reduce the size of the system from <img src="https://latex.codecogs.com/png.latex?n+2m"> to less than <img src="https://latex.codecogs.com/png.latex?2n"> when the problem is not degenerate).</p>
<p>However, the above system is not always solvable, e.g.&nbsp;when <img src="https://latex.codecogs.com/png.latex?P"> is not positive definite or when the problem is degenerate. In this case, a small perutrbation can be added <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20I"> can be added, <img src="https://latex.codecogs.com/png.latex?%0A%5Cunderbrace%7B%5Cleft%5B%5Cbegin%7Barray%7D%7Bccc%7D%0AP+%5Cdelta%20I%20&amp;%20A_%7B%5Cmathcal%7BL%7D%7D%5ET%20&amp;%20A_%7B%5Cmathcal%7BU%7D%7D%5ET%20%5C%5C%0AA_%7B%5Cmathcal%7BL%7D%7D%20&amp;%20-%5Cdelta%20I%20&amp;%20%5C%5C%0AA_%7B%5Cmathcal%7BU%7D%7D%20&amp;%20&amp;%20-%5Cdelta%20I%0A%5Cend%7Barray%7D%5Cright%5D%7D_%7BK%7D%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%0A%5Chat%7Bx%7D%5E%7B%5Cprime%7D%20%5C%5C%0A%5Chat%7By%7D_%7B%5Cmathcal%7BL%7D%7D%20%5C%5C%0A%5Chat%7By%7D_%7B%5Cmathcal%7BU%7D%7D%0A%5Cend%7Barray%7D%5Cright%5D%20=%20%5Cunderbrace%7B%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%0A-q%20%5C%5C%0Al_%7B%5Cmathcal%7BL%7D%7D%20%5C%5C%0Au_%7B%5Cmathcal%7BU%7D%7D%0A%5Cend%7Barray%7D%5Cright%5D%7D_%7Bg%7D%0A"> with small positive <img src="https://latex.codecogs.com/png.latex?%5Cdelta">.</p>
<p>To solve the problem, we can consider the following iteration: <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bt%7D%5E%7Bt+1%7D%20=%20(I%20-%20(K+%5CDelta%20K)%5E%7B-1%7DK)%5Chat%7Bt%7D%5Ek%20+%20(K+%5CDelta%20K)%5E%7B-1%7Dg%0A"></p>


</section>

 ]]></description>
  <category>Optimization</category>
  <guid>https://xuwkk.github.io/blog/posts/learning/optimization/osqp.html</guid>
  <pubDate>Tue, 03 Sep 2024 23:00:00 GMT</pubDate>
</item>
<item>
  <title>Bender’s Decomposition for (Mixed-Integer) Linear Programming</title>
  <dc:creator>Wangkun Xu</dc:creator>
  <link>https://xuwkk.github.io/blog/posts/learning/optimization/bender.html</link>
  <description><![CDATA[ 





<section id="benders-decomposition-for-lp" class="level2">
<h2 class="anchored" data-anchor-id="benders-decomposition-for-lp">Bender’s Decomposition for LP</h2>
<section id="formulation" class="level3">
<h3 class="anchored" data-anchor-id="formulation">Formulation</h3>
<p>The Bender’s decomposition is usually used for solving LP with complicating variables. A complicating variables are those varialbes avoiding the LP to be solved 1. distributedly or from the 2. straight-forward solution.</p>
<p>Consider the following LP: <span id="eq-LP"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmin_%7Bx,y%7D%20&amp;%20%5Cquad%20%5Csum_%7Bi=1%7D%5En%20c_i%20x_i%20+%20%5Csum_%7Bj=1%7D%5Em%20d_j%20y_j%20%5C%5C%0A%5Ctext%7Bs.t.%7D%20&amp;%20%5Cquad%20%5Csum_%7Bi=1%7D%5En%20a_%7Bli%7D%20x_i%20+%20%5Csum_%7Bj=1%7D%5Em%20b_%7Blj%7D%20y_j%20%5Cleq%20b%5E%7B(l)%7D,%20%5Cquad%20l=1,%5Cldots,q%20%5C%5C%0A&amp;%20%5Cquad%200%20%5Cleq%20x_i%20%5Cleq%20x_i%5E%7Bup%7D,%20%5Cquad%20i=1,%5Cldots,n%20%5C%5C%0A&amp;%20%5Cquad%200%20%5Cleq%20y_j%20%5Cleq%20y_j%5E%7Bup%7D,%20%5Cquad%20j=1,%5Cldots,m%0A%5Cend%7Baligned%7D%0A%5Ctag%7B1%7D"></span> where <img src="https://latex.codecogs.com/png.latex?x_i%5E%7Bup%7D"> and <img src="https://latex.codecogs.com/png.latex?y_j%5E%7Bup%7D"> are the upper bounds of <img src="https://latex.codecogs.com/png.latex?x_i"> and <img src="https://latex.codecogs.com/png.latex?y_j">, respectively.</p>
<p>If <img src="https://latex.codecogs.com/png.latex?x_i">s are known, this LP can be easily solved distributedly for each <img src="https://latex.codecogs.com/png.latex?j=1,%5Cldots,m">. The Bender’s decomposition is to efficiently solve subproblems for <img src="https://latex.codecogs.com/png.latex?y_j">s with the cost of iterations.</p>
<p>Introduce the <strong>value function</strong>: for given <img src="https://latex.codecogs.com/png.latex?x"> (as a constant), <span id="eq-value_function"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Calpha(x)%20=%20%5Cmin_%7By%7D%20&amp;%20%5Cquad%20%5Csum_%7Bj=1%7D%5Em%20d_j%20y_j%20%5C%5C%0A%5Ctext%7Bs.t.%7D%20&amp;%20%5Cquad%20%5Csum_%7Bj=1%7D%5Em%20b_%7Blj%7D%20y_j%20%5Cleq%20b%5E%7B(l)%7D%20-%20%5Csum_%7Bi=1%7D%5En%20a_%7Bli%7D%20x_i,%20%5Cquad%20l=1,%5Cldots,q%20%5C%5C%0A&amp;%20%5Cquad%200%20%5Cleq%20y_j%20%5Cleq%20y_j%5E%7Bup%7D,%20%5Cquad%20j=1,%5Cldots,m%0A%5Cend%7Baligned%7D%0A%5Ctag%7B2%7D"></span> which is convex on <img src="https://latex.codecogs.com/png.latex?x"> (the proof can be found in Boyd’s book). Therefore, <img src="https://latex.codecogs.com/png.latex?%5Calpha(x)"> can be approximated from the lower bound: <img src="https://latex.codecogs.com/png.latex?%0A%5Calpha(x)%20%5Cgeq%20%5Calpha(%5Ctilde%7Bx%7D)%20+%20%5Cnabla%20%5Calpha(%5Ctilde%7Bx%7D)%5ET%20(x%20-%20%5Ctilde%7Bx%7D)%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bx%7D"> is given.</p>
<p>The original LP becomes a Bilevel optimization: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmin_%7Bx%7D%20&amp;%20%5Cquad%20%5Csum_%7Bi=1%7D%5En%20c_i%20x_i%20+%20%5Calpha(x)%20%5C%5C%0A%5Ctext%7Bs.t.%7D%20&amp;%20%5Cquad%200%20%5Cleq%20x_i%20%5Cleq%20x_i%5E%7Bup%7D,%20%5Cquad%20i=1,%5Cldots,n%0A%5Cend%7Baligned%7D%0A"> where the lower level problem is the value function <img src="https://latex.codecogs.com/png.latex?%5Calpha(x)">.</p>
<p>The idea of Bender’s decomposition is now straight-forward. In the original LP Equation&nbsp;1, if we fix <img src="https://latex.codecogs.com/png.latex?x_i">s, the solution is a upper bound of the original LP. In the Bilevel optimization Equation&nbsp;2, if we approximate <img src="https://latex.codecogs.com/png.latex?%5Calpha(x)"> from the lower bound, the solution is a lower bound. Therefore, we can iteratively update the upper bound and the lower bound until they are close enough.</p>
</section>
<section id="algorithm" class="level3">
<h3 class="anchored" data-anchor-id="algorithm">Algorithm</h3>
<p>The iteration index is denoted as <img src="https://latex.codecogs.com/png.latex?(k)">. The algorithm is as follows:</p>
<p>Step 0: Initialize <img src="https://latex.codecogs.com/png.latex?x%5E%7B(0)%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Calpha%5E%7B(0)%7D"> by solving <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmin_%7Bx%7D%20&amp;%20%5Cquad%20%5Csum_%7Bi=1%7D%5En%20c_i%20x_i%20+%20%5Calpha%20%5C%5C%0A%5Ctext%7Bs.t.%7D%20&amp;%20%5Cquad%200%20%5Cleq%20x_i%20%5Cleq%20x_i%5E%7Bup%7D,%20%5Cquad%20i=1,%5Cldots,n%20%5C%5C%0A&amp;%20%5Cquad%20%5Calpha%20%5Cgeq%20%5Calpha%5E%7Bdown%7D%20%5C%5C%0A%5Cend%7Baligned%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Calpha%5E%7Bdown%7D"> is a given lower bound of <img src="https://latex.codecogs.com/png.latex?%5Calpha(x)">. Note that this optimization has solution at the boundary.</p>
<p>Step 1: Solve the subproblem (lower level) given by Equation&nbsp;2: <span id="eq-subproblem"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmin_%7By%7D%20&amp;%20%5Cquad%20%5Csum_%7Bj=1%7D%5Em%20d_j%20y_j%20%5C%5C%0A%5Ctext%7Bs.t.%7D%20&amp;%20%5Cquad%20%5Csum_%7Bj=1%7D%5Em%20b_%7Blj%7D%20y_j%20%5Cleq%20b%5E%7B(l)%7D%20-%20%5Csum_%7Bi=1%7D%5En%20a_%7Bli%7D%20x_i,%20%5Cquad%20l=1,%5Cldots,q%20%5C%5C%0A&amp;%20%5Cquad%200%20%5Cleq%20y_j%20%5Cleq%20y_j%5E%7Bup%7D,%20%5Cquad%20j=1,%5Cldots,m%20%5C%5C%0A&amp;%20%5Cquad%20x_i%20=%20x_i%5E%7B(k)%7D%20%5Cquad%20%5Clambda_i,%20%5Cquad%20i=1,%5Cldots,n%0A%5Cend%7Baligned%7D%0A%5Ctag%7B3%7D"></span> where <img src="https://latex.codecogs.com/png.latex?%5Clambda_i"> is the dual of the <img src="https://latex.codecogs.com/png.latex?i">-th constraint. The sensitivity <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20%5Calpha(x%5E%7B(k)%7D)"> equals to the dual variables. The solution is denoted as <img src="https://latex.codecogs.com/png.latex?y%5E%7B(k)%7D,%20%5Clambda%5E%7B(k)%7D">.</p>
<p>Step 2: Check the convergence. The upper bound is given by the subproblem: <img src="https://latex.codecogs.com/png.latex?%0AU%5E%7B(k)%7D%20=%20%5Csum_%7Bi=1%7D%5En%20c_i%20x_i%5E%7B(k)%7D%20+%20%5Csum_%7Bj=1%7D%5Em%20d_j%20y_j%5E%7B(k)%7D%0A"></p>
<p>While the lower bound is given by the master problem: <img src="https://latex.codecogs.com/png.latex?%0AL%5E%7B(k)%7D%20=%20%5Csum_%7Bi=1%7D%5En%20c_i%20x_i%5E%7B(k)%7D%20+%20%5Calpha%5E%7B(k)%7D%0A"></p>
<p>The gap is defined as: <img src="https://latex.codecogs.com/png.latex?%0AU%5E%7B(k)%7D%20-%20L%5E%7B(k)%7D%20=%20%5Csum_%7Bj=1%7D%5Em%20d_j%20y_j%5E%7B(k)%7D%20-%20%5Calpha%5E%7B(k)%7D%0A"></p>
<p>If the gap is small enough, stop. Otherwise, update <img src="https://latex.codecogs.com/png.latex?k%20=%20k%20+%201"> and go to Step 3.</p>
<p>Step 3: Update the master problem: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmin_%7Bx,%5Calpha%7D%20&amp;%20%5Cquad%20%5Csum_%7Bi=1%7D%5En%20c_i%20x_i%20+%20%5Calpha%20%5C%5C%0A%5Ctext%7Bs.t.%7D%20&amp;%20%5Cquad%200%20%5Cleq%20x_i%20%5Cleq%20x_i%5E%7Bup%7D,%20%5Cquad%20i=1,%5Cldots,n%20%5C%5C%0A&amp;%20%5Cquad%20%5Calpha%20%5Cgeq%20%5Csum_%7Bj=1%7D%5Em%20d_j%20y_j%5E%7B(v)%7D%20+%20%5Csum_%7Bi=1%7D%5En%20%5Clambda_i%5E%7B(v)%7D%20(x_i%20-%20x_i%5E%7B(v)%7D),%20%5Cquad%20v%20=%201,%20%5Ccdots,%20k-1%20%5C%5C%0A&amp;%20%5Cquad%20%5Calpha%20%5Cgeq%20%5Calpha%5E%7Bdown%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>Note that all the previous lower bounds (bender’s cut) are included. Therefore, the size of the master problem becomes larger. Go to step 1.</p>
</section>
<section id="dealing-with-the-infeasibility" class="level3">
<h3 class="anchored" data-anchor-id="dealing-with-the-infeasibility">Dealing with the Infeasibility</h3>
<p>The subproblem Equation&nbsp;3 may be infeasible due to the constraints on <img src="https://latex.codecogs.com/png.latex?x_i">. Therefore, a slack variable <img src="https://latex.codecogs.com/png.latex?s_i%20%5Cgeq%200"> is introduced: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmin_%7By,s%7D%20&amp;%20%5Cquad%20%5Csum_%7Bj=1%7D%5Em%20d_j%20y_j%20+%20M%20%5Csum_%7Bi=1%7D%5En%20s_i%20%5C%5C%0A%5Ctext%7Bs.t.%7D%20&amp;%20%5Cquad%20%5Csum_%7Bj=1%7D%5Em%20b_%7Blj%7D%20y_j%20%5Cleq%20b%5E%7B(l)%7D%20-%20%5Csum_%7Bi=1%7D%5En%20a_%7Bli%7D%20x_i%20+%20s_i,%20%5Cquad%20l=1,%5Cldots,q%20%5C%5C%0A&amp;%20%5Cquad%200%20%5Cleq%20y_j%20%5Cleq%20y_j%5E%7Bup%7D,%20%5Cquad%20j=1,%5Cldots,m%20%5C%5C%0A&amp;%20%5Cquad%20s_i%20%5Cgeq%200,%20%5Cquad%20i=1,%5Cldots,n%0A%5Cend%7Baligned%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?M"> is a large number.</p>
<p>Note that this is equivalent to adding the slack variable to the original LP Equation&nbsp;1 and group <img src="https://latex.codecogs.com/png.latex?s"> into the subproblem.</p>
</section>
</section>
<section id="benders-decomposition-for-milp" class="level2">
<h2 class="anchored" data-anchor-id="benders-decomposition-for-milp">Bender’s Decomposition for MILP</h2>
<p>The algorithm for MILP is very similar to LP. If <img src="https://latex.codecogs.com/png.latex?x"> is a set of integer variables (<img src="https://latex.codecogs.com/png.latex?x_i%5Cin%5Cmathbb%7BN%7D">), then treating <img src="https://latex.codecogs.com/png.latex?x"> as complicating variable results in a simple MILP master problem (as lower bound, note that the <img src="https://latex.codecogs.com/png.latex?%5Calpha"> is included in the master problem as continuous variable) and a LP subproblem (as upper bound).</p>


</section>

 ]]></description>
  <category>Optimization</category>
  <guid>https://xuwkk.github.io/blog/posts/learning/optimization/bender.html</guid>
  <pubDate>Tue, 23 Jul 2024 23:00:00 GMT</pubDate>
</item>
<item>
  <title>Constrained Optimization: the Basic</title>
  <dc:creator>Wangkun Xu</dc:creator>
  <link>https://xuwkk.github.io/blog/posts/learning/optimization/baisc.html</link>
  <description><![CDATA[ 





<p>This post summarized the basic concepts in contrained optimization. The KKT conditions, necessary condition and sufficient condition for optimality will also be introduced.</p>
<section id="general-definition-and-existence-condition" class="level2">
<h2 class="anchored" data-anchor-id="general-definition-and-existence-condition">General Definition and Existence Condition</h2>
<p>Consider the constrained optimization problem <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmin_x%20&amp;%20%5Cquad%20f(x)%20%5C%5C%0A%5Ctext%7Bs.t.%7D%20&amp;%20%5Cquad%20g(x)%20=0%20%5C%5C%0A&amp;%20%5Cquad%20h(x)%20%5Cleq%200%0A%5Cend%7Baligned%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?x%5Cin%5Cmathbb%7BR%7D%5En">, <img src="https://latex.codecogs.com/png.latex?f:%5Cmathbb%7BR%7D%5En%5Crightarrow%5Cmathbb%7BR%7D">, <img src="https://latex.codecogs.com/png.latex?g:%5Cmathbb%7BR%7D%5En%5Crightarrow%5Cmathbb%7BR%7D%5Em">, <img src="https://latex.codecogs.com/png.latex?g:%5Cmathbb%7BR%7D%5En%5Crightarrow%5Cmathbb%7BR%7D%5Ep">.</p>
<p>A point <img src="https://latex.codecogs.com/png.latex?x"> satisfying all constraints is an <em>admissible point</em>. The set of all admissible points are <em>admissible set</em>, denoted as <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BX%7D">.</p>
<p>In this post, it is assumed that <img src="https://latex.codecogs.com/png.latex?f,%20g,%20h"> are two times differentiable.</p>
<p>An <em>open ball</em> with center <img src="https://latex.codecogs.com/png.latex?x%5E%5Cstar"> and radius <img src="https://latex.codecogs.com/png.latex?%5Ctheta%3E0"> is <img src="https://latex.codecogs.com/png.latex?%0AB%5Cleft(x%5E%7B%5Cstar%7D,%20%5Ctheta%5Cright)=%5Cleft%5C%7Bx%20%5Cin%20%5Cmathbb%7BR%7D%5En%20%5Cmid%5Cleft%5C%7Cx-x%5E%7B%5Cstar%7D%5Cright%5C%7C%3C%5Ctheta%5Cright%5C%7D%0A"></p>
<p>A point <img src="https://latex.codecogs.com/png.latex?x%5E%5Cstar%20%5Cin%20%5Cmathcal%7BX%7D"> is a <em>constrained local minimizer</em> if there exists <img src="https://latex.codecogs.com/png.latex?%5Ctheta%3E0"> such that <img src="https://latex.codecogs.com/png.latex?%0Af(y)%20%5Cgeq%20f(x)%20%5Cquad%20%5Cforall%20y%20%5Cin%20%5Cmathcal%7BX%7D%20%5Ccap%20B%5Cleft(x%5E%7B%5Cstar%7D,%20%5Ctheta%5Cright)%0A"></p>
<p>A point <img src="https://latex.codecogs.com/png.latex?x%5E%5Cstar%20%5Cin%20%5Cmathcal%7BX%7D"> is a <em>constrained global minimizer</em> if <img src="https://latex.codecogs.com/png.latex?%0Af(y)%20%5Cgeq%20f(x)%20%5Cquad%20%5Cforall%20y%20%5Cin%20%5Cmathcal%7BX%7D%0A"></p>
<p>If the <img src="https://latex.codecogs.com/png.latex?%5Cgeq"> becomes <img src="https://latex.codecogs.com/png.latex?%3E"> with the extra condition <img src="https://latex.codecogs.com/png.latex?y%5Cneq%20x%5E%5Cstar">, then <img src="https://latex.codecogs.com/png.latex?x%5E%5Cstar"> is a <em>strictly constrained (global) minimizer</em>.</p>
<p>The <img src="https://latex.codecogs.com/png.latex?i">-th inequality constraint is <em>active</em> at <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bx%7D"> if <img src="https://latex.codecogs.com/png.latex?h_i(%5Ctilde%7Bx%7D)=0">. Otherwise, it is <em>inactive</em>. An index set of active inequality constraints is denoted as <img src="https://latex.codecogs.com/png.latex?I_a(%5Ctilde%7Bx%7D)%20=%20%5C%7Bi%5Cin%5C%7B1,%5Ccdots,p%5C%7D%5Cmid%20h_i(%5Ctilde%7Bx%7D)=0%5C%7D">. A vector <img src="https://latex.codecogs.com/png.latex?h_a(%5Ctilde%7Bx%7D)%20=%20%5C%7Bh_i(%5Ctilde%7Bx%7D)%5Cmid%20i%5Cin%20I_a(%5Ctilde%7Bx%7D)%5C%7D"> is the subvector of active inequality constraints.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Sometimes the equality constraints are also referred as <em>active</em> constraints.</p>
</div>
</div>
<p>A point <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bx%7D"> is a <em>regular point</em> if the <strong>gradients</strong> of active inequality constraints and equality cosntraints are linearly independent. E.g., <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20g_i(x),i=1,%5Ccdots,m"> and <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20h_j(x),j%5Cin%20I_a(x)"> are linearly independent. This is also referred as <em>linear independence constraint qualification (LICQ)</em>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>When the constraints are linear, e.g.&nbsp;<img src="https://latex.codecogs.com/png.latex?A%5Ctilde%7Bx%7D%20=%20b">, then <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bx%7D"> is a regular point if <img src="https://latex.codecogs.com/png.latex?A"> is full <strong>row</strong> rank.</p>
</div>
</div>
<p>The optimality condition is relatively simple for regular points (or the LICQ condition holds). Consider the Lagrange function <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D(x,%5Clambda,%5Cnu)%20=%20f(x)%20+%20%5Clambda%5ETg(x)%20+%20%5Cnu%5ETh(x)%0A"></p>
<p><em>First-order necessary condition</em></p>
<p>Suppose <img src="https://latex.codecogs.com/png.latex?x%5E%5Cstar"> is a constrained local minimizer and <img src="https://latex.codecogs.com/png.latex?x%5E%5Cstar"> is a <strong>regular points</strong> for the constraints (aka, the LICQ condition holds). Then there exists unique mutiplier <img src="https://latex.codecogs.com/png.latex?%5Clambda%5E%5Cstar"> and <img src="https://latex.codecogs.com/png.latex?%5Cnu%5E%5Cstar"> such that <span id="eq-KKT"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cnabla_x%20L%5Cleft(x%5E%7B%5Cstar%7D,%20%5Clambda%5E%7B%5Cstar%7D,%20%5Cnu%5E%7B%5Cstar%7D%5Cright)=0%20%5C%5C%0A&amp;%20g%5Cleft(x%5E%7B%5Cstar%7D%5Cright)=0%20%5C%5C%0A&amp;%20h%5Cleft(x%5E%7B%5Cstar%7D%5Cright)%20%5Cleq%200%20%5C%5C%0A&amp;%20%5Cnu%5E%7B%5Cstar%7D%20%5Cgeq%200%20%5C%5C%0A&amp;%20%5Cleft(%5Cnu%5E%7B%5Cstar%7D%5Cright)%5ET%20h%5Cleft(x%5E%7B%5Cstar%7D%5Cright)=0%0A%5Cend%7Baligned%7D%0A%5Ctag%7B1%7D"></span> where Equation&nbsp;1 is the Karush-Kuhn-Tucker (KKT) conditions.</p>
<p>The first condition is the stationary condition. It gives that the gradient of the objective can be written as weighted sum of the gradients of the constraints.</p>
<p>The last condition is the <em>complementarity slackness</em>. It implies that if the <img src="https://latex.codecogs.com/png.latex?i">-th inequality constraint is inactive, then the corresponding multiplier <img src="https://latex.codecogs.com/png.latex?%5Cnu%5E%5Cstar_i"> is zero.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Other constraint qualification (QC) or regularity condition is also available, e.g., the <em>Mangasarian-Fromovitz constraint qualification (MFCQ)</em>. For convex optimization, the <em>Slater’s condition</em> is also a popular choice.</p>
</div>
</div>
<p>In Boyd’s book, the necessary condition is stated as,</p>
<blockquote class="blockquote">
<p>For any optimization problem with diﬀerentiable objective and constraint functions for which <strong>strong duality obtains</strong>, any pair of primal and dual optimal points must satisfy the KKT conditions. In this sense, the condition of strong duality performs the role of QC.</p>
</blockquote>
<p><em>Strict Compelmentarity Condition</em></p>
<p>Let <img src="https://latex.codecogs.com/png.latex?x%5E%5Cstar"> be a local solution and <img src="https://latex.codecogs.com/png.latex?%5Cnu%5E%5Cstar"> be the multiplier of the inequality constraint. Then the <em>strict complementarity condition</em> holds if <img src="https://latex.codecogs.com/png.latex?%5Cnu%5E%5Cstar_i%3E0"> for all <img src="https://latex.codecogs.com/png.latex?i%5Cin%5Cmathcal%7BI%7D_a(%5Ctilde%7Bx%7D)">. This condition implies at most one of the <img src="https://latex.codecogs.com/png.latex?%5Cnu%5E%5Cstar_i"> and <img src="https://latex.codecogs.com/png.latex?h_i(x%5E%5Cstar)"> is zero.</p>
<p><em>Second-order sufficient condition</em></p>
<p>1). Assume that there exist <img src="https://latex.codecogs.com/png.latex?x%5E%5Cstar,%20%5Clambda%5E%5Cstar,%20%5Cnu%5E%5Cstar"> such that the KKT conditions hold. 2). Suppose moreover that <img src="https://latex.codecogs.com/png.latex?%5Cnu%5E%5Cstar"> is such that the strict complementary condition holds at <img src="https://latex.codecogs.com/png.latex?x%5E%5Cstar">. 3). Assume final that <img src="https://latex.codecogs.com/png.latex?%0As%5ET%5Cnabla%5E2_%7Bxx%7DL(x%5E%5Cstar,%5Clambda%5E%5Cstar,%5Cnu%5E%5Cstar)s%20%3E%200%0A"> such that for all <img src="https://latex.codecogs.com/png.latex?s%5Cneq%200">, <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A%5Cnabla_x%20g(x%5E%5Cstar)%20%5C%5C%0A%5Cnabla_x%20h_a(x%5E%5Cstar)%0A%5Cend%7Bbmatrix%7D%20s%20=%200%0A"> holds. Then <img src="https://latex.codecogs.com/png.latex?x%5E%5Cstar"> is a strict constrained local minimizer.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The necessary and sufficient conditions become global if the optimization is convex.</p>
</div>
</div>
</section>
<section id="conditions-for-convex-optimization" class="level2">
<h2 class="anchored" data-anchor-id="conditions-for-convex-optimization">Conditions for Convex Optimization</h2>
<p>As mentioned above, the local (strict) contrained minimizer is also a global (strict) constrained minimizer if the optimization problem is convex.</p>
<p>Convex optimization must be in the form of <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmin_x%20&amp;%20%5Cquad%20f(x)%20%5C%5C%0A%5Ctext%7Bs.t.%7D%20&amp;%20%5Cquad%20Ax%20=b%20%5C%5C%0A&amp;%20%5Cquad%20h_i(x)%20%5Cleq%200,%20%5Cquad%20i=1,%5Ccdots,p%0A%5Cend%7Baligned%7D%0A"></p>
<p>In Boyd’s book, the constraint qualification is defined as</p>
<blockquote class="blockquote">
<p>There are many results that establish conditions on the problem, beyond convexity, under which <strong>strong duality</strong> holds. These conditions are called constraint qualiﬁcations.</p>
</blockquote>
<p>A commonly used QC for convex optimization is the <em>Slater’s condition</em>: There exists <img src="https://latex.codecogs.com/png.latex?x%5Cin%20%5Ctext%7Brelient%7D%5Cmathcal%7BX%7D"> such that <img src="https://latex.codecogs.com/png.latex?h_i(x)%3C0"> for all <img src="https://latex.codecogs.com/png.latex?i=1,%5Ccdots,p"> and <img src="https://latex.codecogs.com/png.latex?Ax%20=%20b">. Slator’s condition statet that for convex optimization, the strong duality holds if the Slater’s condition holds.</p>
<p>Then back to the necessary condition, for convex problem, if the slator condition holds, the strong duality condition holds, the KKT condition Equation&nbsp;1 must be satisfied.</p>
<p>E.g.Convex + Slator condition -&gt; Strong Duality -&gt; KKT condition.</p>
<p>For the sufficiency, for convex optimization problem, the KKT condition is sufficient for (primal and dual) optimality. I.e., any pair of <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bx%7D,%20%5Ctilde%7B%5Clambda%7D,%20%5Ctilde%7B%5Cnu%7D"> is primal and dual optimal.</p>
<p>E.g. Convex + KKY -&gt; Optimality.</p>
<p>With the Slator’s condition, the KKT condition is both necessary and sufficient for (primal and dual) optimality (zero-duality gap) for convex problem.</p>


</section>

 ]]></description>
  <category>Optimization</category>
  <guid>https://xuwkk.github.io/blog/posts/learning/optimization/baisc.html</guid>
  <pubDate>Thu, 27 Jun 2024 23:00:00 GMT</pubDate>
</item>
<item>
  <title>Tangent Sensitivity Analysis of ODEs</title>
  <dc:creator>Wangkun Xu</dc:creator>
  <link>https://xuwkk.github.io/blog/posts/learning/autometic_differentiation/forward_mode_ode.html</link>
  <description><![CDATA[ 





<p>This post summarizes my learning note on the forward-mode (or tangent method) for sensitivity analysis of ordinary differential equations (ODEs). Similar to the previous posts on tangent method for <a href="posts/learning/autometic_differentiation/adjoint_linear_equation.qmd">linear</a> and <a href="posts/learning/autometic_differentiation/adjoint_nonlinear_equation.qmd">nonlinear</a> equations, it can be extended for finding the sensitivity of the solution of ODEs or the gradient through the NeuralODE with repsect to the initial condition or the parameters. This post is based on the <a href="https://www.youtube.com/watch?v=69KlO-kbxJ8">YouTube video: Neural ODEs - Pushforward/Jvp rule</a>.</p>
<p>Consider the nonlinear ODE: <span id="eq-ode"><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bdu%7D%7Bdt%7D%20=%20f(u,%20%5Ctheta)%0A%5Ctag%7B1%7D"></span> which is evaluated at <img src="https://latex.codecogs.com/png.latex?t=T"> with initial condition <img src="https://latex.codecogs.com/png.latex?u(t=0)=u_0"> as <img src="https://latex.codecogs.com/png.latex?%0Aq(%5Ctheta,u_0,T)%20=%20u(T)%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5Cin%5Cmathbb%7BR%7D%5EP"> is the parameter; <img src="https://latex.codecogs.com/png.latex?u_0%5Cin%5Cmathbb%7BR%7D%5EN"> is the initial condition; <img src="https://latex.codecogs.com/png.latex?T"> is the final time; and <img src="https://latex.codecogs.com/png.latex?u(T)%5Cin%5Cmathbb%7BR%7D%5EN"> is the final state. The ODE can be solved by <strong>any</strong> ODE solver, such as the Euler method or Runge-Kutta method.</p>
<p>Our task is to forward-propagate the tangent information (a vector) on the inpus <img src="https://latex.codecogs.com/png.latex?%5Cdot%7B%5Ctheta%7D%5Cin%5Cmathbb%7BR%7D%5EP,%20%5Cdot%7Bu%7D_0%5Cin%5Cmathbb%7BR%7D%5E%7BN%7D,%20T%5Cin%5Cmathbb%7BR%7D"> to the output <img src="https://latex.codecogs.com/png.latex?%5Cdot%7Bu%7D(T)%5Cin%5Cmathbb%7BR%7D%5EN"> without unrolling the solver and applying forward-mode AD to its operation.</p>
<p>In general, the forward-mode AD can be found the Jacobian-vector product on the total derivative of the output: <img src="https://latex.codecogs.com/png.latex?%0A%5Cdot%7Bu%7D(T)%20=%20%5Cunderbrace%7B%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20%5Ctheta%7D%5Cdot%7B%5Ctheta%7D%7D_%7B%5Cdot%7Bu%7D(T)_%5Ctheta%7D%20+%20%5Cunderbrace%7B%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20u_0%7D%5Cdot%7Bu%7D_0%7D_%7B%5Cdot%7Bu%7D(T)_%7Bu_0%7D%7D%20+%20%5Cunderbrace%7B%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20T%7D%5Cdot%7BT%7D%7D_%7B%5Cdot%7Bu%7D(T)_T%7D%0A"></p>
<p>We can find the tangent information item by item.</p>
<section id="tangent-condition-from-parameter-theta" class="level2">
<h2 class="anchored" data-anchor-id="tangent-condition-from-parameter-theta">Tangent Condition from parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"></h2>
<p>From <img src="https://latex.codecogs.com/png.latex?%0Au(T)%20=%20u_0%20+%20%5Cint_0%5ET%20f(u(t),%20%5Ctheta)%20dt%0A"> take the total derivative with respect to <img src="https://latex.codecogs.com/png.latex?%5Ctheta">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%7D%7Bd%5Ctheta%7Du(T)%20=%20%5Cfrac%7Bd%7D%7Bd%5Ctheta%7Du_0%20+%20%5Cint_0%5ET%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20u%7D%5Cfrac%7Bd%7D%7Bd%5Ctheta%7Du(t)%20+%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20%5Ctheta%7D%20dt%0A"></p>
<p>Multiply both sides by <img src="https://latex.codecogs.com/png.latex?%5Cdot%7B%5Ctheta%7D">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cunderbrace%7B%5Cfrac%7Bd%7D%7Bd%5Ctheta%7Du(T)%20%5Ccdot%20%5Cdot%7B%5Ctheta%7D%7D_%7B%5Cdot%7Bu%7D(T)_%5Ctheta%7D%20=%20%5Cunderbrace%7B%5Cfrac%7Bd%7D%7Bd%5Ctheta%7Du_0%20%5Ccdot%20%5Cdot%7B%5Ctheta%7D%7D_%7B=0%7D%20+%20%5Cint_0%5ET%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20u%7D%5Cunderbrace%7B%5Cfrac%7Bd%7D%7Bd%5Ctheta%7Du(t)%20%5Ccdot%20%5Cdot%7B%5Ctheta%7D%7D_%7B%5Cdot%7Bu%7D(t)_%5Ctheta%7D%20+%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20%5Ctheta%7D%20%5Ccdot%20%5Cdot%7B%5Ctheta%7D%20dt%0A"></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>It is assumed that the initial condition <img src="https://latex.codecogs.com/png.latex?u_0"> is not dependent on <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. However, this may not be true for example when the event is considered.</p>
</div>
</div>
<p>The above relationship is the solution of the following ODE on state <img src="https://latex.codecogs.com/png.latex?%5Cdot%7Bu%7D(t)_%5Ctheta">: <span id="eq-solution"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cfrac%7Bd%7D%7Bdt%7D%5Cdot%7Bu%7D(t)_%5Ctheta%20&amp;=%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20u%7D%5Cdot%7Bu%7D(t)_%5Ctheta%20+%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20%5Ctheta%7D%5Cdot%7B%5Ctheta%7D%20%5C%5C%0A%5Cdot%7Bu%7D(t=0)_%5Ctheta%20&amp;=%200%0A%5Cend%7Baligned%7D%0A%5Ctag%7B2%7D"></span></p>
<p>By solving the tangent ODE using <strong>any</strong> ODE solver, we can find the tangent information <img src="https://latex.codecogs.com/png.latex?%5Cdot%7Bu%7D(t)_%5Ctheta">. Note that this ODE is linear and inhomogeneous, although the original ODE can be nonlinear.</p>
<section id="alternative-derivation" class="level3">
<h3 class="anchored" data-anchor-id="alternative-derivation">Alternative Derivation</h3>
<p>In this section, we derive the tangent condition from the perspective of automatic differentiation in deep learning. The sensitivity derived can be used for optimization, local sensitivity analysis, dynamic/control, or neural ODE.</p>
<p>Consider there is a cost function <img src="https://latex.codecogs.com/png.latex?J(u,%5Ctheta)"> associated with the state <img src="https://latex.codecogs.com/png.latex?u(t)">: <img src="https://latex.codecogs.com/png.latex?%0AJ(u,%5Ctheta)%20=%20%5Cint_0%5ETg(u(t),%20%5Ctheta)dt%0A"></p>
<p>For instance, we can set a quadratic loss as <img src="https://latex.codecogs.com/png.latex?g(u(t),%20%5Ctheta)%20=%20u(t)%5ETQu(t)">. Cost on finite time instance is also possible.</p>
<p>The total derivative of the cost function with respect to <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> is: <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7BdJ%7D%7Bd%5Ctheta%7D%20=%20%5Cint_0%5ET%20%5Cfrac%7Bd%7D%7Bd%5Ctheta%7D%20g(u,%5Ctheta)dt%20=%20%5Cint_0%5ET%20%5Cfrac%7B%5Cpartial%20g%7D%7B%5Cpartial%20%5Ctheta%7D%20+%20%5Cfrac%7B%5Cpartial%20g%7D%7B%5Cpartial%20u%7D%5Cfrac%7Bd%20u%7D%7Bd%5Ctheta%7D%20dt%20%5Cin%20%5Cmathbb%7BR%7D%5E%7B1%5Ctimes%20P%7D%0A"> which is a row vector.</p>
<p>The only term that is difficult to solve is <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%20u%7D%7Bd%5Ctheta%7D%5Cin%5Cmathbb%7BR%7D%5E%7BN%5Ctimes%20P%7D">, e.g., <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bdu%7D%7Bd%5Ctheta%7D%20=%20%5B%5Cfrac%7Bdu%7D%7Bd%5Ctheta_1%7D,%20%5Ccdots.%20%5Cfrac%7Bdu%7D%7Bd%5Ctheta_P%7D%5D"> and each <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bdu%7D%7Bd%5Ctheta_i%7D%5Cin%5Cmathbb%7BR%7D%5EN">.</p>
<p>Then do total derivative on Equation&nbsp;1: <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%7D%7Bd%5Ctheta%7D%20%5Cfrac%7Bdu%7D%7Bdt%7D%20=%20%5Cfrac%7Bd%7D%7Bdt%7D%20%5Cfrac%7Bdu%7D%7Bd%5Ctheta%7D%20=%20%5Cfrac%7Bd%7D%7Bd%5Ctheta%7D%20f(u,%20%5Ctheta)%20=%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20u%7D%5Cfrac%7Bdu%7D%7Bd%5Ctheta%7D%20+%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20%5Ctheta%7D%0A"></p>
<p>Therefore, there is <img src="https://latex.codecogs.com/png.latex?P"> linear ODEs to solve for <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bdu%7D%7Bd%5Ctheta%7D">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cfrac%7Bd%7D%7Bdt%7D%5Cfrac%7Bdu%7D%7Bd%5Ctheta_i%7D%20&amp;=%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20u%7D%5Cfrac%7Bdu%7D%7Bd%5Ctheta_i%7D%20+%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20%5Ctheta_i%7D%20%5C%5C%0A%5Cfrac%7Bdu%7D%7Bd%5Ctheta_i%7D(t=0)%20&amp;=%200%0A%5Cend%7Baligned%7D%0A"></p>
<p>Once we have <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bdu%7D%7Bd%5Ctheta%7D">, we can find the gradient of the cost function with respect to <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> by plugging it into the total derivative of the cost function.</p>
</section>
</section>
<section id="tangent-condition-from-initial-condition-u_0" class="level2">
<h2 class="anchored" data-anchor-id="tangent-condition-from-initial-condition-u_0">Tangent Condition from initial condition <img src="https://latex.codecogs.com/png.latex?u_0"></h2>
<p>Find the total derivative of <img src="https://latex.codecogs.com/png.latex?u(T)"> with respect to <img src="https://latex.codecogs.com/png.latex?u_0"> on both sides of Equation&nbsp;2: <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%7D%7Bdu_0%7Du(T)%20=%20%5Cfrac%7B%5Cpartial%20u_0%7D%7B%5Cpartial%20u_0%7D%20+%20%5Cint_0%5ET%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20u%7D%5Cfrac%7Bd%7D%7Bdu_0%7Du(t)%20dt%0A"></p>
<p>Multiply both sides by <img src="https://latex.codecogs.com/png.latex?%5Cdot%7Bu%7D_0">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cunderbrace%7B%5Cfrac%7Bd%7D%7Bdu_0%7Du(T)%20%5Ccdot%20%5Cdot%7Bu%7D_0%7D_%7B%5Cdot%7Bu%7D(T)_%7Bu_0%7D%7D%20=%20%5Cunderbrace%7B%5Cfrac%7B%5Cpartial%20u_0%7D%7B%5Cpartial%20u_0%7D%20%5Ccdot%20%5Cdot%7Bu%7D_0%7D_%7B=%5Cdot%7Bu%7D_0%7D%20+%20%5Cint_0%5ET%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20u%7D%5Cunderbrace%7B%5Cfrac%7Bd%7D%7Bdu_0%7Du(t)%20%5Ccdot%20%5Cdot%7Bu%7D_0%7D_%7B%5Cdot%7Bu%7D(t)_%7Bu_0%7D%7D%20dt%0A"> which is again as the solution of the following ODE on state <img src="https://latex.codecogs.com/png.latex?%5Cdot%7Bu%7D(t)_%7Bu_0%7D">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cfrac%7Bd%7D%7Bdt%7D%5Cdot%7Bu%7D(t)_%7Bu_0%7D%20&amp;=%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20u%7D%5Cdot%7Bu%7D(t)_%7Bu_0%7D%20%5C%5C%0A%5Cdot%7Bu%7D(t=0)_%7Bu_0%7D%20&amp;=%20%5Cdot%7Bu%7D_0%0A%5Cend%7Baligned%7D%0A"> which is a linear and homogeneous ODE.</p>
</section>
<section id="tangent-condition-from-final-time-t" class="level2">
<h2 class="anchored" data-anchor-id="tangent-condition-from-final-time-t">Tangent Condition from final time <img src="https://latex.codecogs.com/png.latex?T"></h2>
<p>Find the total derivative of <img src="https://latex.codecogs.com/png.latex?u(T)"> with respect to <img src="https://latex.codecogs.com/png.latex?T"> on both sides of Equation&nbsp;2: <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%7D%7BdT%7Du(T)%20=%20%5Cfrac%7Bd%7D%7BdT%7Du_0%20+%20f(u(T),%20%5Ctheta)%0A"> Note that we have <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%7D%7BdT%7D%5Cint_0%5ET%20f(u(t),%20%5Ctheta)%20dt%20=%20f(u(T),%20%5Ctheta)">.</p>
<p>Multiply both sides by <img src="https://latex.codecogs.com/png.latex?%5Cdot%7BT%7D">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cunderbrace%7B%5Cfrac%7Bd%7D%7BdT%7Du(T)%20%5Ccdot%20%5Cdot%7BT%7D%7D_%7B%5Cdot%7Bu%7D(T)_T%7D%20=%20%5Cunderbrace%7B%5Cfrac%7Bd%7D%7BdT%7Du_0%20%5Ccdot%20%5Cdot%7BT%7D%7D_%7B=0%7D%20+%20f(u(T),%20%5Ctheta)%20%5Ccdot%20%5Cdot%7BT%7D%0A"> which is an algebraic equation (only for the final time).</p>
</section>
<section id="computation" class="level2">
<h2 class="anchored" data-anchor-id="computation">Computation</h2>
<p>Note that <img src="https://latex.codecogs.com/png.latex?%5Cdot%7B%5Ctheta%7D,%20%5Cdot%7Bu%7D_0,%20%5Cdot%7BT%7D"> are known vectors. Meanwhile, the Jacobian <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20u%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20%5Ctheta%7D"> can be found by analytical derivation or automatic differentiation. In most of the cases, the Jacobians do not need to be computed or stored explicitly. Instead, the Jacobian-vector product (JVP) can be computed by the forward-mode AD, e.g.&nbsp;by <code>torch.autograd.forward_ad</code> in PyTorch.</p>
<p>Because <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20u%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20%5Ctheta%7D"> are functions of <img src="https://latex.codecogs.com/png.latex?u(t)">, its value is dependent on the solution of the primal ODE. One method is to store the solution of the primal ODE. However, a more compact method is to simutaneously solve the primal ODE and the tangent ODEs. For example, the augmented ODE system is: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cfrac%7Bdu%7D%7Bdt%7D%20&amp;=%20f(u,%20%5Ctheta)%20%5C%5C%0A%5Cfrac%7Bd%7D%7Bdt%7D%5Cdot%7Bu%7D(t)_%7Bu_0%7D%20&amp;=%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20u%7D%5Cdot%7Bu%7D(t)_%7Bu_0%7D%20%5C%5C%0A%5Cfrac%7Bd%7D%7Bdt%7D%5Cdot%7Bu%7D(t)_%5Ctheta%20&amp;=%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20u%7D%5Cdot%7Bu%7D(t)_%5Ctheta%20+%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20%5Ctheta%7D%5Cdot%7B%5Ctheta%7D%0A%5Cend%7Baligned%7D%0A"> where the initial conditions are <img src="https://latex.codecogs.com/png.latex?u(t=0)=u_0">, <img src="https://latex.codecogs.com/png.latex?%5Cdot%7Bu%7D(t=0)_%7Bu_0%7D=%5Cdot%7Bu%7D_0"> and <img src="https://latex.codecogs.com/png.latex?%5Cdot%7Bu%7D(t=0)_%5Ctheta=0">. The ODEs can be solved by any ODE solver, such as the Euler method or Runge-Kutta method.</p>
<p>The tangent method can also be used to find the Jacobian of the output with respect to the parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> by setting the tangent vector <img src="https://latex.codecogs.com/png.latex?%5Cdot%7B%5Ctheta%7D"> to be the unit vectors. Not suprisingly, the <img src="https://latex.codecogs.com/png.latex?P"> number of ODEs need to be solved to find the Jacobian <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%20u(t)%7D%7Bd%20%5Ctheta%7D">.</p>
</section>
<section id="pytorch-implementation" class="level2">
<h2 class="anchored" data-anchor-id="pytorch-implementation">Pytorch Implementation</h2>
<p>An example code snippet can be found at <a href="https://github.com/xuwkk/blog_code">my-github</a> including a parallel batched implementation.</p>


</section>

 ]]></description>
  <category>Auto Differentiation</category>
  <category>Implicit Function</category>
  <category>NeuralODE</category>
  <guid>https://xuwkk.github.io/blog/posts/learning/autometic_differentiation/forward_mode_ode.html</guid>
  <pubDate>Mon, 17 Jun 2024 23:00:00 GMT</pubDate>
</item>
<item>
  <title>Economic Dispatch</title>
  <dc:creator>Wangkun Xu</dc:creator>
  <link>https://xuwkk.github.io/blog/posts/learning/power_system/ed.html</link>
  <description><![CDATA[ 





<p>The economic dispatch problem is a fundamental optimization problem in power system operation. It aims to minimize the total generation cost while meeting the power demand and satisfying the operational constraints. This post provides an overview of the economic dispatch problem, its formulation, and solution methods. The main reference of this post is <a href="https://u.osu.edu/conejo.1/courses/power-system-operations/">Conejo’s lecture notes</a>. Apart from the basic setting, the on-off and set-point conditions of the unit-commitment problem is also passes through to the ED formulation.</p>
<p>This post is also a supplement to the github repository <a href="https://github.com/xuwkk/power_system_operation">PowerSystemOperation</a></p>



 ]]></description>
  <category>Power System</category>
  <guid>https://xuwkk.github.io/blog/posts/learning/power_system/ed.html</guid>
  <pubDate>Thu, 13 Jun 2024 23:00:00 GMT</pubDate>
</item>
<item>
  <title>Unit Commitment Problem</title>
  <dc:creator>Wangkun Xu</dc:creator>
  <link>https://xuwkk.github.io/blog/posts/learning/power_system/ncuc.html</link>
  <description><![CDATA[ 





<p>In the previous post <a href="posts/learning/power_system/power_system_operation.qmd">Power System Operation: AC and DC Power Flow Model</a>, we discuss how to model the power system in the steady-state for both AC and DC formats. In this post, unit commitment (UC) problem and econimic dispatch (ED) will be formulated. The UC problem is usually solved in day-ahead manner to decide the on/off conditions and the set-points of the generator. It includes 1). the simplest UC problem without binary variables, 2). the UC problem with binary variables. Both problem considers the network constraints and reserve requirements. The ED problem focuses more on ‘real-time’ balance of the load and generation and meet the physical constraints and safety requirements.</p>
<p>Note that the UC and ED can be formulated in different ways, including stochastic, robust, and chance-constrained formats. In this post, we will focus on the deterministic formulation.</p>
<p>The main reference is <a href="https://u.osu.edu/conejo.1/courses/power-system-operations/">Conejo’s lecture notes</a>.</p>
<section id="unit-commitment-without-binary-variables" class="level2">
<h2 class="anchored" data-anchor-id="unit-commitment-without-binary-variables">Unit Commitment Without Binary Variables</h2>
<p>The UC without binary varibales can be formulated as linear programming (LP) problem as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmin_%7BP_g,%20%5Ctheta,%20P_%7Bls%7D,%20P_%7Bsc%7D,%20P_%7Bwc%7D%7D%20&amp;%20%5Csum_%7Bt=1%7D%5ET%20c_%7Bgv%7D%5ETP_g(t)%20+%20c_%7Bls%7D%5ETP_%7Bls%7D(t)%20+%20c_%7Bsc%7D%5ETP_%7Bsc%7D(t)%20+%20c_%7Bwc%7D%5ETP_%7Bwc%7D(t)%20%5C%5C%0A%5Ctext%7Bs.t.%7D%20&amp;%20P_g%5E%7B%5Ctext%7Bmin%7D%7D%20%5Cleq%20P_g(t)%20%5Cleq%20P_g%5E%7B%5Ctext%7Bmax%7D%7D%20%5C%5C%0A&amp;%20-R_%7B%5Ctext%7Bdown%7D%7D%20%5Cleq%20P_g(t)%20-%20P_g(t-1)%20%5Cleq%20R_%7B%5Ctext%7Bup%7D%7D%20%5C%5C%0A&amp;%20-P_f%5E%7B%5Ctext%7Bmax%7D%7D%20%5Cleq%20B_f%5Ctheta(t)%20+%20P_%7Bf,%5Ctext%7Bshift%7D%7D%20%5Cleq%20P_f%5E%7B%5Ctext%7Bmax%7D%7D%20%5C%5C%0A&amp;%20C_gP_g(t)%20+%20C_s(P_s(t)%20-%20P_%7Bsc%7D(t))%20+%20C_w(P_w(t)%20-%20P_%7Bwc%7D(t))%20-%20C_l(P_l(t)%20-%20P_%7Bls%7D(t))%20%5C%5C%0A&amp;%20%20%5Cqquad%20=%20B_%7B%5Ctext%7Bbus%7D%7D%5Ctheta(t)%20+%20P_%7B%5Ctext%7Bbus,shift%7D%7D%20%5C%5C%0A&amp;%20%5Ctheta_%7B%5Ctext%7Bref%7D%7D(t)%20=%20%5Ctheta_0,%20%5Cquad%20%5Cforall%20t=1,2,%5Cldots,T%20%5C%5C%0A&amp;%20%5Csum%20P_g%5E%7B%5Ctext%7Bmax%7D%7D%20%5Cgeq%20%5Csum_%7Bi=1%7D%5ETP_g(t)%20+%20r(t)%20%5C%5C%0A&amp;%200%20%5Cleq%20P_%7Bls%7D(t)%20%5Cleq%20P_l(t),%20%5Cquad%20%5Cforall%20t=1,2,%5Cldots,T%20%5C%5C%0A&amp;%200%20%5Cleq%20P_%7Bsc%7D(t)%20%5Cleq%20P_s(t),%20%5Cquad%20%5Cforall%20t=1,2,%5Cldots,T%20%5C%5C%0A&amp;%200%20%5Cleq%20P_%7Bwc%7D(t)%20%5Cleq%20P_w(t),%20%5Cquad%20%5Cforall%20t=1,2,%5Cldots,T%20%5C%5C%0A%5Cend%7Baligned%7D%0A"></p>
<p>The decision variables include</p>
<ul>
<li>Generator output <img src="https://latex.codecogs.com/png.latex?P_g(t)"></li>
<li>Load shedding <img src="https://latex.codecogs.com/png.latex?P_%7Bls%7D(t)"></li>
<li>Solar energy curtailment <img src="https://latex.codecogs.com/png.latex?P_%7Bsc%7D(t)"></li>
<li>Wind energy curtailment <img src="https://latex.codecogs.com/png.latex?P_%7Bwc%7D(t)"></li>
<li>Bus voltage angle <img src="https://latex.codecogs.com/png.latex?%5Ctheta(t)"></li>
</ul>
<p>All the decision variables are vectors and from <img src="https://latex.codecogs.com/png.latex?t=1"> to <img src="https://latex.codecogs.com/png.latex?T">. The objective function is to minimize the total cost, which includes the generator cost, load shedding cost, solar curtailment cost, and wind curtailment cost. The constraints include the generator output limits, ramping limits, line flow limits, power balance, bus voltage angle reference, and reserve requirements. Meanwhile, the parameters include</p>
<ul>
<li>Load <img src="https://latex.codecogs.com/png.latex?P_l(t)"></li>
<li>Solar generation <img src="https://latex.codecogs.com/png.latex?P_s(t)"></li>
<li>Wind generation <img src="https://latex.codecogs.com/png.latex?P_w(t)"></li>
<li>Reserve requirement <img src="https://latex.codecogs.com/png.latex?r(t)"> which are assumed to be known.</li>
</ul>
<p>The initial condition, e.g., <img src="https://latex.codecogs.com/png.latex?P_g(0)"> should also be given.</p>
<p>The definition of the bus and branch admittance matrix <img src="https://latex.codecogs.com/png.latex?B_%7B%5Ctext%7Bbus%7D%7D"> and <img src="https://latex.codecogs.com/png.latex?B_f"> are the same as the previous post <a href="posts/learning/power_system/power_system_operation.qmd">Power System Operation: AC and DC Power Flow Model</a>.</p>
</section>
<section id="unit-commitment-with-binary-variables" class="level2">
<h2 class="anchored" data-anchor-id="unit-commitment-with-binary-variables">Unit Commitment With Binary Variables</h2>
<p>The UC with binary variables can be formulated as mixed-integer linear programming (MILP) problem as follows. Apart from the decision variables in the previous section, the binary variables <img src="https://latex.codecogs.com/png.latex?u(t)"> are introduced to decide the on/off status of the generator.</p>
<p>The optimization problem can be formulated as <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmin_%7Bu_g,%20y_g,%20z_g,%20P_g,%20%5Ctheta,%20P_%7Bls%7D,%20P_%7Bsc%7D,%20P_%7Bwc%7D%7D%20&amp;%20%5Cquad%20%5Csum_%7Bt=1%7D%5ET%20c_%7Bgf%7D%5ETu_g(t)%20+%20c_%7Bgv%7D%5ETP_g(t)%20+%20c_%7Bgsu%7D%5ETy_g(t)%20+%20c_%7Bgsd%7D%5ETz_g(t)%20+%20c_%7Bls%7D%5ETP_%7Bls%7D(t)%20+%20c_%7Bsc%7D%5ETP_%7Bsc%7D(t)%20+%20c_%7Bwc%7D%5ETP_%7Bwc%7D(t)%20%5C%5C%0A%5Ctext%7Bs.t.%7D%20&amp;%20%5Cquad%20y_g(t)%20-%20z_g(t)%20=%20u_g(t)%20-%20u_g(t-1)%20%5C%5C%0A&amp;%20%5Cquad%20y_g(t)%20+%20z_g(t)%20%5Cleq%201%20%5C%5C%0A&amp;%20%5Cquad%20u_g(t)%5Ccirc%20P_g%5E%7B%5Ctext%7Bmin%7D%7D%20%5Cleq%20P_g(t)%20%5Cleq%20u_g(t)%5Ccirc%20P_g%5E%7B%5Ctext%7Bmax%7D%7D%20%5C%5C%0A&amp;%20%5Cquad%20P_g(t)%20-%20P_g(t-1)%20%5Cleq%20R_%7Bgu%7Du_g(t-1)%20+%20R_%7Bgsu%7D%20y_g(t)%5C%5C%0A&amp;%20%5Cquad%20P_g(t-1)%20-%20P_g(t)%20%5Cleq%20R_%7Bdu%7Du_g(t)%20+%20R_%7Bgsd%7D%20z_g(t)%5C%5C%0A&amp;%20%5Cquad%20-P_f%5E%7B%5Ctext%7Bmax%7D%7D%20%5Cleq%20B_f%5Ctheta(t)%20+%20P_%7Bf,%5Ctext%7Bshift%7D%7D%20%5Cleq%20P_f%5E%7B%5Ctext%7Bmax%7D%7D%20%5C%5C%0A&amp;%20%5Cquad%20C_gP_g(t)%20+%20C_s(P_s(t)%20-%20P_%7Bsc%7D(t))%20+%20C_w(P_w(t)%20-%20P_%7Bwc%7D(t))%20-%20C_l(P_l(t)%20-%20P_%7Bls%7D(t))%20%5C%5C%0A&amp;%20%5Cquad%20%20%5Cqquad%20=%20B_%7B%5Ctext%7Bbus%7D%7D%5Ctheta(t)%20+%20P_%7B%5Ctext%7Bbus,shift%7D%7D%20%5C%5C%0A&amp;%20%5Cquad%20%5Ctheta_%7B%5Ctext%7Bref%7D%7D(t)%20=%20%5Ctheta_0,%20%5Cquad%20%5Cforall%20t=1,2,%5Cldots,T%20%5C%5C%0A&amp;%20%5Cquad%20%5Csum%20u_g(t)%20%5Ccirc%20P_g%5E%7B%5Ctext%7Bmax%7D%7D%20%5Cgeq%20%5Csum_%7Bi=1%7D%5ETP_g(t)%20+%20r(t)%20%5C%5C%0A&amp;%20%5Cquad%200%20%5Cleq%20P_%7Bls%7D(t)%20%5Cleq%20P_l(t),%20%5Cquad%20%5Cforall%20t=1,2,%5Cldots,T%20%5C%5C%0A&amp;%20%5Cquad%200%20%5Cleq%20P_%7Bsc%7D(t)%20%5Cleq%20P_s(t),%20%5Cquad%20%5Cforall%20t=1,2,%5Cldots,T%20%5C%5C%0A&amp;%20%5Cquad%200%20%5Cleq%20P_%7Bwc%7D(t)%20%5Cleq%20P_w(t),%20%5Cquad%20%5Cforall%20t=1,2,%5Cldots,T%20%5C%5C%0A&amp;%20%5Cquad%20u_g(t)%20%5Cin%20%5C%7B0,1%5C%7D,%20%5Cquad%20y_g(t)%20%5Cin%20%5C%7B0,1%5C%7D,%20%5Cquad%20z_g(t)%20%5Cin%20%5C%7B0,1%5C%7D%0A%5Cend%7Baligned%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Ccirc"> denotes the element-wise product.</p>
<p>Apart from the decision variables in the previous section, the following binary variables are introduced:</p>
<ul>
<li>Generator on/off status <img src="https://latex.codecogs.com/png.latex?u(t)"> with cost <img src="https://latex.codecogs.com/png.latex?c_%7Bgf%7D"></li>
<li>Generator start-up status <img src="https://latex.codecogs.com/png.latex?y(t)"> with cost <img src="https://latex.codecogs.com/png.latex?c_%7Bgsu%7D"></li>
<li>Generator shut-down status <img src="https://latex.codecogs.com/png.latex?z(t)"> with cost <img src="https://latex.codecogs.com/png.latex?c_%7Bgsd%7D"></li>
</ul>
<p>The objective function is to minimize the total cost, which includes the generator fixed (on) cost, generator variable cost, generator start-up cost, generator shut-down cost, load shedding cost, solar curtailment cost, and wind curtailment cost.</p>
<p>The first two constraints denote the relationship between the on-off status and the start-up and shut-down status, which can be formulated as the following table:</p>
<table class="caption-top table">
<caption>The relationship between the on-off status and the start-up and shut-down status</caption>
<thead>
<tr class="header">
<th style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?u_g(t-1)"></th>
<th style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?u_g(t)"></th>
<th style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?y_g(t)"></th>
<th style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?z_g(t)"></th>
<th style="text-align: center;">Explain</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">Keep off</td>
</tr>
<tr class="even">
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">Start-up</td>
</tr>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Shut-down</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">Keep on</td>
</tr>
</tbody>
</table>
<p>The ramp-up constraint is also modified where <img src="https://latex.codecogs.com/png.latex?R_%7Bgu%7D"> is the ramp-up limit as the previous section and <img src="https://latex.codecogs.com/png.latex?R_%7Bgsu%7D"> is the start-up ramp-up limit. The ramp-up status only has three states</p>
<table class="caption-top table">
<caption>The relationship between the on-off status and the start-up status</caption>
<thead>
<tr class="header">
<th style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?u_g(t-1)"></th>
<th style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?y_g(t)"></th>
<th style="text-align: center;">Explain</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">Keep off</td>
</tr>
<tr class="even">
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Start-up</td>
</tr>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">Shut-down or Keep on</td>
</tr>
</tbody>
</table>
<p>The ramp-down constraint is also modified where <img src="https://latex.codecogs.com/png.latex?R_%7Bdu%7D"> is the ramp-down limit as the previous section and <img src="https://latex.codecogs.com/png.latex?R_%7Bgsd%7D"> is the shut-down ramp-down limit.</p>
<p>Similarly, the ramp-down constraint is modified where <img src="https://latex.codecogs.com/png.latex?R_%7Bdu%7D"> is the ramp-down limit as the previous section and <img src="https://latex.codecogs.com/png.latex?R_%7Bgsd%7D"> is the shut-down ramp-down limit. The ramp-down status only has three states as well</p>
<table class="caption-top table">
<caption>The relationship between the on-off status and the shut-down status</caption>
<thead>
<tr class="header">
<th style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?u_g(t)"></th>
<th style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?z_g(t)"></th>
<th style="text-align: center;">Explain</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">Keep off</td>
</tr>
<tr class="even">
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Shut down</td>
</tr>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">Keep on or Start up</td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The initial conditions <img src="https://latex.codecogs.com/png.latex?u_g(0),%20y_g(0),%20z_g(0),%20P_g(0)"> need to be given.</p>
</div>
</div>


</section>

 ]]></description>
  <category>Power System</category>
  <guid>https://xuwkk.github.io/blog/posts/learning/power_system/ncuc.html</guid>
  <pubDate>Wed, 12 Jun 2024 23:00:00 GMT</pubDate>
</item>
<item>
  <title>Deep Implicit Layers: Differentiable Convex Layer</title>
  <dc:creator>Wangkun Xu</dc:creator>
  <link>https://xuwkk.github.io/blog/posts/learning/autometic_differentiation/differetiable_convex_layer.html</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In this post, I summarize my learning note on another type of implicit model - the differentiable convex layer. This layer has similar formulation, forward, and backward passes as the fixed point layer. This is because its implicit function can be formulated by the KKT condition, no matter which off-the-shelf optimization solver is used. This topic is related to the <a href="../../../posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html">tangent and adjoint sensitivity analysis of nonlinear system</a> as the KKT condition forms a nonlinear system in general. The main reference is the paper Differentiable Convex Optimization Layers by <a href="https://arxiv.org/abs/1910.12430">Amos et al.&nbsp;(2019)</a> and the <a href="https://implicit-layers-tutorial.org/differentiable_optimization/">blog: differentiable optimization</a>.</p>
</section>
<section id="formulation-on-convex-optimization-layer" class="level2">
<h2 class="anchored" data-anchor-id="formulation-on-convex-optimization-layer">Formulation on Convex Optimization Layer</h2>
<p>Consider the following optimization proble: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cunderset%7Bz%7D%7B%5Coperatorname%7Bminimize%7D%7D%20&amp;%20f(z)%20%5C%5C%0A%5Ctext%20%7B%20subject%20to%20%7D%20&amp;%20g(z)%20%5Cleq%200%20%5C%5C%0A&amp;%20h(z)=0,%0A%5Cend%7Baligned%7D%0A"></p>
<p>To make the optimization problem convex, we assume that <img src="https://latex.codecogs.com/png.latex?f(z)"> is convex, <img src="https://latex.codecogs.com/png.latex?g(z)"> is convex, and <img src="https://latex.codecogs.com/png.latex?h(z)"> is affine. The KKT condition of this optimization problem is <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0Ag%5Cleft(z%5E%7B%5Cstar%7D%5Cright)%20&amp;%20%5Cleq%200%20%5C%5C%0Ah%5Cleft(z%5E%7B%5Cstar%7D%5Cright)%20&amp;%20=0%20%5C%5C%0A%5Clambda%5E%7B%5Cstar%7D%20&amp;%20%5Cgeq%200%20%5C%5C%0A%5Clambda%5E%7B%5Cstar%7D%20%5Ccirc%20g%5Cleft(z%5E%7B%5Cstar%7D%5Cright)%20&amp;%20=0%20%5C%5C%0A%5Cnabla%20f%5Cleft(z%5E%7B%5Cstar%7D%5Cright)+%5Csum_%7Bi=1%7D%5Em%20%5Clambda_i%5E%7B%5Cstar%7D%20%5Cnabla%20g_i%5Cleft(z%5E%7B%5Cstar%7D%5Cright)+%5Csum_%7Bi=1%7D%5Ep%20%5Cnu_i%5E%7B%5Cstar%7D%20%5Cnabla%20h_i%5Cleft(z%5E%7B%5Cstar%7D%5Cright)%20&amp;%20=0%0A%5Cend%7Baligned%7D%0A"></p>
<p>Similarly to the previous parametric setting, we can define the convex layer as <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0Az%5E%7B%5Cstar%7D(x)=%5Cunderset%7Bz%7D%7B%5Coperatorname%7Bargmin%7D%7D%20&amp;%20f(z,%20x)%20%5C%5C%0A%5Ctext%20%7B%20subject%20to%20%7D%20&amp;%20g(z,%20x)%20%5Cleq%200%20%5C%5C%0A&amp;%20h(z,%20x)=0%0A%5Cend%7Baligned%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?z"> is the decision variable and <img src="https://latex.codecogs.com/png.latex?x"> is the input of the layer or the parameter of the optimization problem. Apart from <img src="https://latex.codecogs.com/png.latex?z%5E%5Cstar(x)"> (as an implicit function of <img src="https://latex.codecogs.com/png.latex?x">), we can also include the dual variables, <img src="https://latex.codecogs.com/png.latex?%0A(z%5E%5Cstar,%20%5Clambda%5E%5Cstar,%20%5Cnu%5E%5Cstar)(x)%0A"></p>
<p>The differentiable convex layer can be viewed as 1) a nonlinear implicit equation <img src="https://latex.codecogs.com/png.latex?G(x,%5Clambda,%5Cnu,x)%20=%200"> and 2) a fixed point iteration as iterative optimization algorithm (e.g., interior point, SQP, ADMM, etc) can be used to solve the optimization problem.</p>
<p>The equality condition KKT can be written as: <img src="https://latex.codecogs.com/png.latex?%0AG(z,%20%5Clambda,%20%5Cnu,x)=%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%0A%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20z%7D%20f(z,%20x)+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20z%7D%20g(z,%20x)%5ET%20%5Clambda+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20z%7D%20h(z,%20x)%5ET%20%5Cnu%20%5C%5C%0A%5Clambda%20%5Ccirc%20g(z,x)%20%5C%5C%0Ah(z,x)%0A%5Cend%7Barray%7D%5Cright%5D%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Ccirc"> denotes the element-wise product.</p>
<p>Assume that the optimal primal and dual pair has been found by the <strong>forward pass</strong>. At the optimal point, we have <img src="https://latex.codecogs.com/png.latex?%0AG(z%5E%5Cstar(x),%20%5Clambda%5E%5Cstar(x),%20%5Cnu%5E%5Cstar(x),%20x)%20=%200%0A"> which is an implicit function of <img src="https://latex.codecogs.com/png.latex?x">.</p>
<section id="general-formulation" class="level3">
<h3 class="anchored" data-anchor-id="general-formulation">General Formulation</h3>
<p>The KKT condition is exactly the implicit function defined in the <a href="posts/learning/autometic_differentiation/deep_implicit_layers.qmd">deep equilibrium model (DEQ)</a> (We will also show that the interrior-point method, which is basically the Newton’s method, can be used to solve the KKT condition later).</p>
<p>Denote <img src="https://latex.codecogs.com/png.latex?y(x)%20=%20(z%5E%5Cstar(x),%20%5Clambda%5E%5Cstar(x),%20%5Cnu%5E%5Cstar(x))">, then we can directly use the method implemented in <a href="../../../posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html">nonlinear equation sensitivity analysis</a> to compute the gradient of <img src="https://latex.codecogs.com/png.latex?y(x)"> with respect to <img src="https://latex.codecogs.com/png.latex?x">. Do the <strong>total derivative</strong> with respect to <img src="https://latex.codecogs.com/png.latex?x">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20G%7D%7Bd%20x%7D%20=%20%5Cfrac%7B%5Cpartial%20G%7D%7B%5Cpartial%20y%7D%5Cfrac%7Bd%20y%7D%7Bd%20x%7D%20+%20%5Cfrac%7B%5Cpartial%20G%7D%7B%5Cpartial%20x%7D%20=%200%0A"></p>
<p>Therefore, the Jacobian of the decision <img src="https://latex.codecogs.com/png.latex?y"> with respect to the input <img src="https://latex.codecogs.com/png.latex?x"> is found as <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20y%7D%7Bd%20x%7D%20=%20-%5Cleft(%5Cfrac%7B%5Cpartial%20G%7D%7B%5Cpartial%20y%7D%5Cright)%5E%7B-1%7D%20%5Cfrac%7B%5Cpartial%20G%7D%7B%5Cpartial%20x%7D%0A"></p>
<p>In the backward pass, the adjoint of the optimal point (denote as <img src="https://latex.codecogs.com/png.latex?%5Cdot%7By%7D">) is computed backpropagated by the scalar loss <img src="https://latex.codecogs.com/png.latex?%5Cell(%5Ccdot)"> as <img src="https://latex.codecogs.com/png.latex?%5Cdot%7By%7D%20=%20%5Cfrac%7B%5Cpartial%20%5Cell%7D%7B%5Cpartial%20y%7D">. Note that when <img src="https://latex.codecogs.com/png.latex?%5Cell(%5Ccdot)"> does not depend on the primal and dual variables, the associated adjoint is zero.</p>
<p>Consequently, <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20y%7D%7Bd%20x%7D%20=%20-%20%5Cfrac%7Bd%20%5Cell%7D%7Bd%20y%7D%20%5Cleft(%5Cfrac%7B%5Cpartial%20G%7D%7B%5Cpartial%20y%7D%5Cright)%5E%7B-1%7D%20%5Cfrac%7B%5Cpartial%20G%7D%7B%5Cpartial%20x%7D%0A"></p>
<p>The same treatment as in <a href="../../../posts/learning/autometic_differentiation/deep_implicit_layers.html">fixed point iteration</a> can be used to compute the above gradient. In detail, the first two items form the adjoint equation of <img src="https://latex.codecogs.com/png.latex?y"> whose adjoint (or gradient) needs to be modified. <span id="eq-adjoint"><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft(%5Cfrac%7B%5Cpartial%20G%7D%7B%5Cpartial%20y%7D%5Cright)%5ET%20%5Cdot%7By%7D%20=%20-%5Cfrac%7B%5Cpartial%20%5Cell%7D%7B%5Cpartial%20y%7D%0A%5Ctag%7B1%7D"></span></p>
</section>
<section id="detailed-formulation" class="level3">
<h3 class="anchored" data-anchor-id="detailed-formulation">Detailed Formulation</h3>
<p>The partial Jacobian <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20G%7D%7B%5Cpartial%20y%7D"> can be analytically computed as <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20z,%20%5Clambda,%20%5Cnu%7D%20G%5Cleft(z%5E%7B%5Cstar%7D,%20%5Clambda%5E%7B%5Cstar%7D,%20%5Cnu%5E%7B%5Cstar%7D,%20x%5Cright)=%0A%5Cleft%5B%5Cbegin%7Barray%7D%7Bccc%7D%0A%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial%20z%5E2%7D%20f%5Cleft(z%5E%7B%5Cstar%7D,%20x%5Cright)+%5Csum_%7Bi=1%7D%5Em%20%5Clambda_i%5E%7B%5Cstar%7D%20%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial%20z%5E2%7D%20g_i%5Cleft(z%5E%7B%5Cstar%7D,%20x%5Cright)%20&amp;%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20z%7D%20g%5Cleft(z%5E%7B%5Cstar%7D,%20x%5Cright)%20&amp;%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20z%7D%20h%5Cleft(z%5E%7B%5Cstar%7D,%20x%5Cright)%20%5C%5C%0A%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20z%7D%20g%5Cleft(z%5E%7B%5Cstar%7D,%20x%5Cright)%5ET%20%5Coperatorname%7Bdiag%7D%5Cleft(%5Clambda%5E%7B%5Cstar%7D%5Cright)%20&amp;%20%5Coperatorname%7Bdiag%7D%5Cleft(g%5Cleft(z%5E%7B%5Cstar%7D,%20x%5Cright)%5Cright)%20&amp;%200%20%5C%5C%0A%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20z%7D%20h%5Cleft(z%5E%7B%5Cstar%7D,%20x%5Cright)%5ET%20&amp;%200%20&amp;%200%0A%5Cend%7Barray%7D%5Cright%5D%0A"></p>
</section>
<section id="quadratic-programming-formulation" class="level3">
<h3 class="anchored" data-anchor-id="quadratic-programming-formulation">Quadratic Programming Formulation</h3>
<p>In the <a href="https://arxiv.org/abs/1703.00443">OptNet paper</a>, the author implemented the differentiable QP layer. In this section, the explicit formulations are derived. We will follow the same notation in this paper.</p>
<p>Consider the following QP problem: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmin_z%20&amp;%20%5Cquad%20%5Cfrac%7B1%7D%7B2%7D%20z%5ET%20Q%20z%20+%20q%5ET%20z%20%5C%5C%0A%5Ctext%7Bs.t.%7D%20&amp;%20%5Cquad%20A%20z%20=%20b%20%5C%5C%0A&amp;%20%5Cquad%20G%20z%20%5Cleq%20h%0A%5Cend%7Baligned%7D%0A"> where the parameters <img src="https://latex.codecogs.com/png.latex?Q,%20q,%20A,%20b,%20C,%20h"> are functions of the input <img src="https://latex.codecogs.com/png.latex?z_i"> (the term <img src="https://latex.codecogs.com/png.latex?z_i"> is used as in the paper this is referred to the output from the previous layer). Our goal is to compute the Jacobian of <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%20%5Cell%7D%7Bd%20z_i%7D">.</p>
<p>The KKT condition (only the equality part) of the above QP can be written as <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AQz%5E%5Cstar%20+%20q%20+%20A%5ET%20%5Cnu%5E%5Cstar%20+%20G%5ET%20%5Clambda%5E%5Cstar%20&amp;=%200%20%5C%5C%0AAz%5E%5Cstar%20-%20b%20&amp;=%200%20%5C%5C%0AD(%5Clambda%5E%5Cstar)%20(Gz%5E%5Cstar%20-%20h)%20&amp;=%200%0A%5Cend%7Baligned%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?D(%5Clambda%5E%5Cstar)"> is a diagonal matrix with the diagonal elements being <img src="https://latex.codecogs.com/png.latex?%5Clambda%5E%5Cstar">.</p>
<p>Compactly, it is written as <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0AQ%20&amp;%20G%5ET%20&amp;%20A%5ET%20%5C%5C%0AD(%5Clambda%5E%5Cstar)G%20&amp;%20D(Gz%5E%5Cstar%20-%20h)%20&amp;%200%20%5C%5C%0AA%20&amp;%200%20&amp;%200%0A%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20z%5E%5Cstar%20%5C%5C%20%5Clambda%5E%5Cstar%20%5C%5C%20%5Cnu%5E%5Cstar%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%20-q%20%5C%5C%200%20%5C%5C%20b%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>Differentiating the KKT equations with respect to the input <img src="https://latex.codecogs.com/png.latex?z_i"> gives (it is assumed that all parameters are dependent on <img src="https://latex.codecogs.com/png.latex?z_i">): <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cfrac%7BdQ%7D%7Bdz_i%7Dz%5E%5Cstar%20+%20Q%5Cfrac%7Bdz%5E%5Cstar%7D%7Bdz_i%7D%20+%20%5Cfrac%7Bdq%7D%7Bdz_i%7D%20+%20%5Cfrac%7BdA%5ET%7D%7Bdz_i%7D%5Cnu%5E%5Cstar%20+%20A%5ET%20%5Cfrac%7Bd%5Cnu%5E%5Cstar%7D%7Bdz_i%7D%20+%20%5Cfrac%7BdG%5ET%7D%7Bdz_i%7D%5Clambda%5E%5Cstar%20+%20G%5ET%20%5Cfrac%7Bd%5Clambda%5E%5Cstar%7D%7Bdz_i%7D%20&amp;=%200%20%5C%5C%0A%5Cfrac%7BdA%7D%7Bdz_i%7Dz%5E%5Cstar%20+%20A%5Cfrac%7Bdz%5E%5Cstar%7D%7Bdz_i%7D%20-%20%5Cfrac%7Bdb%7D%7Bdz_i%7D%20&amp;=%200%20%5C%5C%0AD(Gz%5E%5Cstar%20-%20h)%5Cfrac%7Bd%5Clambda%5E%5Cstar%7D%7Bdz_i%7D%20+%20D(%5Clambda%5E%5Cstar)%5Cleft(%5Cfrac%7BdG%7D%7Bdz_i%7Dz%5E%5Cstar%20+%20G%5Cfrac%7Bdz%5E%5Cstar%7D%7Bdz_i%7D%20-%20%5Cfrac%7Bdh%7D%7Bdz_i%7D%20%5Cright)%20&amp;=0%0A%5Cend%7Baligned%7D%0A"> which can be compactly written as matrix form <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0AQ%20&amp;%20G%5ET%20&amp;%20A%5ET%20%5C%5C%0AD(%5Clambda%5E%5Cstar)G%20&amp;%20D(Gz%5E%5Cstar%20-%20h)%20&amp;%200%20%5C%5C%0AA%20&amp;%200%20&amp;%200%0A%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20%5Cfrac%7Bdz%7D%7Bdz_i%7D%20%5C%5C%20%5Cfrac%7Bd%5Clambda%7D%7Bdz_i%7D%20%5C%5C%20%5Cfrac%7Bd%5Cnu%7D%7Bdz_i%7D%20%5Cend%7Bbmatrix%7D%20=%20-%5Cbegin%7Bbmatrix%7D%20%5Cfrac%7BdQ%7D%7Bdz_i%7Dz%5E%5Cstar%20+%20%5Cfrac%7Bdq%7D%7Bdz_i%7D%20+%20%5Cfrac%7BdA%5ET%7D%7Bdz_i%7D%5Cnu%5E%5Cstar%20+%20%5Cfrac%7BdG%5ET%7D%7Bdz_i%7D%5Clambda%20%5C%5C%20D(%5Clambda%5E%5Cstar)%5Cfrac%7BdG%7D%7Bdz_i%7Dz%5E%5Cstar%20-%20D(%5Clambda%5E%5Cstar)%5Cfrac%7Bdh%7D%7Bdz_i%7D%20%5C%5C%20%5Cfrac%7BdA%7D%7Bdz_i%7Dz%20-%20%5Cfrac%7Bdb%7D%7Bdz_i%7D%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>The paper also suggest using the adjoint method to compute the gradient of the loss with respect to the input <img src="https://latex.codecogs.com/png.latex?z_i">. For example, in most cases, <img src="https://latex.codecogs.com/png.latex?%5Cell"> is not dependent on <img src="https://latex.codecogs.com/png.latex?%5Clambda"> and <img src="https://latex.codecogs.com/png.latex?%5Cnu">, so the adjoint equation (after transpose) is <img src="https://latex.codecogs.com/png.latex?%0A%5Cdot%7By%7D%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bl%7D%0Ad_z%20%5C%5C%0Ad_%5Clambda%20%5C%5C%0Ad_%5Cnu%0A%5Cend%7Barray%7D%5Cright%5D=%20-%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bccc%7D%0AQ%20&amp;%20G%5ET%20D%5Cleft(%5Clambda%5E%7B%5Cstar%7D%5Cright)%20&amp;%20A%5ET%20%5C%5C%0AG%20&amp;%20D%5Cleft(G%20z%5E%7B%5Cstar%7D-h%5Cright)%20&amp;%200%20%5C%5C%0AA%20&amp;%200%20&amp;%200%0A%5Cend%7Barray%7D%5Cright%5D%5E%7B-1%7D%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%0A%5Cleft(%5Cfrac%7B%5Cpartial%20%5Cell%7D%7B%5Cpartial%20z%5E%7B%5Cstar%7D%7D%5Cright)%5ET%20%5C%5C%0A0%20%5C%5C%0A0%0A%5Cend%7Barray%7D%5Cright%5D%0A"></p>
<p><strong>Derive eq.(8) in the paper</strong> requires matrix differentiation. For instance, we can obtain <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%5Cell%7D%7BdA%7D%20=%20%5Cfrac%7Bd%5Cell%7D%7Bdz%7D%20%5Cfrac%7Bdz%7D%7BdA%7D"> by differentiating the KKT condition with respect to <img src="https://latex.codecogs.com/png.latex?A">.</p>
</section>
<section id="efficient-forward-and-backward-pass" class="level3">
<h3 class="anchored" data-anchor-id="efficient-forward-and-backward-pass">Efficient Forward and Backward Pass</h3>
<p>Today’s state-of-the-art QP solvers like Gurobi and CPLEX do not have the capability of solving multiple optimization problems on the <strong>GPU</strong> in parallel across the entire <strong>minibatch</strong>.</p>
<p>The paper develops a GPU-based primal-dual interior point method (PDIPM). It considers the following QP with slack variables <img src="https://latex.codecogs.com/png.latex?s">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmin_%7Bz,%20s%7D%20&amp;%20%5Cquad%20%5Cfrac%7B1%7D%7B2%7D%20z%5ET%20Q%20z%20+%20q%5ET%20z%20%5C%5C%0A%5Ctext%7Bs.t.%7D%20&amp;%20%5Cquad%20A%20z%20=%20b%20%5C%5C%0A&amp;%20%5Cquad%20G%20z%20+%20s%20=%20h%20%5C%5C%0A&amp;%20%5Cquad%20s%20%5Cgeq%200%0A%5Cend%7Baligned%7D%0A"></p>
<p>Then the KKT condition can be written as <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AQz%20+%20q%20+%20A%5ET%20%5Cnu%20+%20G%5ET%20%5Clambda%20&amp;=%200%20%5C%5C%0AAz%20-%20b%20&amp;=%200%20%5C%5C%0AGz%20+%20s%20-%20h%20&amp;=%200%20%5C%5C%0A%5Clambda%20%5Ccirc%20s%20&amp;=%200%0A%5Cend%7Baligned%7D%0A"></p>
<p>The last condition is exactly the complementarity slackness.</p>
<p>A Newton’s method can be used to solve the above system, with some modification, it can becomes the PDIPM.</p>
<p>Taking the first-order expansion of the KKT condition, we have <img src="https://latex.codecogs.com/png.latex?%0AK%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%0A%5CDelta%20z%5E%7B%5Ctext%20%7Baff%20%7D%7D%20%5C%5C%0A%5CDelta%20s%5E%7B%5Ctext%20%7Baff%20%7D%7D%20%5C%5C%0A%5CDelta%20%5Clambda%5E%7B%5Ctext%20%7Baff%20%7D%7D%20%5C%5C%0A%5CDelta%20%5Cnu%5E%7B%5Ctext%20%7Baff%20%7D%7D%0A%5Cend%7Barray%7D%5Cright%5D=%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%0A-%5Cleft(A%5ET%20%5Cnu+G%5ET%20%5Clambda+Q%20z+q%5Cright)%20%5C%5C%0A-S%20%5Clambda%20%5C%5C%0A-(G%20z+s-h)%20%5C%5C%0A-(A%20z-b)%0A%5Cend%7Barray%7D%5Cright%5D%0A"> where <img src="https://latex.codecogs.com/png.latex?%0AK=%5Cleft%5B%5Cbegin%7Barray%7D%7Bcccc%7D%0AQ%20&amp;%200%20&amp;%20G%5ET%20&amp;%20A%5ET%20%5C%5C%0A0%20&amp;%20D(%5Clambda)%20&amp;%20D(s)%20&amp;%200%20%5C%5C%0AG%20&amp;%20I%20&amp;%200%20&amp;%200%20%5C%5C%0AA%20&amp;%200%20&amp;%200%20&amp;%200%0A%5Cend%7Barray%7D%5Cright%5D%0A"></p>
<p>The solution (with some modification) will becomes the update direction.</p>
<p>There are several modifications that can be made to improve the efficiency of the PDIPM. First, a symmetric KKT matrix can be used that can be easily solved, by scaling the second block equation by <img src="https://latex.codecogs.com/png.latex?D(1/S)">. Then the second block row becomes <img src="https://latex.codecogs.com/png.latex?%0AD(%5Clambda/S)%20%5CDelta%20s%5E%7B%5Ctext%20%7Baff%20%7D%7D%20+%20%5CDelta%20%5Clambda%5E%7B%5Ctext%20%7Baff%20%7D%7D%20=%20-%20%5Clambda%0A"> and <img src="https://latex.codecogs.com/png.latex?%0AK_%7Bsym%7D%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcccc%7D%0AQ%20&amp;%200%20&amp;%20G%5ET%20&amp;%20A%5ET%20%5C%5C%0A0%20&amp;%20D(%5Clambda/S)%20&amp;%20I%20&amp;%200%20%5C%5C%0AG%20&amp;%20I%20&amp;%200%20&amp;%200%20%5C%5C%0AA%20&amp;%200%20&amp;%200%20&amp;%200%0A%5Cend%7Barray%7D%5Cright%5D%0A"> which is symmetric.</p>
<p>Second, as the PDIPM requires to solve linear system during the iteration, the paper uses the block factorization technique and pre-factorize portions of them that don’t change during the iteration.</p>
<p>Third, because the KKT condition represents a nonlinear implicit function, the backpropagation can be found by the implicit function theorem. The paper also finds out that the above matrix (that needs to be inverted when solving the adjoint equation) has been already computed/factorized during the forward pass using the interior point method. Therefore, the computational cost of backpropagation is ‘almost free’. This finding is similar to the <a href="../../../posts/learning/autometic_differentiation/deep_implicit_layers.html">fixed point iteration</a> where the Jacobian of the fixed point iteration is already computed during the forward pass using the Newton’s method.</p>
</section>
<section id="steps" class="level3">
<h3 class="anchored" data-anchor-id="steps">Steps</h3>
<p>Similar to the <a href="../../../posts/learning/autometic_differentiation/deep_implicit_layers.html">fixed point iteration</a>. The steps of implementing the differentiable convex layer are:</p>
<ol type="1">
<li><strong>Outside the gradient tape</strong>, solve the optimization problem to find the optimal primal and dual variables <img src="https://latex.codecogs.com/png.latex?(z%5E%5Cstar,%20%5Clambda%5E%5Cstar,%20%5Cnu%5E%5Cstar)"> using any off-the-shelf optimization solver, such as interior point, SQP, ADMM, etc.</li>
<li><strong>Inside the gradient tape</strong>, engage the input <img src="https://latex.codecogs.com/png.latex?x"> to the computation graph by <img src="https://latex.codecogs.com/png.latex?(z,%5Clambda,%5Cnu):=%20(z%5E%5Cstar,%5Clambda%5E%5Cstar,%5Cnu%5E%5Cstar)%20-%20G(z%5E%5Cstar,%5Clambda%5E%5Cstar,%5Cnu%5E%5Cstar,x)">. This will provide the the gradient <img src="https://latex.codecogs.com/png.latex?-%5Cfrac%7Bd%20G%7D%7Bd%20x%7D"> in the computation graph.</li>
<li><strong>Register the gradient</strong> of <img src="https://latex.codecogs.com/png.latex?(z,%5Clambda,%5Cnu)"> with the solution of the linear adjoint system Equation&nbsp;1.</li>
</ol>
</section>
<section id="pytorch-implementation" class="level3">
<h3 class="anchored" data-anchor-id="pytorch-implementation">Pytorch Implementation</h3>
<p>An example code snippet, including the an implementation of <code>OptNet</code> and comparison to <code>CvxpyLayers</code> can be found at <a href="https://github.com/xuwkk/blog_code">my-github</a>.</p>


</section>
</section>

 ]]></description>
  <category>Auto Differentiation</category>
  <category>Implicit Function</category>
  <category>Paper Read</category>
  <category>Optimization</category>
  <guid>https://xuwkk.github.io/blog/posts/learning/autometic_differentiation/differetiable_convex_layer.html</guid>
  <pubDate>Mon, 10 Jun 2024 23:00:00 GMT</pubDate>
</item>
<item>
  <title>Deep Implicit Layers: Fixed-Point Iteration</title>
  <dc:creator>Wangkun Xu</dc:creator>
  <link>https://xuwkk.github.io/blog/posts/learning/autometic_differentiation/deep_implicit_layers.html</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In the previous posts, I summarize the mathematical background of the adjoint method for <a href="../../../posts/learning/autometic_differentiation/adjoint_linear_equation.html">linear system</a> and <a href="../../../posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html">nonlinear system</a>. This post will summarize them in the view of deep implicit layers. The main reference of this post is <a href="https://implicit-layers-tutorial.org/introduction/.">Deep Implicit Layers</a>.</p>
<p>The nonlinear equation <img src="https://latex.codecogs.com/png.latex?g(x,z)=0"> can be viewed beyond the a simple algebraic equation. Similiar to the sensitivity analysis of the equation, we can design the neural network as an implicit function of the parameter and the solution. The gradinet used for backpropagation can be found by the same idea of sensitivity analysis in the previous posts. This means that we can 1. Encode the implicit layer with physical meaning as part of the neural network. 2. Regard the entire neural network or part of it as a implicit model. For example, ResNet can be viewed as NeuralODE and feedforward neural network can be viewed as deep equilibrium model.</p>
<table class="caption-top table">
<caption>Implicit layers for different types of equations.</caption>
<colgroup>
<col style="width: 60%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th>Equation Type</th>
<th style="text-align: left;">Neural Network</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Algebraic equation (fixed point iteration)</td>
<td style="text-align: left;">Deep equilibrium model</td>
</tr>
<tr class="even">
<td>Ordinary differential equation</td>
<td style="text-align: left;">NeuralODE</td>
</tr>
<tr class="odd">
<td>Convex optimization</td>
<td style="text-align: left;">Differentiable convex layer</td>
</tr>
</tbody>
</table>
<p>The benefits of using implicit layers are 1. The solution can be found by off-the-shelf solver, regardless of the the layer itself. E.g., the fixed point iteration can be solved by Newton’s method; the ODE can be solved by the ODE solver such as Euler’s method; the convex optimization can be solved by the convex optimization solver, such as ADMM. And more. 2. Because the solution procedure is separated from the layer, it does not need to be recorded on the computational graph (although the solution procedure can be unrolled on the computatonal graph). This improves the memory efficiency and numetical stability. 3. Because the forward pass of implicit layer requires a solution procedure which is usually an iterative process (thus repeated nonlinearity), the representation power of the implicit layer is stronger than the explicit layer.</p>
</section>
<section id="fixed-point-iteration" class="level2">
<h2 class="anchored" data-anchor-id="fixed-point-iteration">Fixed Point Iteration</h2>
<p>A fixed point iteration <img src="https://latex.codecogs.com/png.latex?%0Az%5E%7B%5Cstar%7D=%5Ctanh%20%5Cleft(W%20z%5E%7B%5Cstar%7D+x%5Cright)%0A"> can be written as a nonlinear equation <img src="https://latex.codecogs.com/png.latex?%0Ag(x,%20z)=z-%5Ctanh%20%5Cleft(W%20z+x%5Cright)=0%0A"> where <img src="https://latex.codecogs.com/png.latex?W%5Cin%5Cmathbb%7BR%7D%5E%7Bn%5Ctimes%20n%7D">.</p>
<section id="forward-pass" class="level3">
<h3 class="anchored" data-anchor-id="forward-pass">Forward Pass</h3>
<p>For a given <img src="https://latex.codecogs.com/png.latex?x">, Newton’s method can be used to iteratively solve the equation, <img src="https://latex.codecogs.com/png.latex?%0Az:=%20z%20-%20%5Cleft(%5Cfrac%7B%5Cpartial%20g%7D%7B%5Cpartial%20z%7D%5Cright)%5E%7B-1%7D%20g(x,z)%0A"></p>
<p>The partial Jacobian of <img src="https://latex.codecogs.com/png.latex?g"> with repect to <img src="https://latex.codecogs.com/png.latex?z"> can be found by automatic differentation tool such as <code>torch.func.jacfwd()</code> in general. For the simple case, it can be found analytically as <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20g%7D%7B%5Cpartial%20z%7D%20=%20I%20-%20%5Ctext%7Bdiag%7D(%5Ctanh'(Wz+x))W%0A"></p>
<p>Note that the forward pass is not recorded on the computational graph.</p>
</section>
<section id="backward-pass" class="level3">
<h3 class="anchored" data-anchor-id="backward-pass">Backward Pass</h3>
<p>We can use the same technique in <a href="../../../posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html">nonlinear adjoint method</a> to do the <strong>reverse mode</strong> differentiation or <strong>bachpropagation</strong>. To have a quick review, do the <strong>total</strong> derivative of <img src="https://latex.codecogs.com/png.latex?g"> with respect to <img src="https://latex.codecogs.com/png.latex?x">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20g%7D%7Bd%20x%7D%20=%20%5Cfrac%7B%5Cpartial%20g%7D%7B%5Cpartial%20x%7D%20+%20%5Cfrac%7B%5Cpartial%20g%7D%7B%5Cpartial%20z%7D%20%5Cfrac%7Bd%20z%7D%7Bd%20x%7D%20=%200%0A"></p>
<p>The Jacobian <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%20z%7D%7Bd%20x%7D"> can be found as <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20z%7D%7Bd%20x%7D%20=%20-%5Cleft(%5Cfrac%7B%5Cpartial%20g%7D%7B%5Cpartial%20z%7D%5Cright)%5E%7B-1%7D%20%5Cfrac%7B%5Cpartial%20g%7D%7B%5Cpartial%20x%7D%0A"></p>
<p>Note that the term <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20g%7D%7B%5Cpartial%20z%7D"> has already been computed in the forward pass, e.g.&nbsp;we can directly use the Jacobian and its inverse calculated in the last iteration of Newton’s method.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>This is a very common observation in various implicit layer application where you can do the reverse-mode differentiation for ‘free’.</p>
</div>
</div>
<p>Directly solving the Jacobian <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%20z%7D%7Bd%20x%7D"> is not efficient. As in the Tangent method, the number of linear systems that need to solve is the same as the number of the parameters <img src="https://latex.codecogs.com/png.latex?x">.</p>
<p>Modern deep learning model is trained by reverse-mode differentiation, which is more efficient than the forward-mode differentiation. Let <img src="https://latex.codecogs.com/png.latex?%5Cell(%5Ccdot)"> be the scalar loss function, the Jacobian with respect to <img src="https://latex.codecogs.com/png.latex?x"> can be found by <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20%5Cell%7D%7Bd%20x%7D%20=%20%5Cfrac%7Bd%20%5Cell%7D%7Bd%20z%7D%20%5Cfrac%7Bd%20z%7D%7Bd%20x%7D%20=%20%5Cfrac%7Bd%20%5Cell%7D%7Bd%20z%7D%20%5Cleft(-%5Cleft(%5Cfrac%7B%5Cpartial%20g%7D%7B%5Cpartial%20z%7D%5Cright)%5E%7B-1%7D%20%5Cfrac%7B%5Cpartial%20g%7D%7B%5Cpartial%20x%7D%5Cright)%0A"></p>
<p>Again we can find the vector Jacobian product of the first two terms as the new adjoint of <img src="https://latex.codecogs.com/png.latex?z"> denoted as <img src="https://latex.codecogs.com/png.latex?%5Cdot%7Bz%7D">: <span id="eq-adjoint"><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft(%5Cfrac%7B%5Cpartial%20g%7D%7B%5Cpartial%20z%7D%5Cright)%5ET%20%5Cdot%7Bz%7D%20=%20%5Cleft(%5Cfrac%7Bd%20%5Cell%7D%7Bd%20z%7D%5Cright)%5ET%0A%5Ctag%7B1%7D"></span></p>
<p>Then we need to re-engage <img src="https://latex.codecogs.com/png.latex?x"> on the computational graph. This can be done by calculating <img src="https://latex.codecogs.com/png.latex?%0Az:=%20z%20-%20g(x,z)%0A"> whose gradient with respect to <img src="https://latex.codecogs.com/png.latex?x"> is <img src="https://latex.codecogs.com/png.latex?-%5Cfrac%7B%5Cpartial%20g%7D%7B%5Cpartial%20x%7D">, which is the last term in <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%5Cell%7D%7Bdx%7D">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bdz%7D%7Bdx%7D%20=%20-%5Cfrac%7B%5Cpartial%20g%7D%7B%5Cpartial%20x%7D%0A"></p>
<p>(The minus sign depends how the adjoint system is defined.)</p>
<p>To sum up, the process of forward and backward pass of fixed-point iteration is</p>
<ol type="1">
<li>Forward pass: Solve the nonlinear equation <img src="https://latex.codecogs.com/png.latex?g(x,z)=0"> by off-the-shelf solver. This is outside the automatic differentiation tape.</li>
<li>In the automatic differentiation tape, re-engage <img src="https://latex.codecogs.com/png.latex?x"> on the computational graph by <img src="https://latex.codecogs.com/png.latex?z:=%20z%20-%20g(x,z)">.</li>
<li>Modify the gradient of the above <img src="https://latex.codecogs.com/png.latex?z"> as the solution to Equation&nbsp;1, using the <code>register_hook()</code> method in PyTorch.</li>
</ol>
</section>
</section>
<section id="pytorch-implementation" class="level2">
<h2 class="anchored" data-anchor-id="pytorch-implementation">PyTorch Implementation</h2>
<p>Here, I implement a simple fixed-point iteration layer in PyTorch and compare it to the previous method in the <a href="../../../posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html">nonlinear adjoint method</a>.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torch <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> nn</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> loss_fn(z):</span>
<span id="cb1-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(z<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, axis <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).mean()</span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> FixedPointLayer(torch.nn.Module):</span>
<span id="cb1-8">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, W, tol <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-4</span>, max_iter <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb1-9">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>(FixedPointLayer, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>).<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb1-10">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.W <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.nn.Parameter(W, requires_grad <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb1-11">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.tol <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tol</span>
<span id="cb1-12">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.max_iter <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> max_iter</span>
<span id="cb1-13">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># implement by vmap</span></span>
<span id="cb1-14">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.implicit_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.vmap(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.implicit_model_)</span>
<span id="cb1-15">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.jac_batched <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.vmap(torch.func.jacfwd(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.implicit_model_, argnums <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>))</span>
<span id="cb1-16"></span>
<span id="cb1-17">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> implicit_model_(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, z, x):</span>
<span id="cb1-18">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> torch.tanh(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.W <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x)</span>
<span id="cb1-19">    </span>
<span id="cb1-20">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> newton_step(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, z, x, g):</span>
<span id="cb1-21">        J <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.jac_batched(z, x)</span>
<span id="cb1-22">        z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> torch.linalg.solve(J, g)</span>
<span id="cb1-23">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> z, J</span>
<span id="cb1-24"></span>
<span id="cb1-25">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x):</span>
<span id="cb1-26">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.iteration <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb1-27">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad():</span>
<span id="cb1-28">            z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tanh(x)</span>
<span id="cb1-29">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">while</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.iteration <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.max_iter:</span>
<span id="cb1-30">                g <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.implicit_model(z, x)</span>
<span id="cb1-31">                <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.err <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.norm(g)</span>
<span id="cb1-32"></span>
<span id="cb1-33">                <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.err <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.tol:</span>
<span id="cb1-34">                    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">break</span></span>
<span id="cb1-35"></span>
<span id="cb1-36">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># newton's method</span></span>
<span id="cb1-37">                z, J <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.newton_step(z, x, g)</span>
<span id="cb1-38">                <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.iteration <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb1-39">        </span>
<span id="cb1-40">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># re-engage the autograd tape</span></span>
<span id="cb1-41">        z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.implicit_model(z, x)</span>
<span id="cb1-42">        z.register_hook(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> grad : torch.linalg.solve(J.transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>), grad))</span>
<span id="cb1-43"></span>
<span id="cb1-44">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> z</span>
<span id="cb1-45"></span>
<span id="cb1-46"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> implicit_model(W, x, z):</span>
<span id="cb1-47">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># the g function</span></span>
<span id="cb1-48">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> torch.tanh(W <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x)</span>
<span id="cb1-49"></span>
<span id="cb1-50"></span>
<span id="cb1-51"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> implicit_model_test(W, x, z):</span>
<span id="cb1-52"></span>
<span id="cb1-53">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> x.dim() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:</span>
<span id="cb1-54">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># single sample case</span></span>
<span id="cb1-55">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'using the implicit model on one sample'</span>)</span>
<span id="cb1-56">        z_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> z.clone().detach()</span>
<span id="cb1-57">        x_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x.clone().detach()</span>
<span id="cb1-58">        </span>
<span id="cb1-59">        dl_dz <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.func.grad(loss_fn)(z_)</span>
<span id="cb1-60">        df_dW, df_dz <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.func.jacfwd(implicit_model, argnums <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>))(W, x_, z_)</span>
<span id="cb1-61">        </span>
<span id="cb1-62">        adjoint_variable <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.linalg.solve(df_dz.T, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>dl_dz)</span>
<span id="cb1-63">        </span>
<span id="cb1-64">        dl_dW <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.einsum(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'i,ikl-&gt;kl'</span>, adjoint_variable, df_dW)</span>
<span id="cb1-65">    </span>
<span id="cb1-66">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb1-67">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'using the implicit model on all samples'</span>)</span>
<span id="cb1-68">        z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> z.clone().detach()</span>
<span id="cb1-69">        x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x.clone().detach()</span>
<span id="cb1-70">        </span>
<span id="cb1-71">        dl_dz <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.func.grad(loss_fn)(z)</span>
<span id="cb1-72"></span>
<span id="cb1-73">        jacfwd_batched <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.vmap(torch.func.jacfwd(implicit_model, argnums <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)), in_dims <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>))</span>
<span id="cb1-74">        df_dW, df_dz <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> jacfwd_batched(W, x, z)</span>
<span id="cb1-75"></span>
<span id="cb1-76">        adjoint_variable <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.linalg.solve(df_dz.transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>), <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>dl_dz)</span>
<span id="cb1-77"></span>
<span id="cb1-78">        dl_dW <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.einsum(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bi,bikl-&gt;kl'</span>, adjoint_variable, df_dW)</span>
<span id="cb1-79">    </span>
<span id="cb1-80">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dl_dz'</span>, dl_dz.shape)</span>
<span id="cb1-81">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'df_dW'</span>, df_dW.shape)</span>
<span id="cb1-82">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'df_dz'</span>, df_dz.shape)</span>
<span id="cb1-83">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'adjoint_variable'</span>, adjoint_variable.shape)</span>
<span id="cb1-84">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dl_dW'</span>, dl_dW)</span>
<span id="cb1-85"></span>
<span id="cb1-86"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># maint function</span></span>
<span id="cb1-87">torch.random.manual_seed(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-88"></span>
<span id="cb1-89">batch_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="cb1-90">n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span></span>
<span id="cb1-91">W <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.randn(n,n).double() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span></span>
<span id="cb1-92">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.randn(batch_size,n, requires_grad<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>).double()</span>
<span id="cb1-93"></span>
<span id="cb1-94"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'using the model'</span>)</span>
<span id="cb1-95">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> FixedPointLayer(W, tol<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-10</span>, max_iter <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>).double()</span>
<span id="cb1-96"></span>
<span id="cb1-97"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># check with the numerical gradient</span></span>
<span id="cb1-98">torch.autograd.gradcheck(model, x, check_undefined_grad<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, raise_exception<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb1-99"></span>
<span id="cb1-100">z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(x)</span>
<span id="cb1-101">loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> loss_fn(z)</span>
<span id="cb1-102">loss.backward()</span>
<span id="cb1-103"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(model.W.grad)</span>
<span id="cb1-104"></span>
<span id="cb1-105"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># implicit model method</span></span>
<span id="cb1-106">implicit_model_test(W, x[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], z[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb1-107">implicit_model_test(W, x, z)</span></code></pre></div>
<p>In the above example, <code>torch.vmap()</code> is used for multi-batch implementation of the implicit function and the Jacobian. The <code>torch.func.jacfwd()</code> is used for the Jacobian calculation. Note that the Jacobian <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20g%7D%7B%5Cpartial%20z%7D"> is found by automatic differentiation, instead of analytical computation. The <code>torch.linalg.solve()</code> is used for the linear system solution. The <code>torch.einsum()</code> is used for the matrix multiplication.</p>
<p>In detail, in</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">jacfwd_batched <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.vmap(torch.func.jacfwd(implicit_model, argnums <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)), in_dims <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span></code></pre></div>
<p>The <code>torch.func.jacfwd(implicit_model, argnums = (0,2))</code> is to find the Jacobian of the implicit model with respect to <img src="https://latex.codecogs.com/png.latex?W"> and <img src="https://latex.codecogs.com/png.latex?z">. The <code>torch.vmap()</code> is to apply the <code>torch.func.jacfwd()</code> to each batch of the input. The <code>in_dims = (None, 0, 0)</code> is to specify that only <code>z</code> and <code>x</code> are batched while <code>W</code> is not.</p>


</section>

 ]]></description>
  <category>Auto Differentiation</category>
  <category>Implicit Function</category>
  <guid>https://xuwkk.github.io/blog/posts/learning/autometic_differentiation/deep_implicit_layers.html</guid>
  <pubDate>Mon, 10 Jun 2024 23:00:00 GMT</pubDate>
</item>
<item>
  <title>Power System Operation: AC and DC Power Flow Model</title>
  <dc:creator>Wangkun Xu</dc:creator>
  <link>https://xuwkk.github.io/blog/posts/learning/power_system/power_system_operation.html</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>This post is the first one in the series of power system operation. In this post, we will introduce the basic modelling method of power system operations. I feel this can be a great summary of the knowlege. Meanwhile, this series of post can be a reference to the open-source package <a href="https://github.com/xuwkk/power_system_operation">power system operation</a> I am developing.</p>
<p>I mainly follow the modelling method from MATPOWER. A reference can be found by the <a href="https://www.matpower.org/docs/manual.pdf">MATPOWER Manual</a>.</p>
<p>Matrix form of the power flow model will be followed.</p>
</section>
<section id="ac-model" class="level2">
<h2 class="anchored" data-anchor-id="ac-model">AC Model</h2>
<section id="branch-model" class="level3">
<h3 class="anchored" data-anchor-id="branch-model">Branch Model</h3>
<p>The branch model is shown in the following figure.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://xuwkk.github.io/blog/posts/learning/power_system/img/2024-06-10-13-26-13.png" class="img-fluid figure-img"></p>
<figcaption>Branch Model</figcaption>
</figure>
</div>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?z_s%20=%20r_s%20+%20jx_s">: series impedance.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Ctau,%20%5Ctheta_%7B%5Ctext%7Bshift%7D%7D">: transformer tap ratio magnitude and phase angle (in radians). The transformer is located at the from bus on a branch. If there is no transformer, <img src="https://latex.codecogs.com/png.latex?%5Ctau%20=%201"> and <img src="https://latex.codecogs.com/png.latex?%5Ctheta_%7B%5Ctext%7Bshift%7D%7D%20=%200">.</li>
<li><img src="https://latex.codecogs.com/png.latex?b_s">: total charging susceptance.</li>
</ul>
<p>For a single branch, <img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5B%5Cbegin%7Barray%7D%7Bl%7D%0Ai_f%20%5C%5C%0Ai_t%0A%5Cend%7Barray%7D%5Cright%5D=Y_%7Bb%20r%7D%5Cleft%5B%5Cbegin%7Barray%7D%7Bl%7D%0Av_f%20%5C%5C%0Av_t%0A%5Cend%7Barray%7D%5Cright%5D%0A"> where the branch admittance matrix <img src="https://latex.codecogs.com/png.latex?Y_%7Bb%20r%7D"> can be found by KCL law: <span id="eq-ybr_ac"><img src="https://latex.codecogs.com/png.latex?%0AY_%7Bb%20r%7D=%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%0A%5Cleft(y_s+j%20%5Cfrac%7Bb_c%7D%7B2%7D%5Cright)%20%5Cfrac%7B1%7D%7B%5Ctau%5E2%7D%20&amp;%20-y_s%20%5Cfrac%7B1%7D%7B%5Ctau%20e%5E%7B-j%20%5Ctheta_%7B%5Ctext%20%7Bshif%20%7D%7D%7D%7D%20%5C%5C%0A-y_s%20%5Cfrac%7B1%7D%7B%5Ctau%20e%5E%7Bj%20%5Ctheta_%7B%5Ctext%20%7Bshift%20%7D%7D%7D%7D%20&amp;%20y_s+j%20%5Cfrac%7Bb_c%7D%7B2%7D%0A%5Cend%7Barray%7D%5Cright%5D%0A%5Ctag%7B1%7D"></span></p>
<p>Note that <img src="https://latex.codecogs.com/png.latex?Y_%7Bb%20r%7D"> is in general not symmetric unless <img src="https://latex.codecogs.com/png.latex?%5Ctheta_%7B%5Ctext%7Bshift%7D%7D%20=%200">.</p>
<p>Let’s denote the four elements of branch <img src="https://latex.codecogs.com/png.latex?i"> as <img src="https://latex.codecogs.com/png.latex?%0AY_%7Bb%20r%7D%5Ei=%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%0Ay_%7Bf%20f%7D%5Ei%20&amp;%20y_%7Bf%20t%7D%5Ei%20%5C%5C%0Ay_%7Bt%20f%7D%5Ei%20&amp;%20y_%7Bt%20t%7D%5Ei%0A%5Cend%7Barray%7D%5Cright%5D%0A"></p>
<p>The branch number of the four elements can be summarized into vectors <img src="https://latex.codecogs.com/png.latex?Y_%7Bff%7D">, <img src="https://latex.codecogs.com/png.latex?Y_%7Bft%7D">, <img src="https://latex.codecogs.com/png.latex?Y_%7Btf%7D">, <img src="https://latex.codecogs.com/png.latex?Y_%7Btt%7D">.</p>
<p>Meanwhile, the from-side and to-side incidence matrix <img src="https://latex.codecogs.com/png.latex?C_f"> and <img src="https://latex.codecogs.com/png.latex?C_t"> are deifned such that the <img src="https://latex.codecogs.com/png.latex?(i,j)"> entry of <img src="https://latex.codecogs.com/png.latex?C_f"> and the <img src="https://latex.codecogs.com/png.latex?(i,k)"> entry of <img src="https://latex.codecogs.com/png.latex?C_t"> are 1 if branch <img src="https://latex.codecogs.com/png.latex?i"> is connected from bus <img src="https://latex.codecogs.com/png.latex?j"> to <img src="https://latex.codecogs.com/png.latex?k">, respectively, and 0 otherwise. The branch-to-bus <strong>incidence matrix</strong> <img src="https://latex.codecogs.com/png.latex?A%20=%20C_f%20-%20C_t">.</p>
</section>
<section id="generator-model" class="level3">
<h3 class="anchored" data-anchor-id="generator-model">Generator Model</h3>
<p>The generator complex power injection can be written as <img src="https://latex.codecogs.com/png.latex?%0AS_g%20=%20P_g%20+%20jQ_g%0A"></p>
<p>The generator incidence matrix <img src="https://latex.codecogs.com/png.latex?C_g"> is defined such that the <img src="https://latex.codecogs.com/png.latex?(i,j)"> entry of <img src="https://latex.codecogs.com/png.latex?C_g"> is 1 if generator <img src="https://latex.codecogs.com/png.latex?j"> is connected to bus <img src="https://latex.codecogs.com/png.latex?i">, and 0 otherwise. Therefore, its contribution to bus (nodal) power injection is <img src="https://latex.codecogs.com/png.latex?%0AS_%7Bg,%5Ctext%7Bbus%7D%7D%20=%20C_g%20S_g%0A"></p>
<p>Other type of generators, such as solar or wind renewables can be defined in the same way. The constant renewable power injection can also be viewed as negative load.</p>
</section>
<section id="load-model" class="level3">
<h3 class="anchored" data-anchor-id="load-model">Load Model</h3>
<p>A constant power load is modeled as active and reactive power consumption at each bus. The load complex power injection can be written as <img src="https://latex.codecogs.com/png.latex?%0AS_d%20=%20P_d%20+%20jQ_d%0A"></p>
</section>
<section id="shunt-elements" class="level3">
<h3 class="anchored" data-anchor-id="shunt-elements">Shunt Elements</h3>
<p>A shunt-connected element, such as a capacitor or an inductor, is modeled as a fixed impedance to ground at a <strong>bus</strong>, whose admittance is <img src="https://latex.codecogs.com/png.latex?%0AY_%7Bsh%7D%20=%20G_%7Bsh%7D%20+%20jB_%7Bsh%7D%0A"></p>
</section>
<section id="network-equations" class="level3">
<h3 class="anchored" data-anchor-id="network-equations">Network Equations</h3>
<p>Let <img src="https://latex.codecogs.com/png.latex?V"> be the bus voltage and <img src="https://latex.codecogs.com/png.latex?I_%7B%5Ctext%7Bbus%7D%7D"> be the bus current injection. <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AI_%7B%5Ctext%7Bbus%7D%7D%20=%20Y_%7B%5Ctext%7Bbus%7D%7D%20V%20%5C%5C%0AI_f%20=%20Y_%7Bf%7D%20V%20%5C%5C%0AI_t%20=%20Y_%7Bt%7D%20V%0A%5Cend%7Baligned%7D%0A"> with the system admittance matrices defined as <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AY_f%20&amp;%20=%5Cleft%5BY_%7Bf%20f%7D%5Cright%5D%20C_f+%5Cleft%5BY_%7Bf%20t%7D%5Cright%5D%20C_t,%20%5C%5C%0AY_t%20&amp;%20=%5Cleft%5BY_%7Bt%20t%7D%5Cright%5D%20C_f+%5Cleft%5BY_%7Bt%20t%7D%5Cright%5D%20C_t,%20%5C%5C%0AY_%7B%5Ctext%20%7Bbus%20%7D%7D%20&amp;%20=C_f%5E%7B%5Ctop%7D%20Y_f+C_t%5E%7B%5Ctop%7D%20Y_t+%5Cleft%5BY_%7Bs%20h%7D%5Cright%5D%20.%0A%5Cend%7Baligned%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?%5B%5Ccdot%5D"> denotes the diagonal matrix of the vector.</p>
<p>In detail, we have the bus current injection as <img src="https://latex.codecogs.com/png.latex?%0AI_%7B%5Ctext%7Bbus%7D%7D%20=%20C_f%5ET%5BY_%7Bff%7D%5DC_f%20V%20+%20C_f%5ET%5BY_%7Bft%7D%5DC_t%20V%20+%20C_t%5ET%5BY_%7Btf%7D%5DC_f%20V%20+%20C_t%5ET%5BY_%7Btt%7D%5DC_t%20V%20+%20%5BY_%7Bsh%7D%5DV%0A"></p>
<p>To understand this, using the first term as an example, <img src="https://latex.codecogs.com/png.latex?C_fV"> is the voltage at the from bus of the branch, <img src="https://latex.codecogs.com/png.latex?%5BY_%7Bff%7D%5DC_fV"> is the current flowing at the from bus of the branch, e.g.&nbsp;the <img src="https://latex.codecogs.com/png.latex?I_f">. <img src="https://latex.codecogs.com/png.latex?C_f%5ET"> is the transpose of <img src="https://latex.codecogs.com/png.latex?C_f">, which is the incidence matrix of the branches connected to the bus.</p>
<p>Then the complex power injection and flows can be written as (the power flow equation) <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AS_%7B%5Ctext%20%7Bbus%20%7D%7D(V)%20&amp;%20=%5BV%5D%20I_%7B%5Ctext%20%7Bbus%20%7D%7D%5E*=%5BV%5D%20Y_%7B%5Ctext%20%7Bbus%20%7D%7D%5E*%20V%5E*,%20%5C%5C%0AS_f(V)%20&amp;%20=%5Cleft%5BC_f%20V%5Cright%5D%20I_f%5E*=%5Cleft%5BC_f%20V%5Cright%5D%20Y_f%5E*%20V%5E*,%20%5C%5C%0AS_t(V)%20&amp;%20=%5Cleft%5BC_t%20V%5Cright%5D%20I_t%5E*=%5Cleft%5BC_t%20V%5Cright%5D%20Y_t%5E*%20V%5E*%20.%0A%5Cend%7Baligned%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?(%5Ccdot)%5E%5Cstar"> is the element-wise conjugate operator on complex number. Note that <img src="https://latex.codecogs.com/png.latex?(AB)%5E%5Cstar%20=%20A%5E%5Cstar%20B%5E%5Cstar">.</p>
<p>The bus injection can be written as (power injection balance) <img src="https://latex.codecogs.com/png.latex?%0Ag_S%5Cleft(V,%20S_g%5Cright)=S_%7B%5Ctext%20%7Bbus%20%7D%7D(V)+S_d-C_g%20S_g=0%20.%0A"></p>
</section>
</section>
<section id="dc-model" class="level2">
<h2 class="anchored" data-anchor-id="dc-model">DC Model</h2>
<p>The DC model takes three assumption on the AC model:</p>
<ol type="1">
<li>The voltage magnitude is fixed at 1 p.u., e.g., <img src="https://latex.codecogs.com/png.latex?v_i%20=%20e%5E%7Bj%5Ctheta_i%7D">.</li>
<li>The branches are lossless, e.g., <img src="https://latex.codecogs.com/png.latex?y_s%20=%200">. The line charging susceptance <img src="https://latex.codecogs.com/png.latex?b_c"> is also ignored. Therefore, the branch admittance is <img src="https://latex.codecogs.com/png.latex?y_s%20=%20%5Cfrac%7B1%7D%7Bjx_s%7D">.</li>
<li>The voltage angle difference on each branch is small, e.g., <img src="https://latex.codecogs.com/png.latex?%5Csin(%5Ctheta_f%20-%20%5Ctheta_t%20-%20%5Ctheta_%7B%5Ctext%7Bshift%7D%7D)%20%5Capprox%20%5Ctheta_f%20-%20%5Ctheta_t%20-%20%5Ctheta_%7B%5Ctext%7Bshift%7D%7D">.</li>
</ol>
<p>Based on the three assumptions, the branch admittance matrix <img src="https://latex.codecogs.com/png.latex?Y_%7Bb%20r%7D"> Equation&nbsp;1 can be simplified as <img src="https://latex.codecogs.com/png.latex?%0AY_%7Bb%20r%7D%20%5Capprox%20%5Cfrac%7B1%7D%7Bj%20x_s%7D%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%0A%5Cfrac%7B1%7D%7B%5Ctau%5E2%7D%20&amp;%20-%5Cfrac%7B1%7D%7B%5Ctau%20e%5E%7B-j%20%5Ctheta_%7B%5Ctext%20%7Bshift%20%7D%7D%7D%7D%20%5C%5C%0A-%5Cfrac%7B1%7D%7B%5Ctau%20e%5E%7Bj%20%5Ctheta_%7B%5Ctext%20%7Bshift%20%7D%7D%7D%7D%20&amp;%201%0A%5Cend%7Barray%7D%5Cright%5D%0A"></p>
<p>Let <img src="https://latex.codecogs.com/png.latex?b_i%20=%20%5Cfrac%7B1%7D%7Bx_s%5Ei%5Ctau%5Ei%7D"> and <img src="https://latex.codecogs.com/png.latex?B_%7Bff%7D"> be the vector of <img src="https://latex.codecogs.com/png.latex?b_i"> for all branches. Let <img src="https://latex.codecogs.com/png.latex?P_%7Bf,%5Ctext%7Bshift%7D%7D"> be the vector of <img src="https://latex.codecogs.com/png.latex?-%5Ctheta_%7B%5Ctext%7Bshift%7D%7D%5Eib_i">. Then the bus power injection can be written as <img src="https://latex.codecogs.com/png.latex?%0AP_%7B%5Ctext%7Bbus%7D%7D%20=%20B_%7B%5Ctext%7Bbus%7D%7D(%5CTheta)%20+%20P_%7B%5Ctext%7Bbus,shift%7D%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?%0AP_%7B%5Ctext%20%7Bbus,shift%20%7D%7D=%5Cleft(C_f-C_t%5Cright)%5E%7B%5Ctop%7D%20P_%7Bf,%20%5Ctext%20%7B%20shift%20%7D%7D%0A"></p>
<p>The power flow equation can be written as <img src="https://latex.codecogs.com/png.latex?%0AP_f(%5CTheta)=B_f%20%5CTheta+P_%7Bf,%20%5Ctext%20%7B%20shift%20%7D%7D%20=%20-P_t(%5CTheta)%0A"></p>
<p>The DC-model system matrices can be written as <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AB_f%20&amp;%20=%5Cleft%5BB_%7Bf%20f%7D%5Cright%5D%5Cleft(C_f-C_t%5Cright),%20%5C%5C%0AB_%7B%5Ctext%20%7Bbus%20%7D%7D%20&amp;%20=%5Cleft(C_f-C_t%5Cright)%5E%7B%5Ctop%7D%20B_f%20.%0A%5Cend%7Baligned%7D%0A"></p>
<p>The bus power injection balance can be written as <img src="https://latex.codecogs.com/png.latex?%0Ag_P%5Cleft(%5CTheta,%20P_g%5Cright)=B_%7B%5Cmathrm%7Bbus%7D%7D%20%5CTheta+P_%7B%5Cmathrm%7Bbus%7D,%20%5Ctext%20%7B%20shift%20%7D%7D+P_d+G_%7Bs%20h%7D-C_g%20P_g=0%0A"></p>


</section>

 ]]></description>
  <category>Power System</category>
  <guid>https://xuwkk.github.io/blog/posts/learning/power_system/power_system_operation.html</guid>
  <pubDate>Sat, 08 Jun 2024 23:00:00 GMT</pubDate>
</item>
<item>
  <title>Tangent and Adjoint Sensitivity Analysis of Nonlinear Equations</title>
  <dc:creator>Wangkun Xu</dc:creator>
  <link>https://xuwkk.github.io/blog/posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html</link>
  <description><![CDATA[ 





<p>This post extends from the previous post on <a href="../../../posts/learning/autometic_differentiation/adjoint_linear_equation.html">linear system</a> to find the sensitivities of the parameter of nonlinear systems. This post is largely learnt from the <a href="https://www.youtube.com/watch?v=LzKHSEwxDWI&amp;list=PLISXH-iEM4Jk27AmSvISooRRKH4WtlWKP&amp;index=4&amp;t=29s">YouTube: Adjoint Sensitivities of a Non-Linear system of equations | Full Derivation</a> and <a href="https://www.youtube.com/watch?v=s-tJpZE26CM&amp;list=PLISXH-iEM4Jk27AmSvISooRRKH4WtlWKP&amp;index=5">YouTube: Lagrangian Perspective on the Derivation of Adjoint Sensitivities of Nonlinear Systems</a>. Another reference is <a href="https://implicit-layers-tutorial.org/introduction/.">Deep Implicit Layers</a></p>
<section id="settings" class="level2">
<h2 class="anchored" data-anchor-id="settings">Settings</h2>
<p>Consider a nonlinear system of equations <img src="https://latex.codecogs.com/png.latex?%0Af(x,%20%5Ctheta)%20=%200%0A"> where <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathbb%7BR%7D%5EN"> is the state variable and <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20%5Cin%20%5Cmathbb%7BR%7D%5EP"> is the parameter. Nonlinear equation solvers such as Newton’s method can be used to find <img src="https://latex.codecogs.com/png.latex?x"> given <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. Assume there is a scalar loss function <img src="https://latex.codecogs.com/png.latex?J(x,%5Ctheta)"> and our goal is to find the sensitivity or <strong>total</strong> gradient of <img src="https://latex.codecogs.com/png.latex?J"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Ctheta">: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%20J%7D%7Bd%20%5Ctheta%7D">.</p>
<p>The total derivative of <img src="https://latex.codecogs.com/png.latex?J"> is <span id="eq-d_J_d_theta"><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20J%7D%7Bd%20%5Ctheta%7D%20=%20%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20x%7D%20%5Cfrac%7Bd%20x%7D%7Bd%20%5Ctheta%7D%20+%20%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20%5Ctheta%7D%0A%5Ctag%7B1%7D"></span> where <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%20x%7D%7Bd%20%5Ctheta%7D"> is unknown. Do the total derivative of <img src="https://latex.codecogs.com/png.latex?f"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Ctheta">: <span id="eq-d_f_d_theta"><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20f%7D%7Bd%20%5Ctheta%7D%20=%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%20%5Cfrac%7Bd%20x%7D%7Bd%20%5Ctheta%7D%20+%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20%5Ctheta%7D%20=%200%0A%5Ctag%7B2%7D"></span></p>
<p>Therefore, <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%20x%7D%7Bd%20%5Ctheta%7D"> can be solved by the above <strong>linear</strong> equation. In detail, Equation&nbsp;1 can be rewritten as <span id="eq-d_J_d_theta_2"><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20J%7D%7Bd%20%5Ctheta%7D%20=%20-%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20x%7D%20%5Cleft(%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%20%5Cright)%5E%7B-1%7D%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20%5Ctheta%7D%20+%20%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20%5Ctheta%7D%0A%5Ctag%7B3%7D"></span></p>
</section>
<section id="tangent-sensitivity-analysis" class="level2">
<h2 class="anchored" data-anchor-id="tangent-sensitivity-analysis">Tangent Sensitivity Analysis</h2>
<p>In the tangent (forward) method, the term <img src="https://latex.codecogs.com/png.latex?%5Cleft(%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%20%5Cright)%5E%7B-1%7D%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20%5Ctheta%7D"> is computed by solving the <strong>batch</strong> of linear system Equation&nbsp;2 directly. Denote the <img src="https://latex.codecogs.com/png.latex?i">-th column of <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20%5Ctheta%7D"> as <img src="https://latex.codecogs.com/png.latex?g_i">, then <img src="https://latex.codecogs.com/png.latex?P"> linear systems need to be solved: <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%20%5Cfrac%7Bdx%7D%7Bd%5Ctheta_i%7D%20=%20-g_i%0A"></p>
</section>
<section id="adjoint-sensitivity-analysis" class="level2">
<h2 class="anchored" data-anchor-id="adjoint-sensitivity-analysis">Adjoint Sensitivity Analysis</h2>
<p>Equation&nbsp;3 can be solved from left to right by first computing <img src="https://latex.codecogs.com/png.latex?-%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20x%7D%20%5Cleft(%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%20%5Cright)%5E%7B-1%7D"> and then multiplying <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20%5Ctheta%7D">. The adjoint <strong>linear</strong> system is <span id="eq-adjoint_linear_system"><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft(%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%20%5Cright)%5ET%20%5Clambda%20=%20-%5Cleft(%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20x%7D%5Cright)%5ET%0A%5Ctag%7B4%7D"></span> which can be solved by conjugate gradient method or LU deomposition. The Jacobian matrix <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D"> may not need to be solved explicitely but can be found by VJP.</p>
<p>In the adjoint method, there is only <strong>one</strong> <strong>linear</strong> system to solve (note that the original system is nonlinear), regardless of the number of parameters <img src="https://latex.codecogs.com/png.latex?P">.</p>
<section id="alternative-derivation-using-lagrangian" class="level3">
<h3 class="anchored" data-anchor-id="alternative-derivation-using-lagrangian">Alternative Derivation using Lagrangian</h3>
<p>Similar to the <a href="../../../posts/learning/autometic_differentiation/adjoint_linear_equation.html">linear system</a>, we can derive the adjoint sensitivity analysis for nonlinear system from the Lagrangian perspective.</p>
<p>Consider the equality constrained optimization: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmin_%7Bx%7D%20J(x,%20%5Ctheta)%20%5Cquad%20%5Ctext%7Bs.t.%7D%20%5Cquad%20f(x,%20%5Ctheta)%20=%200%0A"></p>
<p>The Lagrangian is <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D(x,%20%5Clambda,%20%5Ctheta)%20=%20J(x,%20%5Ctheta)%20+%20%5Clambda%5ET%20f(x,%20%5Ctheta)%0A"></p>
<p>Take the total derivative of <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Ctheta">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20%5Cmathcal%7BL%7D%7D%7Bd%20%5Ctheta%7D%20=%20%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20%5Ctheta%7D%20+%20%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20x%7D%20%5Cfrac%7Bdx%7D%7Bd%5Ctheta%7D%20+%20%5Clambda%5ET%20%5Cleft(%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20%5Ctheta%7D%20+%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%20%5Cfrac%7Bdx%7D%7Bd%5Ctheta%7D%5Cright)%20=%20%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20%5Ctheta%7D%20+%20%5Clambda%5ET%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20%5Ctheta%7D%20+%20%5Cleft(%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20x%7D%20+%20%5Clambda%5ET%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%5Cright)%20%5Cfrac%7Bdx%7D%7Bd%5Ctheta%7D%0A"></p>
<p>Because the equality constraint is always satisfied (as <img src="https://latex.codecogs.com/png.latex?x"> is solved from <img src="https://latex.codecogs.com/png.latex?f(x,%20%5Ctheta)%20=%200">), we can set the dual variable <img src="https://latex.codecogs.com/png.latex?%5Clambda"> arbitarily. Here, we can choose to make the coefficient of <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bdx%7D%7Bd%5Ctheta%7D"> to be zero so that this complex term never appears in the final expression. <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20x%7D%20+%20%5Clambda%5ET%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%20=%200%0A"> which is the adjoint equation Equation&nbsp;4.</p>
<p>At last, because <img src="https://latex.codecogs.com/png.latex?f(x,%5Ctheta)=0">, we have <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20%5Cmathcal%7BL%7D%7D%7Bd%20%5Ctheta%7D%20=%20%5Cfrac%7Bd%20J%7D%7Bd%20%5Ctheta%7D%20+%20%5Clambda%5ET%5Cfrac%7Bd%20f%7D%7Bd%20%5Ctheta%7D%20=%20%5Cfrac%7Bd%20J%7D%7Bd%5Ctheta%7D%0A"></p>
</section>
<section id="relation-to-linear-system" class="level3">
<h3 class="anchored" data-anchor-id="relation-to-linear-system">Relation to Linear System</h3>
<p>We can rewrite the <a href="../../../posts/learning/autometic_differentiation/adjoint_linear_equation.html">linear system</a> as <img src="https://latex.codecogs.com/png.latex?%0Af(x,%20%5Ctheta)%20=%20b(%5Ctheta)%20-%20A(%5Ctheta)%20x%20=%200%0A"> with <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20%5Ctheta%7D%20=%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20b%7D%5Cfrac%7Bdb%7D%7Bd%5Ctheta%7D%20+%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20A%7D%5Cfrac%7BdA%7D%7Bd%5Ctheta%7D%20=%20%5Cfrac%7Bdb%7D%7Bd%5Ctheta%7D%20-%20%5Cfrac%7BdA%7D%7Bd%5Ctheta%7Dx%20%5Cneq%200%0A"></p>
<p>Therefore, the total derivative can be written as <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bdf%7D%7Bd%5Ctheta%7D%20=%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%20%5Cfrac%7Bdx%7D%7Bd%5Ctheta%7D%20+%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20%5Ctheta%7D%20=%20-A%20%5Cfrac%7Bdx%7D%7Bd%5Ctheta%7D%20+%20%5Cfrac%7Bdb%7D%7Bd%5Ctheta%7D%20-%20%5Cfrac%7BdA%7D%7Bd%5Ctheta%7Dx%0A"> which recover the derivation of the <a href="../../../posts/learning/autometic_differentiation/adjoint_linear_equation.html">linear system</a>.</p>
</section>
<section id="implementation" class="level3">
<h3 class="anchored" data-anchor-id="implementation">Implementation</h3>
<p>An example implementation of batched-version of adjoint sensitivity analysis has been added to <a href="../../../posts/learning/autometic_differentiation/deep_implicit_layers.html">here</a>.</p>


</section>
</section>

 ]]></description>
  <category>Auto Differentiation</category>
  <category>Implicit Function</category>
  <guid>https://xuwkk.github.io/blog/posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html</guid>
  <pubDate>Tue, 04 Jun 2024 23:00:00 GMT</pubDate>
</item>
<item>
  <title>Tangent and Adjoint Sensitivity Analysis of Linear Equations</title>
  <dc:creator>Wangkun Xu</dc:creator>
  <link>https://xuwkk.github.io/blog/posts/learning/autometic_differentiation/adjoint_linear_equation.html</link>
  <description><![CDATA[ 





<p>This post contains my learning note for <a href="https://www.youtube.com/watch?v=vlFN4qMtoH4&amp;list=PLISXH-iEM4Jk27AmSvISooRRKH4WtlWKP&amp;index=1">YouTube: Adjoint Equation of a Linear System of Equations - by implicit derivative</a>.</p>
<p>All credits go to the author of the video.</p>
<section id="settings" class="level2">
<h2 class="anchored" data-anchor-id="settings">Settings</h2>
<p>Consider a linear system of equations <span id="eq-linear"><img src="https://latex.codecogs.com/png.latex?%0AA(%5Ctheta)%20x%20=%20b(%5Ctheta)%0A%5Ctag%7B1%7D"></span> with a loss function <img src="https://latex.codecogs.com/png.latex?J(x)"> . Our goal is to find the <strong>total</strong> derivative <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%20J%7D%7Bd%20%5Ctheta%7D">. This gradient can be useful for:</p>
<ol type="1">
<li>Gradient-based optimization.</li>
<li>Local sensitivity analysis of linear equations.</li>
</ol>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5Cin%5Cmathbb%7BR%7D%5EP"> (this can be the weights of neural network); <img src="https://latex.codecogs.com/png.latex?A%5Cin%5Cmathbb%7BR%7D%5E%7BM%5Ctimes%20N%7D">; <img src="https://latex.codecogs.com/png.latex?x%5Cin%5Cmathbb%7BR%7D%5EN">; <img src="https://latex.codecogs.com/png.latex?b%5Cin%5Cmathbb%7BR%7D%5EM">; <img src="https://latex.codecogs.com/png.latex?J(x;%5Ctheta):%20%5Cmathbb%7BR%7D%5EN%20%5Ctimes%20%5Cmathbb%7BR%7D%5EP%20%5Crightarrow%20%5Cmathbb%7BR%7D">.</p>
<p>Note that <img src="https://latex.codecogs.com/png.latex?x"> is dependent on <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> through <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?b">. The total derivative <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%20J%7D%7Bd%20%5Ctheta%7D"> can be computed using the chain rule: <span id="eq-loss"><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20J%7D%7Bd%20%5Ctheta%7D%20=%20%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20x%7D%20%5Cfrac%7Bd%20x%7D%7Bd%20%5Ctheta%7D%20+%20%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20%5Ctheta%7D%0A%5Ctag%7B2%7D"></span> where we use the Jacobian convension such that <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%20x%7D%7Bd%20%5Ctheta%7D"> is a matrix of size <img src="https://latex.codecogs.com/png.latex?N%5Ctimes%20P"> and it is difficult to compute directly.</p>
</section>
<section id="tangent-sensitivity-analysis" class="level2">
<h2 class="anchored" data-anchor-id="tangent-sensitivity-analysis">Tangent Sensitivity Analysis</h2>
<p>Do total derivative of (Equation&nbsp;1) with respect to <img src="https://latex.codecogs.com/png.latex?%5Ctheta">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%7D%7Bd%5Ctheta%7D%20(A%20x)%20=%20%20%5Cfrac%7Bd%20b%7D%7Bd%5Ctheta%7D%0A"></p>
<p>where the unknown <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%20x%7D%7Bd%20%5Ctheta%7D"> can found by solving the following linear system</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AA%5Ccdot%5Cfrac%7Bd%20x%7D%7Bd%20%5Ctheta%7D%20%20=%20%5Cfrac%7Bdb%7D%7Bd%5Ctheta%7D%20-%20%5Cfrac%7Bd%20A%7D%7Bd%5Ctheta%7D%20x%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%20x%7D%7Bd%20%5Ctheta%7D"> can be solved as <span id="eq-sol"><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20x%7D%7Bd%20%5Ctheta%7D%20=%20A%5E%7B-1%7D%20%5Cleft(%5Cfrac%7Bdb%7D%7Bd%5Ctheta%7D%20-%20%5Cfrac%7Bd%20A%7D%7Bd%5Ctheta%7D%20x%5Cright)%0A%5Ctag%7B3%7D"></span></p>
<p>Note that the dimension <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%20A%7D%7Bd%20%5Ctheta%7D%5Cin%5Cmathbb%7BR%7D%5E%7BN%5Ctimes%20N%5Ctimes%20P%7D">. Therefore the product <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%20A%7D%7Bd%5Ctheta%7D%20x"> is incorrect (but it is ok here).</p>
<p>This is a <strong>batch</strong> of <strong>linear</strong> system we want to solve. Let <img src="https://latex.codecogs.com/png.latex?%5Ctheta_i"> be the <img src="https://latex.codecogs.com/png.latex?i">-th element of <img src="https://latex.codecogs.com/png.latex?%5Ctheta">, <img src="https://latex.codecogs.com/png.latex?%0AA%5Ccdot%5Cfrac%7Bd%20x%7D%7Bd%20%5Ctheta_i%7D%20%20=%20%5Cfrac%7Bdb%7D%7Bd%5Ctheta_i%7D%20-%20%5Cunderbrace%7B%5Cfrac%7Bd%20A%7D%7Bd%5Ctheta_i%7D%7D_%7BN%5Ctimes%20N%7D%20x,%20%5Cquad%20i=1,%5Cdots,P%0A"></p>
<p>We can view <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%20x%7D%7Bd%20%5Ctheta_i%7D"> as the tangent of <img src="https://latex.codecogs.com/png.latex?x">, e.g., <img src="https://latex.codecogs.com/png.latex?%5Cdot%7Bx%7D_i%20=%20%5Cfrac%7Bd%20x%7D%7Bd%20%5Ctheta_i%7D">. Then solving the above system follows the idea of tangent sensitivity analysis (or forward-mode AD). The matrix-vector products can be computed efficiently using the JVP. However, this method is less efficient when <img src="https://latex.codecogs.com/png.latex?P"> is large.</p>
</section>
<section id="adjoint-sensitivity-analysis" class="level2">
<h2 class="anchored" data-anchor-id="adjoint-sensitivity-analysis">Adjoint Sensitivity Analysis</h2>
<p>Plug (Equation&nbsp;3) into (Equation&nbsp;2), we have <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20J%7D%7Bd%20%5Ctheta%7D%20=%20%5Cunderbrace%7B%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20x%7D%20A%5E%7B-1%7D%7D_%7B%5Clambda%5ET:1%5Ctimes%20N%7D%20%5Cleft(%5Cfrac%7Bdb%7D%7Bd%5Ctheta%7D%20-%20%5Cfrac%7Bd%20A%7D%7Bd%5Ctheta%7D%20x%5Cright)%20+%20%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20%5Ctheta%7D%0A"></p>
<p>Now instead of solving the linear system as in the tangent method (which requires solving <img src="https://latex.codecogs.com/png.latex?P"> linear systems), note that the term <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20x%7D%20A%5E%7B-1%7D"> is a vector of size <img src="https://latex.codecogs.com/png.latex?1%5Ctimes%20N"> which can be solved by the following <strong>linear</strong> system <strong>once</strong>: <span id="eq-adjoint"><img src="https://latex.codecogs.com/png.latex?%0AA%5ET%20%5Clambda%20=%20%5Cleft(%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20x%7D%5Cright)%5ET%0A%5Ctag%7B4%7D"></span></p>
<p>Then the gradient <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%20J%7D%7Bd%20%5Ctheta%7D"> can be computed as <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20J%7D%7Bd%20%5Ctheta%7D%20=%20%5Clambda%5ET%20%5Cleft(%5Cfrac%7Bdb%7D%7Bd%5Ctheta%7D%20-%20%5Cfrac%7Bd%20A%7D%7Bd%5Ctheta%7D%20x%5Cright)%20+%20%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20%5Ctheta%7D%0A"></p>
<p>This is the idea of adjoint sensitivity analysis (or reverse-mode AD). It can be considered as assigning the adjoint of <img src="https://latex.codecogs.com/png.latex?x"> as <img src="https://latex.codecogs.com/png.latex?%5Cbar%7Bx%7D%20=%20%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20x%7D">.</p>
<p>The adjoint sensitivity can be solved by solving two <strong>linear</strong> systems: (Equation&nbsp;1) for <img src="https://latex.codecogs.com/png.latex?x"> and (Equation&nbsp;4) for <img src="https://latex.codecogs.com/png.latex?%5Clambda">. This is more efficient when <img src="https://latex.codecogs.com/png.latex?P"> is large (especially when the loss function is a scalar).</p>
<p>Also note that the Jacobians <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%20A%7D%7Bd%5Ctheta%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%20b%7D%7Bd%5Ctheta%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BdA%7D%7Bd%5Ctheta%7D">, and <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20x%7D"> can be computed by automatic differentiation or analytical solution.</p>
<section id="alternative-derivation-using-lagrange-multiplier" class="level3">
<h3 class="anchored" data-anchor-id="alternative-derivation-using-lagrange-multiplier">Alternative Derivation Using Lagrange Multiplier</h3>
<p>This part is based on the <a href="https://www.youtube.com/watch?v=MlHKW7Ja-qs&amp;list=PLISXH-iEM4Jk27AmSvISooRRKH4WtlWKP&amp;index=2">YouTube: Adjoint Sensitivities of a Linear System of Equations - derived using the Lagrangian</a>.</p>
<p>As the sensitivity analysis can be directly used for purturbation analysis in an optimization problem, e.g., to find how a small change on <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> can affect the objective function <img src="https://latex.codecogs.com/png.latex?J(x)">, we can consider the following optimization problem <img src="https://latex.codecogs.com/png.latex?%0A%5Cmin_%7Bx%7D%20J(x,%20%5Ctheta)%20%5Cquad%20%5Ctext%7Bs.t.%7D%20%5Cquad%20A(%5Ctheta)%20x%20=%20b(%5Ctheta)%0A"></p>
<p>The equality constraint can be regarded as the KKT condition of another convex optimization problem. I.e., the original problem is actually a bi-level optimization problem. The Lagrangian of the above problem is <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D(x,%20%5Ctheta,%20%5Clambda)%20=%20J(x(%5Ctheta),%20%5Clambda)%20+%20%5Clambda%5ET%20(b(%5Ctheta)%20-%20A(%5Ctheta)%20x)%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Clambda"> is the Lagrange multiplier. The total derivative wrt <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> is <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%5Cmathcal%7BL%7D%7D%7Bd%5Ctheta%7D%20%20=%20%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20x%7D%20%5Cfrac%7Bd%20x%7D%7Bd%20%5Ctheta%7D%20+%20%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20%5Ctheta%7D%20+%20%5Clambda%5ET%20%5Cleft(%5Cfrac%7Bdb%7D%7Bd%5Ctheta%7D%20-%20%5Cfrac%7Bd%20A%7D%7Bd%5Ctheta%7D%20x%20-%20A(%5Ctheta)%5Cfrac%7Bdx%7D%7Bd%5Ctheta%7D%5Cright)%0A"></p>
<p>Again, the dimension of <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%20A%7D%7Bd%5Ctheta%7D"> is incorrect. The difficult term is <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bdx%7D%7Bd%5Ctheta%7D">. After some arrangement, we have <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%5Cmathcal%7BL%7D%7D%7Bd%5Ctheta%7D%20=%20%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20%5Ctheta%7D%20+%20%5Clambda%5ET%20%5Cleft(%5Cfrac%7Bdb%7D%7Bd%5Ctheta%7D%20-%20%5Cfrac%7Bd%20A%7D%7Bd%5Ctheta%7D%20x%5Cright)%20+%20%5Cunderbrace%7B%5Cleft(%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20x%7D%20-%20%5Clambda%5ET%20A%5Cright)%7D_%7B%5Crightarrow%200%7D%20%5Cfrac%7Bdx%7D%7Bd%5Ctheta%7D%0A"></p>
<p>Note that because <img src="https://latex.codecogs.com/png.latex?x"> is solved as the solution to <img src="https://latex.codecogs.com/png.latex?Ax%20=%20b">, the equality constraint is always satisfied. Therefore, the value of <img src="https://latex.codecogs.com/png.latex?%5Clambda"> can be <strong>arbitrary</strong>. Consequently, we obtain the adjoint system the same to the previous derivation Equation&nbsp;4.</p>
<p>PLugging in <img src="https://latex.codecogs.com/png.latex?%5Clambda"> into the Lagrangian, we have <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20%5Cmathcal%7BL%7D%7D%7Bd%20%5Ctheta%7D%20=%20%5Cfrac%7Bd%20J%7D%7Bd%20%5Ctheta%7D%20=%20%5Clambda%5ET%20%5Cleft(%5Cfrac%7Bdb%7D%7Bd%5Ctheta%7D%20-%20%5Cfrac%7Bd%20A%7D%7Bd%5Ctheta%7D%20x%5Cright)%20+%20%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20%5Ctheta%7D%0A"> where the first equality is due to <img src="https://latex.codecogs.com/png.latex?Ax=b">.</p>


</section>
</section>

 ]]></description>
  <category>Auto Differentiation</category>
  <category>Implicit Function</category>
  <guid>https://xuwkk.github.io/blog/posts/learning/autometic_differentiation/adjoint_linear_equation.html</guid>
  <pubDate>Sun, 02 Jun 2024 23:00:00 GMT</pubDate>
</item>
<item>
  <title>Welcome To My Blog</title>
  <dc:creator>Wangkun Xu</dc:creator>
  <link>https://xuwkk.github.io/blog/posts/news/</link>
  <description><![CDATA[ 





<p>This is the first post of my blog. I will regularly update my learning note and research outcomes at this website.</p>



 ]]></description>
  <category>News</category>
  <guid>https://xuwkk.github.io/blog/posts/news/</guid>
  <pubDate>Thu, 30 May 2024 23:00:00 GMT</pubDate>
</item>
</channel>
</rss>
