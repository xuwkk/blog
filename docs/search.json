[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog! Here I regularly update learning notes and news about my research."
  },
  {
    "objectID": "posts/news/index.html",
    "href": "posts/news/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post of my blog. I will regularly update my learning note and research outcomes at this website."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Content",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nJun 5, 2024\n\n\nTangent and Adjoint Sensitivity Analysis of Nonlinear Equations\n\n\nLearning Note, Auto Differentiation, Sensitivity Analysis, Implicit Function\n\n\n\n\nJun 3, 2024\n\n\nTangent and Adjoint Sensitivity Analysis of Linear Equations\n\n\nLearning Note, Auto Differentiation, Sensitivity Analysis, Implicit Function\n\n\n\n\nMay 31, 2024\n\n\nWelcome To My Blog\n\n\nNews\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_linear_equation.html",
    "href": "posts/learning/autometic_differentiation/adjoint_linear_equation.html",
    "title": "Tangent and Adjoint Sensitivity Analysis of Linear Equations",
    "section": "",
    "text": "This post contains my learning note for YouTube: Adjoint Equation of a Linear System of Equations - by implicit derivative.\nAll credits go to the author of the video."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_linear_equation.html#settings",
    "href": "posts/learning/autometic_differentiation/adjoint_linear_equation.html#settings",
    "title": "Tangent and Adjoint Sensitivity Analysis of Linear Equations",
    "section": "Settings",
    "text": "Settings\nConsider a linear system of equations \\[\nA(\\theta) x = b(\\theta)\n\\tag{1}\\] with a loss function \\(J(x)\\) . Our goal is to find the total derivative \\(\\frac{d J}{d \\theta}\\). This gradient can be useful for:\n\nGradient-based optimization.\nLocal sensitivity analysis of linear equations.\n\nwhere \\(\\theta\\in\\mathbb{R}^P\\) (this can be the weights of neural network); \\(A\\in\\mathbb{R}^{M\\times N}\\); \\(x\\in\\mathbb{R}^N\\); \\(b\\in\\mathbb{R}^M\\); \\(J(x;\\theta): \\mathbb{R}^N \\times \\mathbb{R}^P \\rightarrow \\mathbb{R}\\).\nNote that \\(x\\) is dependent on \\(\\theta\\) through \\(A\\) and \\(b\\). The total derivative \\(\\frac{d J}{d \\theta}\\) can be computed using the chain rule: \\[\n\\frac{d J}{d \\theta} = \\frac{\\partial J}{\\partial x} \\frac{d x}{d \\theta} + \\frac{\\partial J}{\\partial \\theta}\n\\tag{2}\\] where we use the Jacobian convension such that \\(\\frac{d x}{d \\theta}\\) is a matrix of size \\(N\\times P\\) and it is difficult to compute directly."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_linear_equation.html#tangent-sensitivity-analysis",
    "href": "posts/learning/autometic_differentiation/adjoint_linear_equation.html#tangent-sensitivity-analysis",
    "title": "Tangent and Adjoint Sensitivity Analysis of Linear Equations",
    "section": "Tangent Sensitivity Analysis",
    "text": "Tangent Sensitivity Analysis\nDo total derivative of (Equation 1) with respect to \\(\\theta\\): \\[\n\\frac{d}{d\\theta} (A x) =  \\frac{d b}{d\\theta}\n\\]\nwhere the unknown \\(\\frac{d x}{d \\theta}\\) can found by solving the following linear system\n\\[\nA\\cdot\\frac{d x}{d \\theta}  = \\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x\n\\] where \\(\\frac{d x}{d \\theta}\\) can be solved as \\[\n\\frac{d x}{d \\theta} = A^{-1} \\left(\\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x\\right)\n\\tag{3}\\]\nNote that the dimension \\(\\frac{d A}{d \\theta}\\in\\mathbb{R}^{N\\times N\\times P}\\). Therefore the product \\(\\frac{d A}{d\\theta} x\\) is incorrect (but it is ok here).\nThis is a batch of linear system we want to solve. Let \\(\\theta_i\\) be the \\(i\\)-th element of \\(\\theta\\), \\[\nA\\cdot\\frac{d x}{d \\theta_i}  = \\frac{db}{d\\theta_i} - \\underbrace{\\frac{d A}{d\\theta_i}}_{N\\times N} x, \\quad i=1,\\dots,P\n\\]\nWe can view \\(\\frac{d x}{d \\theta_i}\\) as the tangent of \\(x\\), e.g., \\(\\dot{x}_i = \\frac{d x}{d \\theta_i}\\). Then solving the above system follows the idea of tangent sensitivity analysis (or forward-mode AD). The matrix-vector products can be computed efficiently using the JVP. However, this method is less efficient when \\(P\\) is large."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_linear_equation.html#adjoint-sensitivity-analysis",
    "href": "posts/learning/autometic_differentiation/adjoint_linear_equation.html#adjoint-sensitivity-analysis",
    "title": "Tangent and Adjoint Sensitivity Analysis of Linear Equations",
    "section": "Adjoint Sensitivity Analysis",
    "text": "Adjoint Sensitivity Analysis\nPlug (Equation 3) into (Equation 2), we have \\[\n\\frac{d J}{d \\theta} = \\underbrace{\\frac{\\partial J}{\\partial x} A^{-1}}_{\\lambda^T:1\\times N} \\left(\\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x\\right) + \\frac{\\partial J}{\\partial \\theta}\n\\]\nNow instead of solving the linear system as in the tangent method (which requires solving \\(P\\) linear systems), note that the term \\(\\frac{\\partial J}{\\partial x} A^{-1}\\) is a vector of size \\(1\\times N\\) which can be solved by the following linear system once: \\[\nA^T \\lambda = \\left(\\frac{\\partial J}{\\partial x}\\right)^T\n\\tag{4}\\]\nThen the gradient \\(\\frac{d J}{d \\theta}\\) can be computed as \\[\n\\frac{d J}{d \\theta} = \\lambda^T \\left(\\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x\\right) + \\frac{\\partial J}{\\partial \\theta}\n\\]\nThis is the idea of adjoint sensitivity analysis (or reverse-mode AD). It can be considered as assigning the adjoint of \\(x\\) as \\(\\bar{x} = \\frac{\\partial J}{\\partial x}\\).\nThe adjoint sensitivity can be solved by solving two linear systems: (Equation 1) for \\(x\\) and (Equation 4) for \\(\\lambda\\). This is more efficient when \\(P\\) is large (especially when the loss function is a scalar).\nAlso note that the Jacobians \\(\\frac{d A}{d\\theta}\\), \\(\\frac{d b}{d\\theta}\\), \\(\\frac{dA}{d\\theta}\\), and \\(\\frac{\\partial J}{\\partial x}\\) can be computed by automatic differentiation or analytical solution.\n\nAlternative Derivation Using Lagrange Multiplier\nThis part is based on the YouTube: Adjoint Sensitivities of a Linear System of Equations - derived using the Lagrangian.\nAs the sensitivity analysis can be directly used for purturbation analysis in an optimization problem, e.g., to find how a small change on \\(\\theta\\) can affect the objective function \\(J(x)\\), we can consider the following optimization problem \\[\n\\min_{x} J(x, \\theta) \\quad \\text{s.t.} \\quad A(\\theta) x = b(\\theta)\n\\]\nThe equality constraint can be regarded as the KKT condition of another convex optimization problem. I.e., the original problem is actually a bi-level optimization problem. The Lagrangian of the above problem is \\[\n\\mathcal{L}(x, \\theta, \\lambda) = J(x(\\theta), \\lambda) + \\lambda^T (b(\\theta) - A(\\theta) x)\n\\] where \\(\\lambda\\) is the Lagrange multiplier. The total derivative wrt \\(\\theta\\) is \\[\n\\frac{d\\mathcal{L}}{d\\theta}  = \\frac{\\partial J}{\\partial x} \\frac{d x}{d \\theta} + \\frac{\\partial J}{\\partial \\theta} + \\lambda^T \\left(\\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x - A(\\theta)\\frac{dx}{d\\theta}\\right)\n\\]\nAgain, the dimension of \\(\\frac{d A}{d\\theta}\\) is incorrect. The difficult term is \\(\\frac{dx}{d\\theta}\\). After some arrangement, we have \\[\n\\frac{d\\mathcal{L}}{d\\theta} = \\frac{\\partial J}{\\partial \\theta} + \\lambda^T \\left(\\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x\\right) + \\underbrace{\\left(\\frac{\\partial J}{\\partial x} - \\lambda^T A\\right)}_{\\rightarrow 0} \\frac{dx}{d\\theta}\n\\]\nNote that because \\(x\\) is solved as the solution to \\(Ax = b\\), the equality constraint is always satisfied. Therefore, the value of \\(\\lambda\\) can be arbitrary. Consequently, we obtain the adjoint system the same to the previous derivation Equation 4.\nPLugging in \\(\\lambda\\) into the Lagrangian, we have \\[\n\\frac{d \\mathcal{L}}{d \\theta} = \\frac{d J}{d \\theta} = \\lambda^T \\left(\\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x\\right) + \\frac{\\partial J}{\\partial \\theta}\n\\] where the first equality is due to \\(Ax=b\\)."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html",
    "href": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html",
    "title": "Tangent and Adjoint Sensitivity Analysis of Nonlinear Equations",
    "section": "",
    "text": "This post extends from the previous post on linear system to find the sensitivities of the parameter of nonlinear systems. This post is largely learnt from the YouTube: Adjoint Sensitivities of a Non-Linear system of equations | Full Derivation and YouTube: Lagrangian Perspective on the Derivation of Adjoint Sensitivities of Nonlinear Systems."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#settings",
    "href": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#settings",
    "title": "Tangent and Adjoint Sensitivity Analysis of Nonlinear Equations",
    "section": "Settings",
    "text": "Settings\nConsider a nonlinear system of equations \\[\nf(x, \\theta) = 0\n\\] where \\(x \\in \\mathbb{R}^N\\) is the state variable and \\(\\theta \\in \\mathbb{R}^P\\) is the parameter. Nonlinear equation solvers such as Newton’s method can be used to find \\(x\\) given \\(\\theta\\). Assume there is a scalar loss function \\(J(x,\\theta)\\) and our goal is to find the sensitivity or total gradient of \\(J\\) with respect to \\(\\theta\\): \\(\\frac{d J}{d \\theta}\\).\nThe total derivative of \\(J\\) is \\[\n\\frac{d J}{d \\theta} = \\frac{\\partial J}{\\partial x} \\frac{d x}{d \\theta} + \\frac{\\partial J}{\\partial \\theta}\n\\tag{1}\\] where \\(\\frac{d x}{d \\theta}\\) is unknown. Do the total derivative of \\(f\\) with respect to \\(\\theta\\): \\[\n\\frac{d f}{d \\theta} = \\frac{\\partial f}{\\partial x} \\frac{d x}{d \\theta} + \\frac{\\partial f}{\\partial \\theta} = 0\n\\tag{2}\\]\nTherefore, \\(\\frac{d x}{d \\theta}\\) can be solved by the above linear equation. In detail, Equation 1 can be rewritten as \\[\n\\frac{d J}{d \\theta} = -\\frac{\\partial J}{\\partial x} \\left( \\frac{\\partial f}{\\partial x} \\right)^{-1} \\frac{\\partial f}{\\partial \\theta} + \\frac{\\partial J}{\\partial \\theta}\n\\tag{3}\\]"
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#tangent-sensitivity-analysis",
    "href": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#tangent-sensitivity-analysis",
    "title": "Tangent and Adjoint Sensitivity Analysis of Nonlinear Equations",
    "section": "Tangent Sensitivity Analysis",
    "text": "Tangent Sensitivity Analysis\nIn the tangent (forward) method, the term \\(\\left( \\frac{\\partial f}{\\partial x} \\right)^{-1} \\frac{\\partial f}{\\partial \\theta}\\) is computed by solving the batch of linear system Equation 2 directly. Denote the \\(i\\)-th column of \\(\\frac{\\partial f}{\\partial \\theta}\\) as \\(g_i\\), then \\(P\\) linear systems need to be solved: \\[\n\\frac{\\partial f}{\\partial x} \\frac{dx}{d\\theta_i} = -g_i\n\\]"
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#adjoint-sensitivity-analysis",
    "href": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#adjoint-sensitivity-analysis",
    "title": "Tangent and Adjoint Sensitivity Analysis of Nonlinear Equations",
    "section": "Adjoint Sensitivity Analysis",
    "text": "Adjoint Sensitivity Analysis\nEquation 3 can be solved from left to right by first computing \\(-\\frac{\\partial J}{\\partial x} \\left( \\frac{\\partial f}{\\partial x} \\right)^{-1}\\) and then multiplying \\(\\frac{\\partial f}{\\partial \\theta}\\). The adjoint linear system is \\[\n\\left( \\frac{\\partial f}{\\partial x} \\right)^T \\lambda = -\\left(\\frac{\\partial J}{\\partial x}\\right)^T\n\\tag{4}\\] which can be solved by conjugate gradient method or LU deomposition. The Jacobian matrix \\(\\frac{\\partial f}{\\partial x}\\) may not need to be solved explicitely but can be found by VJP.\nIn the adjoint method, there is only one linear system to solve (note that the original system is nonlinear), regardless of the number of parameters \\(P\\).\n\nAlternative Derivation using Lagrangian\nSimilar to the linear system, we can derive the adjoint sensitivity analysis for nonlinear system from the Lagrangian perspective.\nConsider the equality constrained optimization: \\[\n\\min_{x} J(x, \\theta) \\quad \\text{s.t.} \\quad f(x, \\theta) = 0\n\\]\nThe Lagrangian is \\[\n\\mathcal{L}(x, \\lambda, \\theta) = J(x, \\theta) + \\lambda^T f(x, \\theta)\n\\]\nTake the total derivative of \\(\\mathcal{L}\\) with respect to \\(\\theta\\): \\[\n\\frac{d \\mathcal{L}}{d \\theta} = \\frac{\\partial J}{\\partial \\theta} + \\frac{\\partial J}{\\partial x} \\frac{dx}{d\\theta} + \\lambda^T \\left(\\frac{\\partial f}{\\partial \\theta} + \\frac{\\partial f}{\\partial x} \\frac{dx}{d\\theta}\\right) = \\frac{\\partial J}{\\partial \\theta} + \\lambda^T\\frac{\\partial f}{\\partial \\theta} + \\left(\\frac{\\partial J}{\\partial x} + \\lambda^T \\frac{\\partial f}{\\partial x}\\right) \\frac{dx}{d\\theta}\n\\]\nBecause the equality constraint is always satisfied (as \\(x\\) is solved from \\(f(x, \\theta) = 0\\)), we can set the dual variable \\(\\lambda\\) arbitarily. Here, we can choose to make the coefficient of \\(\\frac{dx}{d\\theta}\\) to be zero so that this complex term never appears in the final expression. \\[\n\\frac{\\partial J}{\\partial x} + \\lambda^T \\frac{\\partial f}{\\partial x} = 0\n\\] which is the adjoint equation Equation 4.\nAt last, because \\(f(x,\\theta)=0\\), we have \\[\n\\frac{d \\mathcal{L}}{d \\theta} = \\frac{\\partial J}{\\partial \\theta} + \\lambda^T\\frac{\\partial f}{\\partial \\theta} = \\frac{d J}{d\\theta}\n\\]\n\n\nRelation to Linear System\nWe can rewrite the linear system as \\[\nf(x, \\theta) = b(\\theta) - A(\\theta) x = 0\n\\] with \\[\n\\frac{\\partial f}{\\partial \\theta} = \\frac{\\partial f}{\\partial b}\\frac{db}{d\\theta} + \\frac{\\partial f}{\\partial A}\\frac{dA}{d\\theta} = \\frac{db}{d\\theta} - \\frac{dA}{d\\theta}x \\neq 0\n\\]\nTherefore, the total derivative can be written as \\[\n\\frac{df}{d\\theta} = \\frac{\\partial f}{\\partial x} \\frac{dx}{d\\theta} + \\frac{\\partial f}{\\partial \\theta} = -A \\frac{dx}{d\\theta} + \\frac{db}{d\\theta} - \\frac{dA}{d\\theta}x\n\\] which recover the derivation of the linear system."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#relation-to-linear-system",
    "href": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#relation-to-linear-system",
    "title": "Tangent and Adjoint Sensitivity Analysis of Nonlinear Equations",
    "section": "Relation to Linear System",
    "text": "Relation to Linear System\nWe can rewrite the linear system as \\[\nf(x, \\theta) = b(\\theta) - A(\\theta) x = 0\n\\] with \\[\n\\frac{\\partial f}{\\partial \\theta} = \\frac{\\partial f}{\\partial b}\\frac{db}{d\\theta} + \\frac{\\partial f}{\\partial A}\\frac{dA}{d\\theta} = \\frac{db}{d\\theta} - \\frac{dA}{d\\theta}x \\neq 0\n\\]\nTherefore, the total derivative can be written as \\[\n\\frac{df}{d\\theta} = \\frac{\\partial f}{\\partial x} \\frac{dx}{d\\theta} + \\frac{\\partial f}{\\partial \\theta} = -A \\frac{dx}{d\\theta} + \\frac{db}{d\\theta} - \\frac{dA}{d\\theta}x\n\\]"
  }
]