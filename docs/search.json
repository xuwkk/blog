[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog! Here I regularly update learning notes and news about my research."
  },
  {
    "objectID": "posts/news/index.html",
    "href": "posts/news/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post of my blog. I will regularly update my learning note and research outcomes at this website."
  },
  {
    "objectID": "posts/learning/power_system/power_system_operation.html",
    "href": "posts/learning/power_system/power_system_operation.html",
    "title": "Power System Operation: AC and DC Power Flow Model",
    "section": "",
    "text": "This post is the first one in the series of power system operation. In this post, we will introduce the basic modelling method of power system operations. I feel this can be a great summary of the knowlege. Meanwhile, this series of post can be a reference to the open-source package power system operation I am developing.\nI mainly follow the modelling method from MATPOWER. A reference can be found by the MATPOWER Manual.\nMatrix form of the power flow model will be followed."
  },
  {
    "objectID": "posts/learning/power_system/power_system_operation.html#introduction",
    "href": "posts/learning/power_system/power_system_operation.html#introduction",
    "title": "Power System Operation: AC and DC Power Flow Model",
    "section": "",
    "text": "This post is the first one in the series of power system operation. In this post, we will introduce the basic modelling method of power system operations. I feel this can be a great summary of the knowlege. Meanwhile, this series of post can be a reference to the open-source package power system operation I am developing.\nI mainly follow the modelling method from MATPOWER. A reference can be found by the MATPOWER Manual.\nMatrix form of the power flow model will be followed."
  },
  {
    "objectID": "posts/learning/power_system/power_system_operation.html#ac-model",
    "href": "posts/learning/power_system/power_system_operation.html#ac-model",
    "title": "Power System Operation: AC and DC Power Flow Model",
    "section": "AC Model",
    "text": "AC Model\n\nBranch Model\nThe branch model is shown in the following figure.\n\n\n\nBranch Model\n\n\n\n\\(z_s = r_s + jx_s\\): series impedance.\n\\(\\tau, \\theta_{\\text{shift}}\\): transformer tap ratio magnitude and phase angle (in radians). The transformer is located at the from bus on a branch. If there is no transformer, \\(\\tau = 1\\) and \\(\\theta_{\\text{shift}} = 0\\).\n\\(b_s\\): total charging susceptance.\n\nFor a single branch, \\[\n\\left[\\begin{array}{l}\ni_f \\\\\ni_t\n\\end{array}\\right]=Y_{b r}\\left[\\begin{array}{l}\nv_f \\\\\nv_t\n\\end{array}\\right]\n\\] where the branch admittance matrix \\(Y_{b r}\\) can be found by KCL law: \\[\nY_{b r}=\\left[\\begin{array}{cc}\n\\left(y_s+j \\frac{b_c}{2}\\right) \\frac{1}{\\tau^2} & -y_s \\frac{1}{\\tau e^{-j \\theta_{\\text {shif }}}} \\\\\n-y_s \\frac{1}{\\tau e^{j \\theta_{\\text {shift }}}} & y_s+j \\frac{b_c}{2}\n\\end{array}\\right]\n\\tag{1}\\]\nNote that \\(Y_{b r}\\) is in general not symmetric unless \\(\\theta_{\\text{shift}} = 0\\).\nLet’s denote the four elements of branch \\(i\\) as \\[\nY_{b r}^i=\\left[\\begin{array}{cc}\ny_{f f}^i & y_{f t}^i \\\\\ny_{t f}^i & y_{t t}^i\n\\end{array}\\right]\n\\]\nThe branch number of the four elements can be summarized into vectors \\(Y_{ff}\\), \\(Y_{ft}\\), \\(Y_{tf}\\), \\(Y_{tt}\\).\nMeanwhile, the from-side and to-side incidence matrix \\(C_f\\) and \\(C_t\\) are deifned such that the \\((i,j)\\) entry of \\(C_f\\) and the \\((i,k)\\) entry of \\(C_t\\) are 1 if branch \\(i\\) is connected from bus \\(j\\) to \\(k\\), respectively, and 0 otherwise. The branch-to-bus incidence matrix \\(A = C_f - C_t\\).\n\n\nGenerator Model\nThe generator complex power injection can be written as \\[\nS_g = P_g + jQ_g\n\\]\nThe generator incidence matrix \\(C_g\\) is defined such that the \\((i,j)\\) entry of \\(C_g\\) is 1 if generator \\(j\\) is connected to bus \\(i\\), and 0 otherwise. Therefore, its contribution to bus (nodal) power injection is \\[\nS_{g,\\text{bus}} = C_g S_g\n\\]\nOther type of generators, such as solar or wind renewables can be defined in the same way. The constant renewable power injection can also be viewed as negative load.\n\n\nLoad Model\nA constant power load is modeled as active and reactive power consumption at each bus. The load complex power injection can be written as \\[\nS_d = P_d + jQ_d\n\\]\n\n\nShunt Elements\nA shunt-connected element, such as a capacitor or an inductor, is modeled as a fixed impedance to ground at a bus, whose admittance is \\[\nY_{sh} = G_{sh} + jB_{sh}\n\\]\n\n\nNetwork Equations\nLet \\(V\\) be the bus voltage and \\(I_{\\text{bus}}\\) be the bus current injection. \\[\n\\begin{aligned}\nI_{\\text{bus}} = Y_{\\text{bus}} V \\\\\nI_f = Y_{f} V \\\\\nI_t = Y_{t} V\n\\end{aligned}\n\\] with the system admittance matrices defined as \\[\n\\begin{aligned}\nY_f & =\\left[Y_{f f}\\right] C_f+\\left[Y_{f t}\\right] C_t, \\\\\nY_t & =\\left[Y_{t t}\\right] C_f+\\left[Y_{t t}\\right] C_t, \\\\\nY_{\\text {bus }} & =C_f^{\\top} Y_f+C_t^{\\top} Y_t+\\left[Y_{s h}\\right] .\n\\end{aligned}\n\\] where \\([\\cdot]\\) denotes the diagonal matrix of the vector.\nIn detail, we have the bus current injection as \\[\nI_{\\text{bus}} = C_f^T[Y_{ff}]C_f V + C_f^T[Y_{ft}]C_t V + C_t^T[Y_{tf}]C_f V + C_t^T[Y_{tt}]C_t V + [Y_{sh}]V\n\\]\nTo understand this, using the first term as an example, \\(C_fV\\) is the voltage at the from bus of the branch, \\([Y_{ff}]C_fV\\) is the current flowing at the from bus of the branch, e.g. the \\(I_f\\). \\(C_f^T\\) is the transpose of \\(C_f\\), which is the incidence matrix of the branches connected to the bus.\nThen the complex power injection and flows can be written as (the power flow equation) \\[\n\\begin{aligned}\nS_{\\text {bus }}(V) & =[V] I_{\\text {bus }}^*=[V] Y_{\\text {bus }}^* V^*, \\\\\nS_f(V) & =\\left[C_f V\\right] I_f^*=\\left[C_f V\\right] Y_f^* V^*, \\\\\nS_t(V) & =\\left[C_t V\\right] I_t^*=\\left[C_t V\\right] Y_t^* V^* .\n\\end{aligned}\n\\] where \\((\\cdot)^\\star\\) is the element-wise conjugate operator on complex number. Note that \\((AB)^\\star = A^\\star B^\\star\\).\nThe bus injection can be written as (power injection balance) \\[\ng_S\\left(V, S_g\\right)=S_{\\text {bus }}(V)+S_d-C_g S_g=0 .\n\\]"
  },
  {
    "objectID": "posts/learning/power_system/power_system_operation.html#dc-model",
    "href": "posts/learning/power_system/power_system_operation.html#dc-model",
    "title": "Power System Operation: AC and DC Power Flow Model",
    "section": "DC Model",
    "text": "DC Model\nThe DC model takes three assumption on the AC model:\n\nThe voltage magnitude is fixed at 1 p.u., e.g., \\(v_i = e^{j\\theta_i}\\).\nThe branches are lossless, e.g., \\(y_s = 0\\). The line charging susceptance \\(b_c\\) is also ignored. Therefore, the branch admittance is \\(y_s = \\frac{1}{jx_s}\\).\nThe voltage angle difference on each branch is small, e.g., \\(\\sin(\\theta_f - \\theta_t - \\theta_{\\text{shift}}) \\approx \\theta_f - \\theta_t - \\theta_{\\text{shift}}\\).\n\nBased on the three assumptions, the branch admittance matrix \\(Y_{b r}\\) Equation 1 can be simplified as \\[\nY_{b r} \\approx \\frac{1}{j x_s}\\left[\\begin{array}{cc}\n\\frac{1}{\\tau^2} & -\\frac{1}{\\tau e^{-j \\theta_{\\text {shift }}}} \\\\\n-\\frac{1}{\\tau e^{j \\theta_{\\text {shift }}}} & 1\n\\end{array}\\right]\n\\]\nLet \\(b_i = \\frac{1}{x_s^i\\tau^i}\\) and \\(B_{ff}\\) be the vector of \\(b_i\\) for all branches. Let \\(P_{f,\\text{shift}}\\) be the vector of \\(-\\theta_{\\text{shift}}^ib_i\\). Then the bus power injection can be written as \\[\nP_{\\text{bus}} = B_{\\text{bus}}(\\Theta) + P_{\\text{bus,shift}}\n\\] where \\[\nP_{\\text {bus,shift }}=\\left(C_f-C_t\\right)^{\\top} P_{f, \\text { shift }}\n\\]\nThe power flow equation can be written as \\[\nP_f(\\Theta)=B_f \\Theta+P_{f, \\text { shift }} = -P_t(\\Theta)\n\\]\nThe DC-model system matrices can be written as \\[\n\\begin{aligned}\nB_f & =\\left[B_{f f}\\right]\\left(C_f-C_t\\right), \\\\\nB_{\\text {bus }} & =\\left(C_f-C_t\\right)^{\\top} B_f .\n\\end{aligned}\n\\]\nThe bus power injection balance can be written as \\[\ng_P\\left(\\Theta, P_g\\right)=B_{\\mathrm{bus}} \\Theta+P_{\\mathrm{bus}, \\text { shift }}+P_d+G_{s h}-C_g P_g=0\n\\]"
  },
  {
    "objectID": "posts/learning/power_system/ncuc.html",
    "href": "posts/learning/power_system/ncuc.html",
    "title": "Unit Commitment Problem",
    "section": "",
    "text": "In the previous post Power System Operation: AC and DC Power Flow Model, we discuss how to model the power system in the steady-state for both AC and DC formats. In this post, unit commitment (UC) problem and econimic dispatch (ED) will be formulated. The UC problem is usually solved in day-ahead manner to decide the on/off conditions and the set-points of the generator. It includes 1). the simplest UC problem without binary variables, 2). the UC problem with binary variables. Both problem considers the network constraints and reserve requirements. The ED problem focuses more on ‘real-time’ balance of the load and generation and meet the physical constraints and safety requirements.\nNote that the UC and ED can be formulated in different ways, including stochastic, robust, and chance-constrained formats. In this post, we will focus on the deterministic formulation.\nThe main reference is Conejo’s lecture notes."
  },
  {
    "objectID": "posts/learning/power_system/ncuc.html#unit-commitment-without-binary-variables",
    "href": "posts/learning/power_system/ncuc.html#unit-commitment-without-binary-variables",
    "title": "Unit Commitment Problem",
    "section": "Unit Commitment Without Binary Variables",
    "text": "Unit Commitment Without Binary Variables\nThe UC without binary varibales can be formulated as linear programming (LP) problem as follows:\n\\[\n\\begin{aligned}\n\\min_{P_g, \\theta, P_{ls}, P_{sc}, P_{wc}} & \\sum_{t=1}^T c_{gv}^TP_g(t) + c_{ls}^TP_{ls}(t) + c_{sc}^TP_{sc}(t) + c_{wc}^TP_{wc}(t) \\\\\n\\text{s.t.} & P_g^{\\text{min}} \\leq P_g(t) \\leq P_g^{\\text{max}} \\\\\n& -R_{\\text{down}} \\leq P_g(t) - P_g(t-1) \\leq R_{\\text{up}} \\\\\n& -P_f^{\\text{max}} \\leq B_f\\theta(t) + P_{f,\\text{shift}} \\leq P_f^{\\text{max}} \\\\\n& C_gP_g(t) + C_s(P_s(t) - P_{sc}(t)) + C_w(P_w(t) - P_{wc}(t)) - C_l(P_l(t) - P_{ls}(t)) \\\\\n&  \\qquad = B_{\\text{bus}}\\theta(t) + P_{\\text{bus,shift}} \\\\\n& \\theta_{\\text{ref}}(t) = \\theta_0, \\quad \\forall t=1,2,\\ldots,T \\\\\n& \\sum P_g^{\\text{max}} \\geq \\sum_{i=1}^TP_g(t) + r(t) \\\\\n& 0 \\leq P_{ls}(t) \\leq P_l(t), \\quad \\forall t=1,2,\\ldots,T \\\\\n& 0 \\leq P_{sc}(t) \\leq P_s(t), \\quad \\forall t=1,2,\\ldots,T \\\\\n& 0 \\leq P_{wc}(t) \\leq P_w(t), \\quad \\forall t=1,2,\\ldots,T \\\\\n\\end{aligned}\n\\]\nThe decision variables include\n\nGenerator output \\(P_g(t)\\)\nLoad shedding \\(P_{ls}(t)\\)\nSolar energy curtailment \\(P_{sc}(t)\\)\nWind energy curtailment \\(P_{wc}(t)\\)\nBus voltage angle \\(\\theta(t)\\)\n\nAll the decision variables are vectors and from \\(t=1\\) to \\(T\\). The objective function is to minimize the total cost, which includes the generator cost, load shedding cost, solar curtailment cost, and wind curtailment cost. The constraints include the generator output limits, ramping limits, line flow limits, power balance, bus voltage angle reference, and reserve requirements. Meanwhile, the parameters include\n\nLoad \\(P_l(t)\\)\nSolar generation \\(P_s(t)\\)\nWind generation \\(P_w(t)\\)\nReserve requirement \\(r(t)\\) which are assumed to be known.\n\nThe initial condition, e.g., \\(P_g(0)\\) should also be given.\nThe definition of the bus and branch admittance matrix \\(B_{\\text{bus}}\\) and \\(B_f\\) are the same as the previous post Power System Operation: AC and DC Power Flow Model."
  },
  {
    "objectID": "posts/learning/power_system/ncuc.html#unit-commitment-with-binary-variables",
    "href": "posts/learning/power_system/ncuc.html#unit-commitment-with-binary-variables",
    "title": "Unit Commitment Problem",
    "section": "Unit Commitment With Binary Variables",
    "text": "Unit Commitment With Binary Variables\nThe UC with binary variables can be formulated as mixed-integer linear programming (MILP) problem as follows. Apart from the decision variables in the previous section, the binary variables \\(u(t)\\) are introduced to decide the on/off status of the generator.\nThe optimization problem can be formulated as \\[\n\\begin{aligned}\n\\min_{u_g, y_g, z_g, P_g, \\theta, P_{ls}, P_{sc}, P_{wc}} & \\quad \\sum_{t=1}^T c_{gf}^Tu_g(t) + c_{gv}^TP_g(t) + c_{gsu}^Ty_g(t) + c_{gsd}^Tz_g(t) + c_{ls}^TP_{ls}(t) + c_{sc}^TP_{sc}(t) + c_{wc}^TP_{wc}(t) \\\\\n\\text{s.t.} & \\quad y_g(t) - z_g(t) = u_g(t) - u_g(t-1) \\\\\n& \\quad y_g(t) + z_g(t) \\leq 1 \\\\\n& \\quad u_g(t)\\circ P_g^{\\text{min}} \\leq P_g(t) \\leq u_g(t)\\circ P_g^{\\text{max}} \\\\\n& \\quad P_g(t) - P_g(t-1) \\leq R_{gu}u_g(t-1) + R_{gsu} y_g(t)\\\\\n& \\quad P_g(t-1) - P_g(t) \\leq R_{du}u_g(t) + R_{gsd} z_g(t)\\\\\n& \\quad -P_f^{\\text{max}} \\leq B_f\\theta(t) + P_{f,\\text{shift}} \\leq P_f^{\\text{max}} \\\\\n& \\quad C_gP_g(t) + C_s(P_s(t) - P_{sc}(t)) + C_w(P_w(t) - P_{wc}(t)) - C_l(P_l(t) - P_{ls}(t)) \\\\\n& \\quad  \\qquad = B_{\\text{bus}}\\theta(t) + P_{\\text{bus,shift}} \\\\\n& \\quad \\theta_{\\text{ref}}(t) = \\theta_0, \\quad \\forall t=1,2,\\ldots,T \\\\\n& \\quad \\sum u_g(t) \\circ P_g^{\\text{max}} \\geq \\sum_{i=1}^TP_g(t) + r(t) \\\\\n& \\quad 0 \\leq P_{ls}(t) \\leq P_l(t), \\quad \\forall t=1,2,\\ldots,T \\\\\n& \\quad 0 \\leq P_{sc}(t) \\leq P_s(t), \\quad \\forall t=1,2,\\ldots,T \\\\\n& \\quad 0 \\leq P_{wc}(t) \\leq P_w(t), \\quad \\forall t=1,2,\\ldots,T \\\\\n& \\quad u_g(t) \\in \\{0,1\\}, \\quad y_g(t) \\in \\{0,1\\}, \\quad z_g(t) \\in \\{0,1\\}\n\\end{aligned}\n\\] where \\(\\circ\\) denotes the element-wise product.\nApart from the decision variables in the previous section, the following binary variables are introduced:\n\nGenerator on/off status \\(u(t)\\) with cost \\(c_{gf}\\)\nGenerator start-up status \\(y(t)\\) with cost \\(c_{gsu}\\)\nGenerator shut-down status \\(z(t)\\) with cost \\(c_{gsd}\\)\n\nThe objective function is to minimize the total cost, which includes the generator fixed (on) cost, generator variable cost, generator start-up cost, generator shut-down cost, load shedding cost, solar curtailment cost, and wind curtailment cost.\nThe first two constraints denote the relationship between the on-off status and the start-up and shut-down status, which can be formulated as the following table:\n\nThe relationship between the on-off status and the start-up and shut-down status\n\n\n\\(u_g(t-1)\\)\n\\(u_g(t)\\)\n\\(y_g(t)\\)\n\\(z_g(t)\\)\nExplain\n\n\n\n\n0\n0\n0\n0\nKeep off\n\n\n0\n1\n1\n0\nStart-up\n\n\n1\n0\n0\n1\nShut-down\n\n\n1\n1\n0\n0\nKeep on\n\n\n\nThe ramp-up constraint is also modified where \\(R_{gu}\\) is the ramp-up limit as the previous section and \\(R_{gsu}\\) is the start-up ramp-up limit. The ramp-up status only has three states\n\nThe relationship between the on-off status and the start-up status\n\n\n\\(u_g(t-1)\\)\n\\(y_g(t)\\)\nExplain\n\n\n\n\n0\n0\nKeep off\n\n\n0\n1\nStart-up\n\n\n1\n0\nShut-down or Keep on\n\n\n\nThe ramp-down constraint is also modified where \\(R_{du}\\) is the ramp-down limit as the previous section and \\(R_{gsd}\\) is the shut-down ramp-down limit.\nSimilarly, the ramp-down constraint is modified where \\(R_{du}\\) is the ramp-down limit as the previous section and \\(R_{gsd}\\) is the shut-down ramp-down limit. The ramp-down status only has three states as well\n\nThe relationship between the on-off status and the shut-down status\n\n\n\\(u_g(t)\\)\n\\(z_g(t)\\)\nExplain\n\n\n\n\n0\n0\nKeep off\n\n\n0\n1\nShut down\n\n\n1\n0\nKeep on or Start up\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe initial conditions \\(u_g(0), y_g(0), z_g(0), P_g(0)\\) need to be given."
  },
  {
    "objectID": "posts/learning/optimization/bender.html",
    "href": "posts/learning/optimization/bender.html",
    "title": "Bender’s Decomposition for (Mixed-Integer) Linear Programming",
    "section": "",
    "text": "The Bender’s decomposition is usually used for solving LP with complicating variables. A complicating variables are those varialbes avoiding the LP to be solved 1. distributedly or from the 2. straight-forward solution.\nConsider the following LP: \\[\n\\begin{aligned}\n\\min_{x,y} & \\quad \\sum_{i=1}^n c_i x_i + \\sum_{j=1}^m d_j y_j \\\\\n\\text{s.t.} & \\quad \\sum_{i=1}^n a_{li} x_i + \\sum_{j=1}^m b_{lj} y_j \\leq b^{(l)}, \\quad l=1,\\ldots,q \\\\\n& \\quad 0 \\leq x_i \\leq x_i^{up}, \\quad i=1,\\ldots,n \\\\\n& \\quad 0 \\leq y_j \\leq y_j^{up}, \\quad j=1,\\ldots,m\n\\end{aligned}\n\\tag{1}\\] where \\(x_i^{up}\\) and \\(y_j^{up}\\) are the upper bounds of \\(x_i\\) and \\(y_j\\), respectively.\nIf \\(x_i\\)s are known, this LP can be easily solved distributedly for each \\(j=1,\\ldots,m\\). The Bender’s decomposition is to efficiently solve subproblems for \\(y_j\\)s with the cost of iterations.\nIntroduce the value function: for given \\(x\\) (as a constant), \\[\n\\begin{aligned}\n\\alpha(x) = \\min_{y} & \\quad \\sum_{j=1}^m d_j y_j \\\\\n\\text{s.t.} & \\quad \\sum_{j=1}^m b_{lj} y_j \\leq b^{(l)} - \\sum_{i=1}^n a_{li} x_i, \\quad l=1,\\ldots,q \\\\\n& \\quad 0 \\leq y_j \\leq y_j^{up}, \\quad j=1,\\ldots,m\n\\end{aligned}\n\\tag{2}\\] which is convex on \\(x\\) (the proof can be found in Boyd’s book). Therefore, \\(\\alpha(x)\\) can be approximated from the lower bound: \\[\n\\alpha(x) \\geq \\alpha(\\tilde{x}) + \\nabla \\alpha(\\tilde{x})^T (x - \\tilde{x})\n\\] where \\(\\tilde{x}\\) is given.\nThe original LP becomes a Bilevel optimization: \\[\n\\begin{aligned}\n\\min_{x} & \\quad \\sum_{i=1}^n c_i x_i + \\alpha(x) \\\\\n\\text{s.t.} & \\quad 0 \\leq x_i \\leq x_i^{up}, \\quad i=1,\\ldots,n\n\\end{aligned}\n\\] where the lower level problem is the value function \\(\\alpha(x)\\).\nThe idea of Bender’s decomposition is now straight-forward. In the original LP Equation 1, if we fix \\(x_i\\)s, the solution is a upper bound of the original LP. In the Bilevel optimization Equation 2, if we approximate \\(\\alpha(x)\\) from the lower bound, the solution is a lower bound. Therefore, we can iteratively update the upper bound and the lower bound until they are close enough.\n\n\n\nThe iteration index is denoted as \\((k)\\). The algorithm is as follows:\nStep 0: Initialize \\(x^{(0)}\\) and \\(\\alpha^{(0)}\\) by solving \\[\n\\begin{aligned}\n\\min_{x} & \\quad \\sum_{i=1}^n c_i x_i + \\alpha \\\\\n\\text{s.t.} & \\quad 0 \\leq x_i \\leq x_i^{up}, \\quad i=1,\\ldots,n \\\\\n& \\quad \\alpha \\geq \\alpha^{down} \\\\\n\\end{aligned}\n\\] where \\(\\alpha^{down}\\) is a given lower bound of \\(\\alpha(x)\\). Note that this optimization has solution at the boundary.\nStep 1: Solve the subproblem (lower level) given by Equation 2: \\[\n\\begin{aligned}\n\\min_{y} & \\quad \\sum_{j=1}^m d_j y_j \\\\\n\\text{s.t.} & \\quad \\sum_{j=1}^m b_{lj} y_j \\leq b^{(l)} - \\sum_{i=1}^n a_{li} x_i, \\quad l=1,\\ldots,q \\\\\n& \\quad 0 \\leq y_j \\leq y_j^{up}, \\quad j=1,\\ldots,m \\\\\n& \\quad x_i = x_i^{(k)} \\quad \\lambda_i, \\quad i=1,\\ldots,n\n\\end{aligned}\n\\tag{3}\\] where \\(\\lambda_i\\) is the dual of the \\(i\\)-th constraint. The sensitivity \\(\\nabla \\alpha(x^{(k)})\\) equals to the dual variables. The solution is denoted as \\(y^{(k)}, \\lambda^{(k)}\\).\nStep 2: Check the convergence. The upper bound is given by the subproblem: \\[\nU^{(k)} = \\sum_{i=1}^n c_i x_i^{(k)} + \\sum_{j=1}^m d_j y_j^{(k)}\n\\]\nWhile the lower bound is given by the master problem: \\[\nL^{(k)} = \\sum_{i=1}^n c_i x_i^{(k)} + \\alpha^{(k)}\n\\]\nThe gap is defined as: \\[\nU^{(k)} - L^{(k)} = \\sum_{j=1}^m d_j y_j^{(k)} - \\alpha^{(k)}\n\\]\nIf the gap is small enough, stop. Otherwise, update \\(k = k + 1\\) and go to Step 3.\nStep 3: Update the master problem: \\[\n\\begin{aligned}\n\\min_{x,\\alpha} & \\quad \\sum_{i=1}^n c_i x_i + \\alpha \\\\\n\\text{s.t.} & \\quad 0 \\leq x_i \\leq x_i^{up}, \\quad i=1,\\ldots,n \\\\\n& \\quad \\alpha \\geq \\sum_{j=1}^m d_j y_j^{(v)} + \\sum_{i=1}^n \\lambda_i^{(v)} (x_i - x_i^{(v)}), \\quad v = 1, \\cdots, k-1 \\\\\n& \\quad \\alpha \\geq \\alpha^{down}\n\\end{aligned}\n\\]\nNote that all the previous lower bounds (bender’s cut) are included. Therefore, the size of the master problem becomes larger. Go to step 1.\n\n\n\nThe subproblem Equation 3 may be infeasible due to the constraints on \\(x_i\\). Therefore, a slack variable \\(s_i \\geq 0\\) is introduced: \\[\n\\begin{aligned}\n\\min_{y,s} & \\quad \\sum_{j=1}^m d_j y_j + M \\sum_{i=1}^n s_i \\\\\n\\text{s.t.} & \\quad \\sum_{j=1}^m b_{lj} y_j \\leq b^{(l)} - \\sum_{i=1}^n a_{li} x_i + s_i, \\quad l=1,\\ldots,q \\\\\n& \\quad 0 \\leq y_j \\leq y_j^{up}, \\quad j=1,\\ldots,m \\\\\n& \\quad s_i \\geq 0, \\quad i=1,\\ldots,n\n\\end{aligned}\n\\] where \\(M\\) is a large number.\nNote that this is equivalent to adding the slack variable to the original LP Equation 1 and group \\(s\\) into the subproblem."
  },
  {
    "objectID": "posts/learning/optimization/bender.html#benders-decomposition-for-lp",
    "href": "posts/learning/optimization/bender.html#benders-decomposition-for-lp",
    "title": "Bender’s Decomposition for (Mixed-Integer) Linear Programming",
    "section": "",
    "text": "The Bender’s decomposition is usually used for solving LP with complicating variables. A complicating variables are those varialbes avoiding the LP to be solved 1. distributedly or from the 2. straight-forward solution.\nConsider the following LP: \\[\n\\begin{aligned}\n\\min_{x,y} & \\quad \\sum_{i=1}^n c_i x_i + \\sum_{j=1}^m d_j y_j \\\\\n\\text{s.t.} & \\quad \\sum_{i=1}^n a_{li} x_i + \\sum_{j=1}^m b_{lj} y_j \\leq b^{(l)}, \\quad l=1,\\ldots,q \\\\\n& \\quad 0 \\leq x_i \\leq x_i^{up}, \\quad i=1,\\ldots,n \\\\\n& \\quad 0 \\leq y_j \\leq y_j^{up}, \\quad j=1,\\ldots,m\n\\end{aligned}\n\\tag{1}\\] where \\(x_i^{up}\\) and \\(y_j^{up}\\) are the upper bounds of \\(x_i\\) and \\(y_j\\), respectively.\nIf \\(x_i\\)s are known, this LP can be easily solved distributedly for each \\(j=1,\\ldots,m\\). The Bender’s decomposition is to efficiently solve subproblems for \\(y_j\\)s with the cost of iterations.\nIntroduce the value function: for given \\(x\\) (as a constant), \\[\n\\begin{aligned}\n\\alpha(x) = \\min_{y} & \\quad \\sum_{j=1}^m d_j y_j \\\\\n\\text{s.t.} & \\quad \\sum_{j=1}^m b_{lj} y_j \\leq b^{(l)} - \\sum_{i=1}^n a_{li} x_i, \\quad l=1,\\ldots,q \\\\\n& \\quad 0 \\leq y_j \\leq y_j^{up}, \\quad j=1,\\ldots,m\n\\end{aligned}\n\\tag{2}\\] which is convex on \\(x\\) (the proof can be found in Boyd’s book). Therefore, \\(\\alpha(x)\\) can be approximated from the lower bound: \\[\n\\alpha(x) \\geq \\alpha(\\tilde{x}) + \\nabla \\alpha(\\tilde{x})^T (x - \\tilde{x})\n\\] where \\(\\tilde{x}\\) is given.\nThe original LP becomes a Bilevel optimization: \\[\n\\begin{aligned}\n\\min_{x} & \\quad \\sum_{i=1}^n c_i x_i + \\alpha(x) \\\\\n\\text{s.t.} & \\quad 0 \\leq x_i \\leq x_i^{up}, \\quad i=1,\\ldots,n\n\\end{aligned}\n\\] where the lower level problem is the value function \\(\\alpha(x)\\).\nThe idea of Bender’s decomposition is now straight-forward. In the original LP Equation 1, if we fix \\(x_i\\)s, the solution is a upper bound of the original LP. In the Bilevel optimization Equation 2, if we approximate \\(\\alpha(x)\\) from the lower bound, the solution is a lower bound. Therefore, we can iteratively update the upper bound and the lower bound until they are close enough.\n\n\n\nThe iteration index is denoted as \\((k)\\). The algorithm is as follows:\nStep 0: Initialize \\(x^{(0)}\\) and \\(\\alpha^{(0)}\\) by solving \\[\n\\begin{aligned}\n\\min_{x} & \\quad \\sum_{i=1}^n c_i x_i + \\alpha \\\\\n\\text{s.t.} & \\quad 0 \\leq x_i \\leq x_i^{up}, \\quad i=1,\\ldots,n \\\\\n& \\quad \\alpha \\geq \\alpha^{down} \\\\\n\\end{aligned}\n\\] where \\(\\alpha^{down}\\) is a given lower bound of \\(\\alpha(x)\\). Note that this optimization has solution at the boundary.\nStep 1: Solve the subproblem (lower level) given by Equation 2: \\[\n\\begin{aligned}\n\\min_{y} & \\quad \\sum_{j=1}^m d_j y_j \\\\\n\\text{s.t.} & \\quad \\sum_{j=1}^m b_{lj} y_j \\leq b^{(l)} - \\sum_{i=1}^n a_{li} x_i, \\quad l=1,\\ldots,q \\\\\n& \\quad 0 \\leq y_j \\leq y_j^{up}, \\quad j=1,\\ldots,m \\\\\n& \\quad x_i = x_i^{(k)} \\quad \\lambda_i, \\quad i=1,\\ldots,n\n\\end{aligned}\n\\tag{3}\\] where \\(\\lambda_i\\) is the dual of the \\(i\\)-th constraint. The sensitivity \\(\\nabla \\alpha(x^{(k)})\\) equals to the dual variables. The solution is denoted as \\(y^{(k)}, \\lambda^{(k)}\\).\nStep 2: Check the convergence. The upper bound is given by the subproblem: \\[\nU^{(k)} = \\sum_{i=1}^n c_i x_i^{(k)} + \\sum_{j=1}^m d_j y_j^{(k)}\n\\]\nWhile the lower bound is given by the master problem: \\[\nL^{(k)} = \\sum_{i=1}^n c_i x_i^{(k)} + \\alpha^{(k)}\n\\]\nThe gap is defined as: \\[\nU^{(k)} - L^{(k)} = \\sum_{j=1}^m d_j y_j^{(k)} - \\alpha^{(k)}\n\\]\nIf the gap is small enough, stop. Otherwise, update \\(k = k + 1\\) and go to Step 3.\nStep 3: Update the master problem: \\[\n\\begin{aligned}\n\\min_{x,\\alpha} & \\quad \\sum_{i=1}^n c_i x_i + \\alpha \\\\\n\\text{s.t.} & \\quad 0 \\leq x_i \\leq x_i^{up}, \\quad i=1,\\ldots,n \\\\\n& \\quad \\alpha \\geq \\sum_{j=1}^m d_j y_j^{(v)} + \\sum_{i=1}^n \\lambda_i^{(v)} (x_i - x_i^{(v)}), \\quad v = 1, \\cdots, k-1 \\\\\n& \\quad \\alpha \\geq \\alpha^{down}\n\\end{aligned}\n\\]\nNote that all the previous lower bounds (bender’s cut) are included. Therefore, the size of the master problem becomes larger. Go to step 1.\n\n\n\nThe subproblem Equation 3 may be infeasible due to the constraints on \\(x_i\\). Therefore, a slack variable \\(s_i \\geq 0\\) is introduced: \\[\n\\begin{aligned}\n\\min_{y,s} & \\quad \\sum_{j=1}^m d_j y_j + M \\sum_{i=1}^n s_i \\\\\n\\text{s.t.} & \\quad \\sum_{j=1}^m b_{lj} y_j \\leq b^{(l)} - \\sum_{i=1}^n a_{li} x_i + s_i, \\quad l=1,\\ldots,q \\\\\n& \\quad 0 \\leq y_j \\leq y_j^{up}, \\quad j=1,\\ldots,m \\\\\n& \\quad s_i \\geq 0, \\quad i=1,\\ldots,n\n\\end{aligned}\n\\] where \\(M\\) is a large number.\nNote that this is equivalent to adding the slack variable to the original LP Equation 1 and group \\(s\\) into the subproblem."
  },
  {
    "objectID": "posts/learning/optimization/bender.html#benders-decomposition-for-milp",
    "href": "posts/learning/optimization/bender.html#benders-decomposition-for-milp",
    "title": "Bender’s Decomposition for (Mixed-Integer) Linear Programming",
    "section": "Bender’s Decomposition for MILP",
    "text": "Bender’s Decomposition for MILP\nThe algorithm for MILP is very similar to LP. If \\(x\\) is a set of integer variables (\\(x_i\\in\\mathbb{N}\\)), then treating \\(x\\) as complicating variable results in a simple MILP master problem (as lower bound, note that the \\(\\alpha\\) is included in the master problem as continuous variable) and a LP subproblem (as upper bound)."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/deep_implicit_layers.html",
    "href": "posts/learning/autometic_differentiation/deep_implicit_layers.html",
    "title": "Deep Implicit Layers: Fixed-Point Iteration",
    "section": "",
    "text": "In the previous posts, I summarize the mathematical background of the adjoint method for linear system and nonlinear system. This post will summarize them in the view of deep implicit layers. The main reference of this post is Deep Implicit Layers.\nThe nonlinear equation \\(g(x,z)=0\\) can be viewed beyond the a simple algebraic equation. Similiar to the sensitivity analysis of the equation, we can design the neural network as an implicit function of the parameter and the solution. The gradinet used for backpropagation can be found by the same idea of sensitivity analysis in the previous posts. This means that we can 1. Encode the implicit layer with physical meaning as part of the neural network. 2. Regard the entire neural network or part of it as a implicit model. For example, ResNet can be viewed as NeuralODE and feedforward neural network can be viewed as deep equilibrium model.\n\nImplicit layers for different types of equations.\n\n\n\n\n\n\nEquation Type\nNeural Network\n\n\n\n\nAlgebraic equation (fixed point iteration)\nDeep equilibrium model\n\n\nOrdinary differential equation\nNeuralODE\n\n\nConvex optimization\nDifferentiable convex layer\n\n\n\nThe benefits of using implicit layers are 1. The solution can be found by off-the-shelf solver, regardless of the the layer itself. E.g., the fixed point iteration can be solved by Newton’s method; the ODE can be solved by the ODE solver such as Euler’s method; the convex optimization can be solved by the convex optimization solver, such as ADMM. And more. 2. Because the solution procedure is separated from the layer, it does not need to be recorded on the computational graph (although the solution procedure can be unrolled on the computatonal graph). This improves the memory efficiency and numetical stability. 3. Because the forward pass of implicit layer requires a solution procedure which is usually an iterative process (thus repeated nonlinearity), the representation power of the implicit layer is stronger than the explicit layer."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/deep_implicit_layers.html#introduction",
    "href": "posts/learning/autometic_differentiation/deep_implicit_layers.html#introduction",
    "title": "Deep Implicit Layers: Fixed-Point Iteration",
    "section": "",
    "text": "In the previous posts, I summarize the mathematical background of the adjoint method for linear system and nonlinear system. This post will summarize them in the view of deep implicit layers. The main reference of this post is Deep Implicit Layers.\nThe nonlinear equation \\(g(x,z)=0\\) can be viewed beyond the a simple algebraic equation. Similiar to the sensitivity analysis of the equation, we can design the neural network as an implicit function of the parameter and the solution. The gradinet used for backpropagation can be found by the same idea of sensitivity analysis in the previous posts. This means that we can 1. Encode the implicit layer with physical meaning as part of the neural network. 2. Regard the entire neural network or part of it as a implicit model. For example, ResNet can be viewed as NeuralODE and feedforward neural network can be viewed as deep equilibrium model.\n\nImplicit layers for different types of equations.\n\n\n\n\n\n\nEquation Type\nNeural Network\n\n\n\n\nAlgebraic equation (fixed point iteration)\nDeep equilibrium model\n\n\nOrdinary differential equation\nNeuralODE\n\n\nConvex optimization\nDifferentiable convex layer\n\n\n\nThe benefits of using implicit layers are 1. The solution can be found by off-the-shelf solver, regardless of the the layer itself. E.g., the fixed point iteration can be solved by Newton’s method; the ODE can be solved by the ODE solver such as Euler’s method; the convex optimization can be solved by the convex optimization solver, such as ADMM. And more. 2. Because the solution procedure is separated from the layer, it does not need to be recorded on the computational graph (although the solution procedure can be unrolled on the computatonal graph). This improves the memory efficiency and numetical stability. 3. Because the forward pass of implicit layer requires a solution procedure which is usually an iterative process (thus repeated nonlinearity), the representation power of the implicit layer is stronger than the explicit layer."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/deep_implicit_layers.html#fixed-point-iteration",
    "href": "posts/learning/autometic_differentiation/deep_implicit_layers.html#fixed-point-iteration",
    "title": "Deep Implicit Layers: Fixed-Point Iteration",
    "section": "Fixed Point Iteration",
    "text": "Fixed Point Iteration\nA fixed point iteration \\[\nz^{\\star}=\\tanh \\left(W z^{\\star}+x\\right)\n\\] can be written as a nonlinear equation \\[\ng(x, z)=z-\\tanh \\left(W z+x\\right)=0\n\\] where \\(W\\in\\mathbb{R}^{n\\times n}\\).\n\nForward Pass\nFor a given \\(x\\), Newton’s method can be used to iteratively solve the equation, \\[\nz:= z - \\left(\\frac{\\partial g}{\\partial z}\\right)^{-1} g(x,z)\n\\]\nThe partial Jacobian of \\(g\\) with repect to \\(z\\) can be found by automatic differentation tool such as torch.func.jacfwd() in general. For the simple case, it can be found analytically as \\[\n\\frac{\\partial g}{\\partial z} = I - \\text{diag}(\\tanh'(Wz+x))W\n\\]\nNote that the forward pass is not recorded on the computational graph.\n\n\nBackward Pass\nWe can use the same technique in nonlinear adjoint method to do the reverse mode differentiation or bachpropagation. To have a quick review, do the total derivative of \\(g\\) with respect to \\(x\\): \\[\n\\frac{d g}{d x} = \\frac{\\partial g}{\\partial x} + \\frac{\\partial g}{\\partial z} \\frac{d z}{d x} = 0\n\\]\nThe Jacobian \\(\\frac{d z}{d x}\\) can be found as \\[\n\\frac{d z}{d x} = -\\left(\\frac{\\partial g}{\\partial z}\\right)^{-1} \\frac{\\partial g}{\\partial x}\n\\]\nNote that the term \\(\\frac{\\partial g}{\\partial z}\\) has already been computed in the forward pass, e.g. we can directly use the Jacobian and its inverse calculated in the last iteration of Newton’s method.\n\n\n\n\n\n\nTip\n\n\n\nThis is a very common observation in various implicit layer application where you can do the reverse-mode differentiation for ‘free’.\n\n\nDirectly solving the Jacobian \\(\\frac{d z}{d x}\\) is not efficient. As in the Tangent method, the number of linear systems that need to solve is the same as the number of the parameters \\(x\\).\nModern deep learning model is trained by reverse-mode differentiation, which is more efficient than the forward-mode differentiation. Let \\(\\ell(\\cdot)\\) be the scalar loss function, the Jacobian with respect to \\(x\\) can be found by \\[\n\\frac{d \\ell}{d x} = \\frac{d \\ell}{d z} \\frac{d z}{d x} = \\frac{d \\ell}{d z} \\left(-\\left(\\frac{\\partial g}{\\partial z}\\right)^{-1} \\frac{\\partial g}{\\partial x}\\right)\n\\]\nAgain we can find the vector Jacobian product of the first two terms as the new adjoint of \\(z\\) denoted as \\(\\dot{z}\\): \\[\n\\left(\\frac{\\partial g}{\\partial z}\\right)^T \\dot{z} = \\left(\\frac{d \\ell}{d z}\\right)^T\n\\tag{1}\\]\nThen we need to re-engage \\(x\\) on the computational graph. This can be done by calculating \\[\nz:= z - g(x,z)\n\\] whose gradient with respect to \\(x\\) is \\(-\\frac{\\partial g}{\\partial x}\\), which is the last term in \\(\\frac{d\\ell}{dx}\\):\n\\[\n\\frac{dz}{dx} = -\\frac{\\partial g}{\\partial x}\n\\]\n(The minus sign depends how the adjoint system is defined.)\nTo sum up, the process of forward and backward pass of fixed-point iteration is\n\nForward pass: Solve the nonlinear equation \\(g(x,z)=0\\) by off-the-shelf solver. This is outside the automatic differentiation tape.\nIn the automatic differentiation tape, re-engage \\(x\\) on the computational graph by \\(z:= z - g(x,z)\\).\nModify the gradient of the above \\(z\\) as the solution to Equation 1, using the register_hook() method in PyTorch."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/deep_implicit_layers.html#pytorch-implementation",
    "href": "posts/learning/autometic_differentiation/deep_implicit_layers.html#pytorch-implementation",
    "title": "Deep Implicit Layers: Fixed-Point Iteration",
    "section": "PyTorch Implementation",
    "text": "PyTorch Implementation\nHere, I implement a simple fixed-point iteration layer in PyTorch and compare it to the previous method in the nonlinear adjoint method.\nimport torch\nfrom torch import nn\n\ndef loss_fn(z):\n    return torch.sum(z**2, axis = -1).mean()\n\nclass FixedPointLayer(torch.nn.Module):\n    def __init__(self, W, tol = 1e-4, max_iter = 1):\n        super(FixedPointLayer, self).__init__()\n        self.W = torch.nn.Parameter(W, requires_grad = True)\n        self.tol = tol\n        self.max_iter = max_iter\n        # implement by vmap\n        self.implicit_model = torch.vmap(self.implicit_model_)\n        self.jac_batched = torch.vmap(torch.func.jacfwd(self.implicit_model_, argnums = 0))\n\n    def implicit_model_(self, z, x):\n        return z - torch.tanh(self.W @ z + x)\n    \n    def newton_step(self, z, x, g):\n        J = self.jac_batched(z, x)\n        z = z - torch.linalg.solve(J, g)\n        return z, J\n\n    def forward(self, x):\n        self.iteration = 0\n        with torch.no_grad():\n            z = torch.tanh(x)\n            while self.iteration &lt; self.max_iter:\n                g = self.implicit_model(z, x)\n                self.err = torch.norm(g)\n\n                if self.err &lt; self.tol:\n                    break\n\n                # newton's method\n                z, J = self.newton_step(z, x, g)\n                self.iteration += 1\n        \n        # re-engage the autograd tape\n        z = z - self.implicit_model(z, x)\n        z.register_hook(lambda grad : torch.linalg.solve(J.transpose(1,2), grad))\n\n        return z\n\ndef implicit_model(W, x, z):\n    # the g function\n    return z - torch.tanh(W @ z + x)\n\n\ndef implicit_model_test(W, x, z):\n\n    if x.dim() == 1:\n        # single sample case\n        print('using the implicit model on one sample')\n        z_ = z.clone().detach()\n        x_ = x.clone().detach()\n        \n        dl_dz = torch.func.grad(loss_fn)(z_)\n        df_dW, df_dz = torch.func.jacfwd(implicit_model, argnums = (0,2))(W, x_, z_)\n        \n        adjoint_variable = torch.linalg.solve(df_dz.T, -dl_dz)\n        \n        dl_dW = torch.einsum('i,ikl-&gt;kl', adjoint_variable, df_dW)\n    \n    else:\n        print('using the implicit model on all samples')\n        z = z.clone().detach()\n        x = x.clone().detach()\n        \n        dl_dz = torch.func.grad(loss_fn)(z)\n\n        jacfwd_batched = torch.vmap(torch.func.jacfwd(implicit_model, argnums = (0,2)), in_dims = (None, 0, 0))\n        df_dW, df_dz = jacfwd_batched(W, x, z)\n\n        adjoint_variable = torch.linalg.solve(df_dz.transpose(1,2), -dl_dz)\n\n        dl_dW = torch.einsum('bi,bikl-&gt;kl', adjoint_variable, df_dW)\n    \n    print('dl_dz', dl_dz.shape)\n    print('df_dW', df_dW.shape)\n    print('df_dz', df_dz.shape)\n    print('adjoint_variable', adjoint_variable.shape)\n    print('dl_dW', dl_dW)\n\n# maint function\ntorch.random.manual_seed(0)\n\nbatch_size = 10\nn = 5\nW = torch.randn(n,n).double() * 0.5\nx = torch.randn(batch_size,n, requires_grad=True).double()\n\nprint('using the model')\nmodel = FixedPointLayer(W, tol=1e-10, max_iter = 50).double()\n\n# check with the numerical gradient\ntorch.autograd.gradcheck(model, x, check_undefined_grad=False, raise_exception=True)\n\nz = model(x)\nloss = loss_fn(z)\nloss.backward()\nprint(model.W.grad)\n\n# implicit model method\nimplicit_model_test(W, x[0], z[0])\nimplicit_model_test(W, x, z)\nIn the above example, torch.vmap() is used for multi-batch implementation of the implicit function and the Jacobian. The torch.func.jacfwd() is used for the Jacobian calculation. Note that the Jacobian \\(\\frac{\\partial g}{\\partial z}\\) is found by automatic differentiation, instead of analytical computation. The torch.linalg.solve() is used for the linear system solution. The torch.einsum() is used for the matrix multiplication.\nIn detail, in\njacfwd_batched = torch.vmap(torch.func.jacfwd(implicit_model, argnums = (0,2)), in_dims = (None, 0, 0)\nThe torch.func.jacfwd(implicit_model, argnums = (0,2)) is to find the Jacobian of the implicit model with respect to \\(W\\) and \\(z\\). The torch.vmap() is to apply the torch.func.jacfwd() to each batch of the input. The in_dims = (None, 0, 0) is to specify that only z and x are batched while W is not."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html",
    "href": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html",
    "title": "Tangent and Adjoint Sensitivity Analysis of Nonlinear Equations",
    "section": "",
    "text": "This post extends from the previous post on linear system to find the sensitivities of the parameter of nonlinear systems. This post is largely learnt from the YouTube: Adjoint Sensitivities of a Non-Linear system of equations | Full Derivation and YouTube: Lagrangian Perspective on the Derivation of Adjoint Sensitivities of Nonlinear Systems. Another reference is Deep Implicit Layers"
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#settings",
    "href": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#settings",
    "title": "Tangent and Adjoint Sensitivity Analysis of Nonlinear Equations",
    "section": "Settings",
    "text": "Settings\nConsider a nonlinear system of equations \\[\nf(x, \\theta) = 0\n\\] where \\(x \\in \\mathbb{R}^N\\) is the state variable and \\(\\theta \\in \\mathbb{R}^P\\) is the parameter. Nonlinear equation solvers such as Newton’s method can be used to find \\(x\\) given \\(\\theta\\). Assume there is a scalar loss function \\(J(x,\\theta)\\) and our goal is to find the sensitivity or total gradient of \\(J\\) with respect to \\(\\theta\\): \\(\\frac{d J}{d \\theta}\\).\nThe total derivative of \\(J\\) is \\[\n\\frac{d J}{d \\theta} = \\frac{\\partial J}{\\partial x} \\frac{d x}{d \\theta} + \\frac{\\partial J}{\\partial \\theta}\n\\tag{1}\\] where \\(\\frac{d x}{d \\theta}\\) is unknown. Do the total derivative of \\(f\\) with respect to \\(\\theta\\): \\[\n\\frac{d f}{d \\theta} = \\frac{\\partial f}{\\partial x} \\frac{d x}{d \\theta} + \\frac{\\partial f}{\\partial \\theta} = 0\n\\tag{2}\\]\nTherefore, \\(\\frac{d x}{d \\theta}\\) can be solved by the above linear equation. In detail, Equation 1 can be rewritten as \\[\n\\frac{d J}{d \\theta} = -\\frac{\\partial J}{\\partial x} \\left( \\frac{\\partial f}{\\partial x} \\right)^{-1} \\frac{\\partial f}{\\partial \\theta} + \\frac{\\partial J}{\\partial \\theta}\n\\tag{3}\\]"
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#tangent-sensitivity-analysis",
    "href": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#tangent-sensitivity-analysis",
    "title": "Tangent and Adjoint Sensitivity Analysis of Nonlinear Equations",
    "section": "Tangent Sensitivity Analysis",
    "text": "Tangent Sensitivity Analysis\nIn the tangent (forward) method, the term \\(\\left( \\frac{\\partial f}{\\partial x} \\right)^{-1} \\frac{\\partial f}{\\partial \\theta}\\) is computed by solving the batch of linear system Equation 2 directly. Denote the \\(i\\)-th column of \\(\\frac{\\partial f}{\\partial \\theta}\\) as \\(g_i\\), then \\(P\\) linear systems need to be solved: \\[\n\\frac{\\partial f}{\\partial x} \\frac{dx}{d\\theta_i} = -g_i\n\\]"
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#adjoint-sensitivity-analysis",
    "href": "posts/learning/autometic_differentiation/adjoint_nonlinear_equation.html#adjoint-sensitivity-analysis",
    "title": "Tangent and Adjoint Sensitivity Analysis of Nonlinear Equations",
    "section": "Adjoint Sensitivity Analysis",
    "text": "Adjoint Sensitivity Analysis\nEquation 3 can be solved from left to right by first computing \\(-\\frac{\\partial J}{\\partial x} \\left( \\frac{\\partial f}{\\partial x} \\right)^{-1}\\) and then multiplying \\(\\frac{\\partial f}{\\partial \\theta}\\). The adjoint linear system is \\[\n\\left( \\frac{\\partial f}{\\partial x} \\right)^T \\lambda = -\\left(\\frac{\\partial J}{\\partial x}\\right)^T\n\\tag{4}\\] which can be solved by conjugate gradient method or LU deomposition. The Jacobian matrix \\(\\frac{\\partial f}{\\partial x}\\) may not need to be solved explicitely but can be found by VJP.\nIn the adjoint method, there is only one linear system to solve (note that the original system is nonlinear), regardless of the number of parameters \\(P\\).\n\nAlternative Derivation using Lagrangian\nSimilar to the linear system, we can derive the adjoint sensitivity analysis for nonlinear system from the Lagrangian perspective.\nConsider the equality constrained optimization: \\[\n\\min_{x} J(x, \\theta) \\quad \\text{s.t.} \\quad f(x, \\theta) = 0\n\\]\nThe Lagrangian is \\[\n\\mathcal{L}(x, \\lambda, \\theta) = J(x, \\theta) + \\lambda^T f(x, \\theta)\n\\]\nTake the total derivative of \\(\\mathcal{L}\\) with respect to \\(\\theta\\): \\[\n\\frac{d \\mathcal{L}}{d \\theta} = \\frac{\\partial J}{\\partial \\theta} + \\frac{\\partial J}{\\partial x} \\frac{dx}{d\\theta} + \\lambda^T \\left(\\frac{\\partial f}{\\partial \\theta} + \\frac{\\partial f}{\\partial x} \\frac{dx}{d\\theta}\\right) = \\frac{\\partial J}{\\partial \\theta} + \\lambda^T\\frac{\\partial f}{\\partial \\theta} + \\left(\\frac{\\partial J}{\\partial x} + \\lambda^T \\frac{\\partial f}{\\partial x}\\right) \\frac{dx}{d\\theta}\n\\]\nBecause the equality constraint is always satisfied (as \\(x\\) is solved from \\(f(x, \\theta) = 0\\)), we can set the dual variable \\(\\lambda\\) arbitarily. Here, we can choose to make the coefficient of \\(\\frac{dx}{d\\theta}\\) to be zero so that this complex term never appears in the final expression. \\[\n\\frac{\\partial J}{\\partial x} + \\lambda^T \\frac{\\partial f}{\\partial x} = 0\n\\] which is the adjoint equation Equation 4.\nAt last, because \\(f(x,\\theta)=0\\), we have \\[\n\\frac{d \\mathcal{L}}{d \\theta} = \\frac{d J}{d \\theta} + \\lambda^T\\frac{d f}{d \\theta} = \\frac{d J}{d\\theta}\n\\]\n\n\nRelation to Linear System\nWe can rewrite the linear system as \\[\nf(x, \\theta) = b(\\theta) - A(\\theta) x = 0\n\\] with \\[\n\\frac{\\partial f}{\\partial \\theta} = \\frac{\\partial f}{\\partial b}\\frac{db}{d\\theta} + \\frac{\\partial f}{\\partial A}\\frac{dA}{d\\theta} = \\frac{db}{d\\theta} - \\frac{dA}{d\\theta}x \\neq 0\n\\]\nTherefore, the total derivative can be written as \\[\n\\frac{df}{d\\theta} = \\frac{\\partial f}{\\partial x} \\frac{dx}{d\\theta} + \\frac{\\partial f}{\\partial \\theta} = -A \\frac{dx}{d\\theta} + \\frac{db}{d\\theta} - \\frac{dA}{d\\theta}x\n\\] which recover the derivation of the linear system.\n\n\nImplementation\nAn example implementation of batched-version of adjoint sensitivity analysis has been added to here."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_linear_equation.html",
    "href": "posts/learning/autometic_differentiation/adjoint_linear_equation.html",
    "title": "Tangent and Adjoint Sensitivity Analysis of Linear Equations",
    "section": "",
    "text": "This post contains my learning note for YouTube: Adjoint Equation of a Linear System of Equations - by implicit derivative.\nAll credits go to the author of the video."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_linear_equation.html#settings",
    "href": "posts/learning/autometic_differentiation/adjoint_linear_equation.html#settings",
    "title": "Tangent and Adjoint Sensitivity Analysis of Linear Equations",
    "section": "Settings",
    "text": "Settings\nConsider a linear system of equations \\[\nA(\\theta) x = b(\\theta)\n\\tag{1}\\] with a loss function \\(J(x)\\) . Our goal is to find the total derivative \\(\\frac{d J}{d \\theta}\\). This gradient can be useful for:\n\nGradient-based optimization.\nLocal sensitivity analysis of linear equations.\n\nwhere \\(\\theta\\in\\mathbb{R}^P\\) (this can be the weights of neural network); \\(A\\in\\mathbb{R}^{M\\times N}\\); \\(x\\in\\mathbb{R}^N\\); \\(b\\in\\mathbb{R}^M\\); \\(J(x;\\theta): \\mathbb{R}^N \\times \\mathbb{R}^P \\rightarrow \\mathbb{R}\\).\nNote that \\(x\\) is dependent on \\(\\theta\\) through \\(A\\) and \\(b\\). The total derivative \\(\\frac{d J}{d \\theta}\\) can be computed using the chain rule: \\[\n\\frac{d J}{d \\theta} = \\frac{\\partial J}{\\partial x} \\frac{d x}{d \\theta} + \\frac{\\partial J}{\\partial \\theta}\n\\tag{2}\\] where we use the Jacobian convension such that \\(\\frac{d x}{d \\theta}\\) is a matrix of size \\(N\\times P\\) and it is difficult to compute directly."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_linear_equation.html#tangent-sensitivity-analysis",
    "href": "posts/learning/autometic_differentiation/adjoint_linear_equation.html#tangent-sensitivity-analysis",
    "title": "Tangent and Adjoint Sensitivity Analysis of Linear Equations",
    "section": "Tangent Sensitivity Analysis",
    "text": "Tangent Sensitivity Analysis\nDo total derivative of (Equation 1) with respect to \\(\\theta\\): \\[\n\\frac{d}{d\\theta} (A x) =  \\frac{d b}{d\\theta}\n\\]\nwhere the unknown \\(\\frac{d x}{d \\theta}\\) can found by solving the following linear system\n\\[\nA\\cdot\\frac{d x}{d \\theta}  = \\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x\n\\] where \\(\\frac{d x}{d \\theta}\\) can be solved as \\[\n\\frac{d x}{d \\theta} = A^{-1} \\left(\\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x\\right)\n\\tag{3}\\]\nNote that the dimension \\(\\frac{d A}{d \\theta}\\in\\mathbb{R}^{N\\times N\\times P}\\). Therefore the product \\(\\frac{d A}{d\\theta} x\\) is incorrect (but it is ok here).\nThis is a batch of linear system we want to solve. Let \\(\\theta_i\\) be the \\(i\\)-th element of \\(\\theta\\), \\[\nA\\cdot\\frac{d x}{d \\theta_i}  = \\frac{db}{d\\theta_i} - \\underbrace{\\frac{d A}{d\\theta_i}}_{N\\times N} x, \\quad i=1,\\dots,P\n\\]\nWe can view \\(\\frac{d x}{d \\theta_i}\\) as the tangent of \\(x\\), e.g., \\(\\dot{x}_i = \\frac{d x}{d \\theta_i}\\). Then solving the above system follows the idea of tangent sensitivity analysis (or forward-mode AD). The matrix-vector products can be computed efficiently using the JVP. However, this method is less efficient when \\(P\\) is large."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/adjoint_linear_equation.html#adjoint-sensitivity-analysis",
    "href": "posts/learning/autometic_differentiation/adjoint_linear_equation.html#adjoint-sensitivity-analysis",
    "title": "Tangent and Adjoint Sensitivity Analysis of Linear Equations",
    "section": "Adjoint Sensitivity Analysis",
    "text": "Adjoint Sensitivity Analysis\nPlug (Equation 3) into (Equation 2), we have \\[\n\\frac{d J}{d \\theta} = \\underbrace{\\frac{\\partial J}{\\partial x} A^{-1}}_{\\lambda^T:1\\times N} \\left(\\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x\\right) + \\frac{\\partial J}{\\partial \\theta}\n\\]\nNow instead of solving the linear system as in the tangent method (which requires solving \\(P\\) linear systems), note that the term \\(\\frac{\\partial J}{\\partial x} A^{-1}\\) is a vector of size \\(1\\times N\\) which can be solved by the following linear system once: \\[\nA^T \\lambda = \\left(\\frac{\\partial J}{\\partial x}\\right)^T\n\\tag{4}\\]\nThen the gradient \\(\\frac{d J}{d \\theta}\\) can be computed as \\[\n\\frac{d J}{d \\theta} = \\lambda^T \\left(\\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x\\right) + \\frac{\\partial J}{\\partial \\theta}\n\\]\nThis is the idea of adjoint sensitivity analysis (or reverse-mode AD). It can be considered as assigning the adjoint of \\(x\\) as \\(\\bar{x} = \\frac{\\partial J}{\\partial x}\\).\nThe adjoint sensitivity can be solved by solving two linear systems: (Equation 1) for \\(x\\) and (Equation 4) for \\(\\lambda\\). This is more efficient when \\(P\\) is large (especially when the loss function is a scalar).\nAlso note that the Jacobians \\(\\frac{d A}{d\\theta}\\), \\(\\frac{d b}{d\\theta}\\), \\(\\frac{dA}{d\\theta}\\), and \\(\\frac{\\partial J}{\\partial x}\\) can be computed by automatic differentiation or analytical solution.\n\nAlternative Derivation Using Lagrange Multiplier\nThis part is based on the YouTube: Adjoint Sensitivities of a Linear System of Equations - derived using the Lagrangian.\nAs the sensitivity analysis can be directly used for purturbation analysis in an optimization problem, e.g., to find how a small change on \\(\\theta\\) can affect the objective function \\(J(x)\\), we can consider the following optimization problem \\[\n\\min_{x} J(x, \\theta) \\quad \\text{s.t.} \\quad A(\\theta) x = b(\\theta)\n\\]\nThe equality constraint can be regarded as the KKT condition of another convex optimization problem. I.e., the original problem is actually a bi-level optimization problem. The Lagrangian of the above problem is \\[\n\\mathcal{L}(x, \\theta, \\lambda) = J(x(\\theta), \\lambda) + \\lambda^T (b(\\theta) - A(\\theta) x)\n\\] where \\(\\lambda\\) is the Lagrange multiplier. The total derivative wrt \\(\\theta\\) is \\[\n\\frac{d\\mathcal{L}}{d\\theta}  = \\frac{\\partial J}{\\partial x} \\frac{d x}{d \\theta} + \\frac{\\partial J}{\\partial \\theta} + \\lambda^T \\left(\\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x - A(\\theta)\\frac{dx}{d\\theta}\\right)\n\\]\nAgain, the dimension of \\(\\frac{d A}{d\\theta}\\) is incorrect. The difficult term is \\(\\frac{dx}{d\\theta}\\). After some arrangement, we have \\[\n\\frac{d\\mathcal{L}}{d\\theta} = \\frac{\\partial J}{\\partial \\theta} + \\lambda^T \\left(\\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x\\right) + \\underbrace{\\left(\\frac{\\partial J}{\\partial x} - \\lambda^T A\\right)}_{\\rightarrow 0} \\frac{dx}{d\\theta}\n\\]\nNote that because \\(x\\) is solved as the solution to \\(Ax = b\\), the equality constraint is always satisfied. Therefore, the value of \\(\\lambda\\) can be arbitrary. Consequently, we obtain the adjoint system the same to the previous derivation Equation 4.\nPLugging in \\(\\lambda\\) into the Lagrangian, we have \\[\n\\frac{d \\mathcal{L}}{d \\theta} = \\frac{d J}{d \\theta} = \\lambda^T \\left(\\frac{db}{d\\theta} - \\frac{d A}{d\\theta} x\\right) + \\frac{\\partial J}{\\partial \\theta}\n\\] where the first equality is due to \\(Ax=b\\)."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/forward_mode_ode.html",
    "href": "posts/learning/autometic_differentiation/forward_mode_ode.html",
    "title": "Tangent Sensitivity Analysis of ODEs",
    "section": "",
    "text": "This post summarizes my learning note on the forward-mode (or tangent method) for sensitivity analysis of ordinary differential equations (ODEs). Similar to the previous posts on tangent method for linear and nonlinear equations, it can be extended for finding the sensitivity of the solution of ODEs or the gradient through the NeuralODE with repsect to the initial condition or the parameters. This post is based on the YouTube video: Neural ODEs - Pushforward/Jvp rule.\nConsider the nonlinear ODE: \\[\n\\frac{du}{dt} = f(u, \\theta)\n\\tag{1}\\] which is evaluated at \\(t=T\\) with initial condition \\(u(t=0)=u_0\\) as \\[\nq(\\theta,u_0,T) = u(T)\n\\] where \\(\\theta\\in\\mathbb{R}^P\\) is the parameter; \\(u_0\\in\\mathbb{R}^N\\) is the initial condition; \\(T\\) is the final time; and \\(u(T)\\in\\mathbb{R}^N\\) is the final state. The ODE can be solved by any ODE solver, such as the Euler method or Runge-Kutta method.\nOur task is to forward-propagate the tangent information (a vector) on the inpus \\(\\dot{\\theta}\\in\\mathbb{R}^P, \\dot{u}_0\\in\\mathbb{R}^{N}, T\\in\\mathbb{R}\\) to the output \\(\\dot{u}(T)\\in\\mathbb{R}^N\\) without unrolling the solver and applying forward-mode AD to its operation.\nIn general, the forward-mode AD can be found the Jacobian-vector product on the total derivative of the output: \\[\n\\dot{u}(T) = \\underbrace{\\frac{\\partial q}{\\partial \\theta}\\dot{\\theta}}_{\\dot{u}(T)_\\theta} + \\underbrace{\\frac{\\partial q}{\\partial u_0}\\dot{u}_0}_{\\dot{u}(T)_{u_0}} + \\underbrace{\\frac{\\partial q}{\\partial T}\\dot{T}}_{\\dot{u}(T)_T}\n\\]\nWe can find the tangent information item by item."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/forward_mode_ode.html#tangent-condition-from-parameter-theta",
    "href": "posts/learning/autometic_differentiation/forward_mode_ode.html#tangent-condition-from-parameter-theta",
    "title": "Tangent Sensitivity Analysis of ODEs",
    "section": "Tangent Condition from parameter \\(\\theta\\)",
    "text": "Tangent Condition from parameter \\(\\theta\\)\nFrom \\[\nu(T) = u_0 + \\int_0^T f(u(t), \\theta) dt\n\\] take the total derivative with respect to \\(\\theta\\): \\[\n\\frac{d}{d\\theta}u(T) = \\frac{d}{d\\theta}u_0 + \\int_0^T \\frac{\\partial f}{\\partial u}\\frac{d}{d\\theta}u(t) + \\frac{\\partial f}{\\partial \\theta} dt\n\\]\nMultiply both sides by \\(\\dot{\\theta}\\): \\[\n\\underbrace{\\frac{d}{d\\theta}u(T) \\cdot \\dot{\\theta}}_{\\dot{u}(T)_\\theta} = \\underbrace{\\frac{d}{d\\theta}u_0 \\cdot \\dot{\\theta}}_{=0} + \\int_0^T \\frac{\\partial f}{\\partial u}\\underbrace{\\frac{d}{d\\theta}u(t) \\cdot \\dot{\\theta}}_{\\dot{u}(t)_\\theta} + \\frac{\\partial f}{\\partial \\theta} \\cdot \\dot{\\theta} dt\n\\]\n\n\n\n\n\n\nNote\n\n\n\nIt is assumed that the initial condition \\(u_0\\) is not dependent on \\(\\theta\\). However, this may not be true for example when the event is considered.\n\n\nThe above relationship is the solution of the following ODE on state \\(\\dot{u}(t)_\\theta\\): \\[\n\\begin{aligned}\n\\frac{d}{dt}\\dot{u}(t)_\\theta &= \\frac{\\partial f}{\\partial u}\\dot{u}(t)_\\theta + \\frac{\\partial f}{\\partial \\theta}\\dot{\\theta} \\\\\n\\dot{u}(t=0)_\\theta &= 0\n\\end{aligned}\n\\tag{2}\\]\nBy solving the tangent ODE using any ODE solver, we can find the tangent information \\(\\dot{u}(t)_\\theta\\). Note that this ODE is linear and inhomogeneous, although the original ODE can be nonlinear.\n\nAlternative Derivation\nIn this section, we derive the tangent condition from the perspective of automatic differentiation in deep learning. The sensitivity derived can be used for optimization, local sensitivity analysis, dynamic/control, or neural ODE.\nConsider there is a cost function \\(J(u,\\theta)\\) associated with the state \\(u(t)\\): \\[\nJ(u,\\theta) = \\int_0^Tg(u(t), \\theta)dt\n\\]\nFor instance, we can set a quadratic loss as \\(g(u(t), \\theta) = u(t)^TQu(t)\\). Cost on finite time instance is also possible.\nThe total derivative of the cost function with respect to \\(\\theta\\) is: \\[\n\\frac{dJ}{d\\theta} = \\int_0^T \\frac{d}{d\\theta} g(u,\\theta)dt = \\int_0^T \\frac{\\partial g}{\\partial \\theta} + \\frac{\\partial g}{\\partial u}\\frac{d u}{d\\theta} dt \\in \\mathbb{R}^{1\\times P}\n\\] which is a row vector.\nThe only term that is difficult to solve is \\(\\frac{d u}{d\\theta}\\in\\mathbb{R}^{N\\times P}\\), e.g., \\(\\frac{du}{d\\theta} = [\\frac{du}{d\\theta_1}, \\cdots. \\frac{du}{d\\theta_P}]\\) and each \\(\\frac{du}{d\\theta_i}\\in\\mathbb{R}^N\\).\nThen do total derivative on Equation 1: \\[\n\\frac{d}{d\\theta} \\frac{du}{dt} = \\frac{d}{dt} \\frac{du}{d\\theta} = \\frac{d}{d\\theta} f(u, \\theta) = \\frac{\\partial f}{\\partial u}\\frac{du}{d\\theta} + \\frac{\\partial f}{\\partial \\theta}\n\\]\nTherefore, there is \\(P\\) linear ODEs to solve for \\(\\frac{du}{d\\theta}\\): \\[\n\\begin{aligned}\n\\frac{d}{dt}\\frac{du}{d\\theta_i} &= \\frac{\\partial f}{\\partial u}\\frac{du}{d\\theta_i} + \\frac{\\partial f}{\\partial \\theta_i} \\\\\n\\frac{du}{d\\theta_i}(t=0) &= 0\n\\end{aligned}\n\\]\nOnce we have \\(\\frac{du}{d\\theta}\\), we can find the gradient of the cost function with respect to \\(\\theta\\) by plugging it into the total derivative of the cost function."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/forward_mode_ode.html#tangent-condition-from-initial-condition-u_0",
    "href": "posts/learning/autometic_differentiation/forward_mode_ode.html#tangent-condition-from-initial-condition-u_0",
    "title": "Tangent Sensitivity Analysis of ODEs",
    "section": "Tangent Condition from initial condition \\(u_0\\)",
    "text": "Tangent Condition from initial condition \\(u_0\\)\nFind the total derivative of \\(u(T)\\) with respect to \\(u_0\\) on both sides of Equation 2: \\[\n\\frac{d}{du_0}u(T) = \\frac{\\partial u_0}{\\partial u_0} + \\int_0^T \\frac{\\partial f}{\\partial u}\\frac{d}{du_0}u(t) dt\n\\]\nMultiply both sides by \\(\\dot{u}_0\\): \\[\n\\underbrace{\\frac{d}{du_0}u(T) \\cdot \\dot{u}_0}_{\\dot{u}(T)_{u_0}} = \\underbrace{\\frac{\\partial u_0}{\\partial u_0} \\cdot \\dot{u}_0}_{=\\dot{u}_0} + \\int_0^T \\frac{\\partial f}{\\partial u}\\underbrace{\\frac{d}{du_0}u(t) \\cdot \\dot{u}_0}_{\\dot{u}(t)_{u_0}} dt\n\\] which is again as the solution of the following ODE on state \\(\\dot{u}(t)_{u_0}\\): \\[\n\\begin{aligned}\n\\frac{d}{dt}\\dot{u}(t)_{u_0} &= \\frac{\\partial f}{\\partial u}\\dot{u}(t)_{u_0} \\\\\n\\dot{u}(t=0)_{u_0} &= \\dot{u}_0\n\\end{aligned}\n\\] which is a linear and homogeneous ODE."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/forward_mode_ode.html#tangent-condition-from-final-time-t",
    "href": "posts/learning/autometic_differentiation/forward_mode_ode.html#tangent-condition-from-final-time-t",
    "title": "Tangent Sensitivity Analysis of ODEs",
    "section": "Tangent Condition from final time \\(T\\)",
    "text": "Tangent Condition from final time \\(T\\)\nFind the total derivative of \\(u(T)\\) with respect to \\(T\\) on both sides of Equation 2: \\[\n\\frac{d}{dT}u(T) = \\frac{d}{dT}u_0 + f(u(T), \\theta)\n\\] Note that we have \\(\\frac{d}{dT}\\int_0^T f(u(t), \\theta) dt = f(u(T), \\theta)\\).\nMultiply both sides by \\(\\dot{T}\\): \\[\n\\underbrace{\\frac{d}{dT}u(T) \\cdot \\dot{T}}_{\\dot{u}(T)_T} = \\underbrace{\\frac{d}{dT}u_0 \\cdot \\dot{T}}_{=0} + f(u(T), \\theta) \\cdot \\dot{T}\n\\] which is an algebraic equation (only for the final time)."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/forward_mode_ode.html#computation",
    "href": "posts/learning/autometic_differentiation/forward_mode_ode.html#computation",
    "title": "Tangent Sensitivity Analysis of ODEs",
    "section": "Computation",
    "text": "Computation\nNote that \\(\\dot{\\theta}, \\dot{u}_0, \\dot{T}\\) are known vectors. Meanwhile, the Jacobian \\(\\frac{\\partial f}{\\partial u}\\) and \\(\\frac{\\partial f}{\\partial \\theta}\\) can be found by analytical derivation or automatic differentiation. In most of the cases, the Jacobians do not need to be computed or stored explicitly. Instead, the Jacobian-vector product (JVP) can be computed by the forward-mode AD, e.g. by torch.autograd.forward_ad in PyTorch.\nBecause \\(\\frac{\\partial f}{\\partial u}\\) and \\(\\frac{\\partial f}{\\partial \\theta}\\) are functions of \\(u(t)\\), its value is dependent on the solution of the primal ODE. One method is to store the solution of the primal ODE. However, a more compact method is to simutaneously solve the primal ODE and the tangent ODEs. For example, the augmented ODE system is: \\[\n\\begin{aligned}\n\\frac{du}{dt} &= f(u, \\theta) \\\\\n\\frac{d}{dt}\\dot{u}(t)_{u_0} &= \\frac{\\partial f}{\\partial u}\\dot{u}(t)_{u_0} \\\\\n\\frac{d}{dt}\\dot{u}(t)_\\theta &= \\frac{\\partial f}{\\partial u}\\dot{u}(t)_\\theta + \\frac{\\partial f}{\\partial \\theta}\\dot{\\theta}\n\\end{aligned}\n\\] where the initial conditions are \\(u(t=0)=u_0\\), \\(\\dot{u}(t=0)_{u_0}=\\dot{u}_0\\) and \\(\\dot{u}(t=0)_\\theta=0\\). The ODEs can be solved by any ODE solver, such as the Euler method or Runge-Kutta method.\nThe tangent method can also be used to find the Jacobian of the output with respect to the parameter \\(\\theta\\) by setting the tangent vector \\(\\dot{\\theta}\\) to be the unit vectors. Not suprisingly, the \\(P\\) number of ODEs need to be solved to find the Jacobian \\(\\frac{d u(t)}{d \\theta}\\)."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/forward_mode_ode.html#pytorch-implementation",
    "href": "posts/learning/autometic_differentiation/forward_mode_ode.html#pytorch-implementation",
    "title": "Tangent Sensitivity Analysis of ODEs",
    "section": "Pytorch Implementation",
    "text": "Pytorch Implementation\nAn example code snippet can be found at my-github including a parallel batched implementation."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/differetiable_convex_layer.html",
    "href": "posts/learning/autometic_differentiation/differetiable_convex_layer.html",
    "title": "Deep Implicit Layers: Differentiable Convex Layer",
    "section": "",
    "text": "In this post, I summarize my learning note on another type of implicit model - the differentiable convex layer. This layer has similar formulation, forward, and backward passes as the fixed point layer. This is because its implicit function can be formulated by the KKT condition, no matter which off-the-shelf optimization solver is used. This topic is related to the tangent and adjoint sensitivity analysis of nonlinear system as the KKT condition forms a nonlinear system in general. The main reference is the paper Differentiable Convex Optimization Layers by Amos et al. (2019) and the blog: differentiable optimization."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/differetiable_convex_layer.html#introduction",
    "href": "posts/learning/autometic_differentiation/differetiable_convex_layer.html#introduction",
    "title": "Deep Implicit Layers: Differentiable Convex Layer",
    "section": "",
    "text": "In this post, I summarize my learning note on another type of implicit model - the differentiable convex layer. This layer has similar formulation, forward, and backward passes as the fixed point layer. This is because its implicit function can be formulated by the KKT condition, no matter which off-the-shelf optimization solver is used. This topic is related to the tangent and adjoint sensitivity analysis of nonlinear system as the KKT condition forms a nonlinear system in general. The main reference is the paper Differentiable Convex Optimization Layers by Amos et al. (2019) and the blog: differentiable optimization."
  },
  {
    "objectID": "posts/learning/autometic_differentiation/differetiable_convex_layer.html#formulation-on-convex-optimization-layer",
    "href": "posts/learning/autometic_differentiation/differetiable_convex_layer.html#formulation-on-convex-optimization-layer",
    "title": "Deep Implicit Layers: Differentiable Convex Layer",
    "section": "Formulation on Convex Optimization Layer",
    "text": "Formulation on Convex Optimization Layer\nConsider the following optimization proble: \\[\n\\begin{aligned}\n\\underset{z}{\\operatorname{minimize}} & f(z) \\\\\n\\text { subject to } & g(z) \\leq 0 \\\\\n& h(z)=0,\n\\end{aligned}\n\\]\nTo make the optimization problem convex, we assume that \\(f(z)\\) is convex, \\(g(z)\\) is convex, and \\(h(z)\\) is affine. The KKT condition of this optimization problem is \\[\n\\begin{aligned}\ng\\left(z^{\\star}\\right) & \\leq 0 \\\\\nh\\left(z^{\\star}\\right) & =0 \\\\\n\\lambda^{\\star} & \\geq 0 \\\\\n\\lambda^{\\star} \\circ g\\left(z^{\\star}\\right) & =0 \\\\\n\\nabla f\\left(z^{\\star}\\right)+\\sum_{i=1}^m \\lambda_i^{\\star} \\nabla g_i\\left(z^{\\star}\\right)+\\sum_{i=1}^p \\nu_i^{\\star} \\nabla h_i\\left(z^{\\star}\\right) & =0\n\\end{aligned}\n\\]\nSimilarly to the previous parametric setting, we can define the convex layer as \\[\n\\begin{aligned}\nz^{\\star}(x)=\\underset{z}{\\operatorname{argmin}} & f(z, x) \\\\\n\\text { subject to } & g(z, x) \\leq 0 \\\\\n& h(z, x)=0\n\\end{aligned}\n\\] where \\(z\\) is the decision variable and \\(x\\) is the input of the layer or the parameter of the optimization problem. Apart from \\(z^\\star(x)\\) (as an implicit function of \\(x\\)), we can also include the dual variables, \\[\n(z^\\star, \\lambda^\\star, \\nu^\\star)(x)\n\\]\nThe differentiable convex layer can be viewed as 1) a nonlinear implicit equation \\(G(x,\\lambda,\\nu,x) = 0\\) and 2) a fixed point iteration as iterative optimization algorithm (e.g., interior point, SQP, ADMM, etc) can be used to solve the optimization problem.\nThe equality condition KKT can be written as: \\[\nG(z, \\lambda, \\nu,x)=\\left[\\begin{array}{c}\n\\frac{\\partial}{\\partial z} f(z, x)+\\frac{\\partial}{\\partial z} g(z, x)^T \\lambda+\\frac{\\partial}{\\partial z} h(z, x)^T \\nu \\\\\n\\lambda \\circ g(z,x) \\\\\nh(z,x)\n\\end{array}\\right]\n\\] where \\(\\circ\\) denotes the element-wise product.\nAssume that the optimal primal and dual pair has been found by the forward pass. At the optimal point, we have \\[\nG(z^\\star(x), \\lambda^\\star(x), \\nu^\\star(x), x) = 0\n\\] which is an implicit function of \\(x\\).\n\nGeneral Formulation\nThe KKT condition is exactly the implicit function defined in the deep equilibrium model (DEQ) (We will also show that the interrior-point method, which is basically the Newton’s method, can be used to solve the KKT condition later).\nDenote \\(y(x) = (z^\\star(x), \\lambda^\\star(x), \\nu^\\star(x))\\), then we can directly use the method implemented in nonlinear equation sensitivity analysis to compute the gradient of \\(y(x)\\) with respect to \\(x\\). Do the total derivative with respect to \\(x\\): \\[\n\\frac{d G}{d x} = \\frac{\\partial G}{\\partial y}\\frac{d y}{d x} + \\frac{\\partial G}{\\partial x} = 0\n\\]\nTherefore, the Jacobian of the decision \\(y\\) with respect to the input \\(x\\) is found as \\[\n\\frac{d y}{d x} = -\\left(\\frac{\\partial G}{\\partial y}\\right)^{-1} \\frac{\\partial G}{\\partial x}\n\\]\nIn the backward pass, the adjoint of the optimal point (denote as \\(\\dot{y}\\)) is computed backpropagated by the scalar loss \\(\\ell(\\cdot)\\) as \\(\\dot{y} = \\frac{\\partial \\ell}{\\partial y}\\). Note that when \\(\\ell(\\cdot)\\) does not depend on the primal and dual variables, the associated adjoint is zero.\nConsequently, \\[\n\\frac{d y}{d x} = - \\frac{d \\ell}{d y} \\left(\\frac{\\partial G}{\\partial y}\\right)^{-1} \\frac{\\partial G}{\\partial x}\n\\]\nThe same treatment as in fixed point iteration can be used to compute the above gradient. In detail, the first two items form the adjoint equation of \\(y\\) whose adjoint (or gradient) needs to be modified. \\[\n\\left(\\frac{\\partial G}{\\partial y}\\right)^T \\dot{y} = -\\frac{\\partial \\ell}{\\partial y}\n\\tag{1}\\]\n\n\nDetailed Formulation\nThe partial Jacobian \\(\\frac{\\partial G}{\\partial y}\\) can be analytically computed as \\[\n\\frac{\\partial}{\\partial z, \\lambda, \\nu} G\\left(z^{\\star}, \\lambda^{\\star}, \\nu^{\\star}, x\\right)=\n\\left[\\begin{array}{ccc}\n\\frac{\\partial^2}{\\partial z^2} f\\left(z^{\\star}, x\\right)+\\sum_{i=1}^m \\lambda_i^{\\star} \\frac{\\partial^2}{\\partial z^2} g_i\\left(z^{\\star}, x\\right) & \\frac{\\partial}{\\partial z} g\\left(z^{\\star}, x\\right) & \\frac{\\partial}{\\partial z} h\\left(z^{\\star}, x\\right) \\\\\n\\frac{\\partial}{\\partial z} g\\left(z^{\\star}, x\\right)^T \\operatorname{diag}\\left(\\lambda^{\\star}\\right) & \\operatorname{diag}\\left(g\\left(z^{\\star}, x\\right)\\right) & 0 \\\\\n\\frac{\\partial}{\\partial z} h\\left(z^{\\star}, x\\right)^T & 0 & 0\n\\end{array}\\right]\n\\]\n\n\nQuadratic Programming Formulation\nIn the OptNet paper, the author implemented the differentiable QP layer. In this section, the explicit formulations are derived. We will follow the same notation in this paper.\nConsider the following QP problem: \\[\n\\begin{aligned}\n\\min_z & \\quad \\frac{1}{2} z^T Q z + q^T z \\\\\n\\text{s.t.} & \\quad A z = b \\\\\n& \\quad G z \\leq h\n\\end{aligned}\n\\] where the parameters \\(Q, q, A, b, C, h\\) are functions of the input \\(z_i\\) (the term \\(z_i\\) is used as in the paper this is referred to the output from the previous layer). Our goal is to compute the Jacobian of \\(\\frac{d \\ell}{d z_i}\\).\nThe KKT condition (only the equality part) of the above QP can be written as \\[\n\\begin{aligned}\nQz^\\star + q + A^T \\nu^\\star + G^T \\lambda^\\star &= 0 \\\\\nAz^\\star - b &= 0 \\\\\nD(\\lambda^\\star) (Gz^\\star - h) &= 0\n\\end{aligned}\n\\] where \\(D(\\lambda^\\star)\\) is a diagonal matrix with the diagonal elements being \\(\\lambda^\\star\\).\nCompactly, it is written as \\[\n\\begin{bmatrix}\nQ & G^T & A^T \\\\\nD(\\lambda^\\star)G & D(Gz^\\star - h) & 0 \\\\\nA & 0 & 0\n\\end{bmatrix} \\begin{bmatrix} z^\\star \\\\ \\lambda^\\star \\\\ \\nu^\\star \\end{bmatrix} = \\begin{bmatrix} -q \\\\ 0 \\\\ b \\end{bmatrix}\n\\]\nDifferentiating the KKT equations with respect to the input \\(z_i\\) gives (it is assumed that all parameters are dependent on \\(z_i\\)): \\[\n\\begin{aligned}\n\\frac{dQ}{dz_i}z^\\star + Q\\frac{dz^\\star}{dz_i} + \\frac{dq}{dz_i} + \\frac{dA^T}{dz_i}\\nu^\\star + A^T \\frac{d\\nu^\\star}{dz_i} + \\frac{dG^T}{dz_i}\\lambda^\\star + G^T \\frac{d\\lambda^\\star}{dz_i} &= 0 \\\\\n\\frac{dA}{dz_i}z^\\star + A\\frac{dz^\\star}{dz_i} - \\frac{db}{dz_i} &= 0 \\\\\nD(Gz^\\star - h)\\frac{d\\lambda^\\star}{dz_i} + D(\\lambda^\\star)\\left(\\frac{dG}{dz_i}z^\\star + G\\frac{dz^\\star}{dz_i} - \\frac{dh}{dz_i} \\right) &=0\n\\end{aligned}\n\\] which can be compactly written as matrix form \\[\n\\begin{bmatrix}\nQ & G^T & A^T \\\\\nD(\\lambda^\\star)G & D(Gz^\\star - h) & 0 \\\\\nA & 0 & 0\n\\end{bmatrix} \\begin{bmatrix} \\frac{dz}{dz_i} \\\\ \\frac{d\\lambda}{dz_i} \\\\ \\frac{d\\nu}{dz_i} \\end{bmatrix} = -\\begin{bmatrix} \\frac{dQ}{dz_i}z^\\star + \\frac{dq}{dz_i} + \\frac{dA^T}{dz_i}\\nu^\\star + \\frac{dG^T}{dz_i}\\lambda \\\\ D(\\lambda^\\star)\\frac{dG}{dz_i}z^\\star - D(\\lambda^\\star)\\frac{dh}{dz_i} \\\\ \\frac{dA}{dz_i}z - \\frac{db}{dz_i} \\end{bmatrix}\n\\]\nThe paper also suggest using the adjoint method to compute the gradient of the loss with respect to the input \\(z_i\\). For example, in most cases, \\(\\ell\\) is not dependent on \\(\\lambda\\) and \\(\\nu\\), so the adjoint equation (after transpose) is \\[\n\\dot{y} = \\left[\\begin{array}{l}\nd_z \\\\\nd_\\lambda \\\\\nd_\\nu\n\\end{array}\\right]= - \\left[\\begin{array}{ccc}\nQ & G^T D\\left(\\lambda^{\\star}\\right) & A^T \\\\\nG & D\\left(G z^{\\star}-h\\right) & 0 \\\\\nA & 0 & 0\n\\end{array}\\right]^{-1}\\left[\\begin{array}{c}\n\\left(\\frac{\\partial \\ell}{\\partial z^{\\star}}\\right)^T \\\\\n0 \\\\\n0\n\\end{array}\\right]\n\\]\nDerive eq.(8) in the paper requires matrix differentiation. For instance, we can obtain \\(\\frac{d\\ell}{dA} = \\frac{d\\ell}{dz} \\frac{dz}{dA}\\) by differentiating the KKT condition with respect to \\(A\\).\n\n\nEfficient Forward and Backward Pass\nToday’s state-of-the-art QP solvers like Gurobi and CPLEX do not have the capability of solving multiple optimization problems on the GPU in parallel across the entire minibatch.\nThe paper develops a GPU-based primal-dual interior point method (PDIPM). It considers the following QP with slack variables \\(s\\): \\[\n\\begin{aligned}\n\\min_{z, s} & \\quad \\frac{1}{2} z^T Q z + q^T z \\\\\n\\text{s.t.} & \\quad A z = b \\\\\n& \\quad G z + s = h \\\\\n& \\quad s \\geq 0\n\\end{aligned}\n\\]\nThen the KKT condition can be written as \\[\n\\begin{aligned}\nQz + q + A^T \\nu + G^T \\lambda &= 0 \\\\\nAz - b &= 0 \\\\\nGz + s - h &= 0 \\\\\n\\lambda \\circ s &= 0\n\\end{aligned}\n\\]\nThe last condition is exactly the complementarity slackness.\nA Newton’s method can be used to solve the above system, with some modification, it can becomes the PDIPM.\nTaking the first-order expansion of the KKT condition, we have \\[\nK\\left[\\begin{array}{c}\n\\Delta z^{\\text {aff }} \\\\\n\\Delta s^{\\text {aff }} \\\\\n\\Delta \\lambda^{\\text {aff }} \\\\\n\\Delta \\nu^{\\text {aff }}\n\\end{array}\\right]=\\left[\\begin{array}{c}\n-\\left(A^T \\nu+G^T \\lambda+Q z+q\\right) \\\\\n-S \\lambda \\\\\n-(G z+s-h) \\\\\n-(A z-b)\n\\end{array}\\right]\n\\] where \\[\nK=\\left[\\begin{array}{cccc}\nQ & 0 & G^T & A^T \\\\\n0 & D(\\lambda) & D(s) & 0 \\\\\nG & I & 0 & 0 \\\\\nA & 0 & 0 & 0\n\\end{array}\\right]\n\\]\nThe solution (with some modification) will becomes the update direction.\nThere are several modifications that can be made to improve the efficiency of the PDIPM. First, a symmetric KKT matrix can be used that can be easily solved, by scaling the second block equation by \\(D(1/S)\\). Then the second block row becomes \\[\nD(\\lambda/S) \\Delta s^{\\text {aff }} + \\Delta \\lambda^{\\text {aff }} = - \\lambda\n\\] and \\[\nK_{sym} = \\left[\\begin{array}{cccc}\nQ & 0 & G^T & A^T \\\\\n0 & D(\\lambda/S) & I & 0 \\\\\nG & I & 0 & 0 \\\\\nA & 0 & 0 & 0\n\\end{array}\\right]\n\\] which is symmetric.\nSecond, as the PDIPM requires to solve linear system during the iteration, the paper uses the block factorization technique and pre-factorize portions of them that don’t change during the iteration.\nThird, because the KKT condition represents a nonlinear implicit function, the backpropagation can be found by the implicit function theorem,\nThe paper also finds out that the above matrix (that needs to be inverted when solving the adjoint equation) has been already computed/factorized during the forward pass using the interior point method. Therefore, the computational cost of backpropagation is ‘almost free’. This finding is similar to the fixed point iteration where the Jacobian of the fixed point iteration is already computed during the forward pass using the Newton’s method.\n\n\nSteps\nSimilar to the fixed point iteration. The steps of implementing the differentiable convex layer are:\n\nOutside the gradient tape, solve the optimization problem to find the optimal primal and dual variables \\((z^\\star, \\lambda^\\star, \\nu^\\star)\\) using any off-the-shelf optimization solver, such as interior point, SQP, ADMM, etc.\nInside the gradient tape, engage the input \\(x\\) to the computation graph by \\((z,\\lambda,\\nu):= (z^\\star,\\lambda^\\star,\\nu^\\star) - G(z^\\star,\\lambda^\\star,\\nu^\\star,x)\\). This will provide the the gradient \\(-\\frac{d G}{d x}\\) in the computation graph.\nRegister the gradient of \\((z,\\lambda,\\nu)\\) with the solution of the linear adjoint system Equation 1.\n\n\n\nPytorch Implementation\nAn example code snippet, including the an implementation of OptNet and comparison to CvxpyLayers can be found at my-github."
  },
  {
    "objectID": "posts/learning/optimization/baisc.html",
    "href": "posts/learning/optimization/baisc.html",
    "title": "Constrained Optimization: the Basic",
    "section": "",
    "text": "This post summarized the basic concepts in contrained optimization. The KKT conditions, necessary condition and sufficient condition for optimality will also be introduced."
  },
  {
    "objectID": "posts/learning/optimization/baisc.html#general-definition-and-existence-condition",
    "href": "posts/learning/optimization/baisc.html#general-definition-and-existence-condition",
    "title": "Constrained Optimization: the Basic",
    "section": "General Definition and Existence Condition",
    "text": "General Definition and Existence Condition\nConsider the constrained optimization problem \\[\n\\begin{aligned}\n\\min_x & \\quad f(x) \\\\\n\\text{s.t.} & \\quad g(x) =0 \\\\\n& \\quad h(x) \\leq 0\n\\end{aligned}\n\\] where \\(x\\in\\mathbb{R}^n\\), \\(f:\\mathbb{R}^n\\rightarrow\\mathbb{R}\\), \\(g:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m\\), \\(g:\\mathbb{R}^n\\rightarrow\\mathbb{R}^p\\).\nA point \\(x\\) satisfying all constraints is an admissible point. The set of all admissible points are admissible set, denoted as \\(\\mathcal{X}\\).\nIn this post, it is assumed that \\(f, g, h\\) are two times differentiable.\nAn open ball with center \\(x^\\star\\) and radius \\(\\theta&gt;0\\) is \\[\nB\\left(x^{\\star}, \\theta\\right)=\\left\\{x \\in \\mathbb{R}^n \\mid\\left\\|x-x^{\\star}\\right\\|&lt;\\theta\\right\\}\n\\]\nA point \\(x^\\star \\in \\mathcal{X}\\) is a constrained local minimizer if there exists \\(\\theta&gt;0\\) such that \\[\nf(y) \\geq f(x) \\quad \\forall y \\in \\mathcal{X} \\cap B\\left(x^{\\star}, \\theta\\right)\n\\]\nA point \\(x^\\star \\in \\mathcal{X}\\) is a constrained global minimizer if \\[\nf(y) \\geq f(x) \\quad \\forall y \\in \\mathcal{X}\n\\]\nIf the \\(\\geq\\) becomes \\(&gt;\\) with the extra condition \\(y\\neq x^\\star\\), then \\(x^\\star\\) is a strictly constrained (global) minimizer.\nThe \\(i\\)-th inequality constraint is active at \\(\\tilde{x}\\) if \\(h_i(\\tilde{x})=0\\). Otherwise, it is inactive. An index set of active inequality constraints is denoted as \\(I_a(\\tilde{x}) = \\{i\\in\\{1,\\cdots,p\\}\\mid h_i(\\tilde{x})=0\\}\\). A vector \\(h_a(\\tilde{x}) = \\{h_i(\\tilde{x})\\mid i\\in I_a(\\tilde{x})\\}\\) is the subvector of active inequality constraints.\n\n\n\n\n\n\nNote\n\n\n\nSometimes the equality constraints are also referred as active constraints.\n\n\nA point \\(\\tilde{x}\\) is a regular point if the gradients of active inequality constraints and equality cosntraints are linearly independent. E.g., \\(\\nabla g_i(x),i=1,\\cdots,m\\) and \\(\\nabla h_j(x),j\\in I_a(x)\\) are linearly independent. This is also referred as linear independence constraint qualification (LICQ).\n\n\n\n\n\n\nNote\n\n\n\nWhen the constraints are linear, e.g. \\(A\\tilde{x} = b\\), then \\(\\tilde{x}\\) is a regular point if \\(A\\) is full row rank.\n\n\nThe optimality condition is relatively simple for regular points (or the LICQ condition holds). Consider the Lagrange function \\[\n\\mathcal{L}(x,\\lambda,\\nu) = f(x) + \\lambda^Tg(x) + \\nu^Th(x)\n\\]\nFirst-order necessary condition\nSuppose \\(x^\\star\\) is a constrained local minimizer and \\(x^\\star\\) is a regular points for the constraints (aka, the LICQ condition holds). Then there exists unique mutiplier \\(\\lambda^\\star\\) and \\(\\nu^\\star\\) such that \\[\n\\begin{aligned}\n& \\nabla_x L\\left(x^{\\star}, \\lambda^{\\star}, \\nu^{\\star}\\right)=0 \\\\\n& g\\left(x^{\\star}\\right)=0 \\\\\n& h\\left(x^{\\star}\\right) \\leq 0 \\\\\n& \\nu^{\\star} \\geq 0 \\\\\n& \\left(\\nu^{\\star}\\right)^T h\\left(x^{\\star}\\right)=0\n\\end{aligned}\n\\tag{1}\\] where Equation 1 is the Karush-Kuhn-Tucker (KKT) conditions.\nThe first condition is the stationary condition. It gives that the gradient of the objective can be written as weighted sum of the gradients of the constraints.\nThe last condition is the complementarity slackness. It implies that if the \\(i\\)-th inequality constraint is inactive, then the corresponding multiplier \\(\\nu^\\star_i\\) is zero.\n\n\n\n\n\n\nNote\n\n\n\nOther constraint qualification (QC) or regularity condition is also available, e.g., the Mangasarian-Fromovitz constraint qualification (MFCQ). For convex optimization, the Slater’s condition is also a popular choice.\n\n\nIn Boyd’s book, the necessary condition is stated as,\n\nFor any optimization problem with diﬀerentiable objective and constraint functions for which strong duality obtains, any pair of primal and dual optimal points must satisfy the KKT conditions. In this sense, the condition of strong duality performs the role of QC.\n\nStrict Compelmentarity Condition\nLet \\(x^\\star\\) be a local solution and \\(\\nu^\\star\\) be the multiplier of the inequality constraint. Then the strict complementarity condition holds if \\(\\nu^\\star_i&gt;0\\) for all \\(i\\in\\mathcal{I}_a(\\tilde{x})\\). This condition implies at most one of the \\(\\nu^\\star_i\\) and \\(h_i(x^\\star)\\) is zero.\nSecond-order sufficient condition\n1). Assume that there exist \\(x^\\star, \\lambda^\\star, \\nu^\\star\\) such that the KKT conditions hold. 2). Suppose moreover that \\(\\nu^\\star\\) is such that the strict complementary condition holds at \\(x^\\star\\). 3). Assume final that \\[\ns^T\\nabla^2_{xx}L(x^\\star,\\lambda^\\star,\\nu^\\star)s &gt; 0\n\\] such that for all \\(s\\neq 0\\), \\[\n\\begin{bmatrix}\n\\nabla_x g(x^\\star) \\\\\n\\nabla_x h_a(x^\\star)\n\\end{bmatrix} s = 0\n\\] holds. Then \\(x^\\star\\) is a strict constrained local minimizer.\n\n\n\n\n\n\nNote\n\n\n\nThe necessary and sufficient conditions become global if the optimization is convex."
  },
  {
    "objectID": "posts/learning/optimization/baisc.html#conditions-for-convex-optimization",
    "href": "posts/learning/optimization/baisc.html#conditions-for-convex-optimization",
    "title": "Constrained Optimization: the Basic",
    "section": "Conditions for Convex Optimization",
    "text": "Conditions for Convex Optimization\nAs mentioned above, the local (strict) contrained minimizer is also a global (strict) constrained minimizer if the optimization problem is convex.\nConvex optimization must be in the form of \\[\n\\begin{aligned}\n\\min_x & \\quad f(x) \\\\\n\\text{s.t.} & \\quad Ax =b \\\\\n& \\quad h_i(x) \\leq 0, \\quad i=1,\\cdots,p\n\\end{aligned}\n\\]\nIn Boyd’s book, the constraint qualification is defined as\n\nThere are many results that establish conditions on the problem, beyond convexity, under which strong duality holds. These conditions are called constraint qualiﬁcations.\n\nA commonly used QC for convex optimization is the Slater’s condition: There exists \\(x\\in \\text{relient}\\mathcal{X}\\) such that \\(h_i(x)&lt;0\\) for all \\(i=1,\\cdots,p\\) and \\(Ax = b\\). Slator’s condition statet that for convex optimization, the strong duality holds if the Slater’s condition holds.\nThen back to the necessary condition, for convex problem, if the slator condition holds, the strong duality condition holds, the KKT condition Equation 1 must be satisfied.\nE.g.Convex + Slator condition -&gt; Strong Duality -&gt; KKT condition.\nFor the sufficiency, for convex optimization problem, the KKT condition is sufficient for (primal and dual) optimality. I.e., any pair of \\(\\tilde{x}, \\tilde{\\lambda}, \\tilde{\\nu}\\) is primal and dual optimal.\nE.g. Convex + KKY -&gt; Optimality.\nWith the Slator’s condition, the KKT condition is both necessary and sufficient for (primal and dual) optimality (zero-duality gap) for convex problem."
  },
  {
    "objectID": "posts/learning/power_system/ed.html",
    "href": "posts/learning/power_system/ed.html",
    "title": "Economic Dispatch",
    "section": "",
    "text": "The economic dispatch problem is a fundamental optimization problem in power system operation. It aims to minimize the total generation cost while meeting the power demand and satisfying the operational constraints. This post provides an overview of the economic dispatch problem, its formulation, and solution methods. The main reference of this post is Conejo’s lecture notes. Apart from the basic setting, the on-off and set-point conditions of the unit-commitment problem is also passes through to the ED formulation.\nThis post is also a supplement to the github repository PowerSystemOperation"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Content",
    "section": "",
    "text": "Date\n\n\nModified\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nJun 11, 2024\n\n\nSep 3, 2024\n\n\nDeep Implicit Layers: Differentiable Convex Layer\n\n\nAuto Differentiation, Implicit Function, Paper Read, Optimization\n\n\n\n\nJun 28, 2024\n\n\nJul 24, 2024\n\n\nConstrained Optimization: the Basic\n\n\nOptimization\n\n\n\n\nJul 24, 2024\n\n\nJul 24, 2024\n\n\nBender’s Decomposition for (Mixed-Integer) Linear Programming\n\n\nOptimization\n\n\n\n\nJun 13, 2024\n\n\nJun 19, 2024\n\n\nUnit Commitment Problem\n\n\nPower System\n\n\n\n\nJun 14, 2024\n\n\nJun 19, 2024\n\n\nEconomic Dispatch\n\n\nPower System\n\n\n\n\nJun 18, 2024\n\n\nJun 18, 2024\n\n\nTangent Sensitivity Analysis of ODEs\n\n\nAuto Differentiation, Implicit Function, NeuralODE\n\n\n\n\nJun 5, 2024\n\n\nJun 18, 2024\n\n\nTangent and Adjoint Sensitivity Analysis of Nonlinear Equations\n\n\nAuto Differentiation, Implicit Function\n\n\n\n\nJun 11, 2024\n\n\nJun 11, 2024\n\n\nDeep Implicit Layers: Fixed-Point Iteration\n\n\nAuto Differentiation, Implicit Function\n\n\n\n\nJun 9, 2024\n\n\nJun 9, 2024\n\n\nPower System Operation: AC and DC Power Flow Model\n\n\nPower System\n\n\n\n\nJun 3, 2024\n\n\nJun 4, 2024\n\n\nTangent and Adjoint Sensitivity Analysis of Linear Equations\n\n\nAuto Differentiation, Implicit Function\n\n\n\n\nMay 31, 2024\n\n\nInvalid Date\n\n\nWelcome To My Blog\n\n\nNews\n\n\n\n\n\nNo matching items"
  }
]